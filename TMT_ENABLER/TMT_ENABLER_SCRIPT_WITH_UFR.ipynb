{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "collapsed": false,
    "name": "notebook_steps"
   },
   "source": [
    "# Step by step heriarcy of notebook-\n",
    "\n",
    "`import_libraries` -> `input_selection` -> `user_input` -> `utils` -> `arr_mapping_fields` -> `product_bundling` -> `round_off` -> `dimension_date_dim` -> `{ufr_logics}` -> `flows` -> `main` -> `main_ufr`-> `qc_mechanism`-> `test`\n",
    "\n",
    "**Note-** Do not alter the cell position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f8a57-fe34-4fde-8ea6-69d739a29bbe",
   "metadata": {
    "collapsed": false,
    "name": "markdown_import_libraries"
   },
   "source": [
    "# Import libraries for data processing and snowpark\n",
    "\n",
    "**Key libraries**\n",
    "\n",
    "- `Snowflake snowpark` libraries\n",
    "    - Session, Mathematical Operations\n",
    "    - Snowpark data types\n",
    "- `Pandas` for data frame processing\n",
    "- `functools.reduce`: For functional programming utilities\n",
    "- `ThreadPoolExecutor`: For parallel processing of function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a946e-b00c-463d-9f27-8d8669a00f7a",
   "metadata": {
    "language": "python",
    "name": "import_libraries"
   },
   "outputs": [],
   "source": [
    "# STANDARD LIBRARY IMPORTS\n",
    "import time\n",
    "import traceback\n",
    "import logging\n",
    "import json\n",
    "import glob\n",
    "from functools import reduce\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# CORE SNOWPARK IMPORTS\n",
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark import Session, DataFrame\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.exceptions import SnowparkSQLException\n",
    "\n",
    "# WINDOW FUNCTIONS\n",
    "from snowflake.snowpark import Window\n",
    "from snowflake.snowpark.window import Window  # Note: This is duplicate - keep one\n",
    "from snowflake.snowpark.functions import row_number, lag\n",
    "\n",
    "# SNOWPARK FUNCTIONS MODULE\n",
    "from snowflake.snowpark import functions as F\n",
    "\n",
    "# COLUMN OPERATIONS\n",
    "from snowflake.snowpark.functions import (\n",
    "    col,\n",
    "    when,\n",
    "    lit,\n",
    "    coalesce,\n",
    "    is_null\n",
    ")\n",
    "\n",
    "# AGGREGATION FUNCTIONS\n",
    "from snowflake.snowpark.functions import (\n",
    "    sum,\n",
    "    # Multiple aliases for sum\n",
    "    sum as sum_sf,\n",
    "    sum as spark_sum,\n",
    "    sum as snowflake_sum,\n",
    "    sum as snowpark_sum,\n",
    "    count,\n",
    "    max,\n",
    "    max as spark_max,\n",
    "    min,\n",
    "    min as spark_min\n",
    ")\n",
    "\n",
    "# DATE/TIME FUNCTIONS\n",
    "from snowflake.snowpark.functions import (\n",
    "    to_date,\n",
    "    dateadd,\n",
    "    add_months,\n",
    "    date_trunc,\n",
    "    datediff,\n",
    "    dayofmonth\n",
    ")\n",
    "\n",
    "# STRING FUNCTIONS\n",
    "from snowflake.snowpark.functions import (\n",
    "    upper,\n",
    "    trim,\n",
    "    regexp_replace,\n",
    "    regexp_count,\n",
    "    concat,\n",
    "    length\n",
    ")\n",
    "\n",
    "# MATHEMATICAL FUNCTIONS\n",
    "from snowflake.snowpark.functions import (\n",
    "    abs,\n",
    "    round\n",
    ")\n",
    "\n",
    "# SEQUENCE FUNCTIONS\n",
    "from snowflake.snowpark.functions import (\n",
    "    seq1,\n",
    "    seq8\n",
    ")\n",
    "\n",
    "# DATA TYPES\n",
    "from snowflake.snowpark.types import *\n",
    "from snowflake.snowpark.types import (\n",
    "    DecimalType,\n",
    "    DateType,\n",
    "    StringType\n",
    ")\n",
    "\n",
    "# SESSION INITIALIZATION\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca88d38-9771-45f2-9c36-3f7d8acbe0af",
   "metadata": {
    "language": "python",
    "name": "input_selection"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "# --- Title ---\n",
    "st.title(\"TMT Enabler Input Parameters with UFR\")\n",
    "\n",
    "# --- User Inputs ---\n",
    "# Lookback list selection (allow multiple)\n",
    "lookback_list = st.multiselect(\n",
    "    \"Select lookback period(s):\",\n",
    "    options=[1, 3, 12],\n",
    "    help=\"Choose one or more of Month (1), Quarter (3), or Year (12).\"\n",
    ")\n",
    "\n",
    "# UFR Logic toggle\n",
    "use_ufr_logic = st.checkbox(\"Use UFR Logic\", value=False, help=\"Check this to enable UFR tag logic\")\n",
    "\n",
    "# UFR Lookback list (only show if UFR logic is enabled)\n",
    "if use_ufr_logic:\n",
    "    if lookback_list:\n",
    "        lookback_list_UFR_selected = st.multiselect(\n",
    "            \"Select UFR lookback period(s):\",\n",
    "            options=lookback_list,  # Only show options that are in lookback_list\n",
    "            help=\"Choose from the selected lookback periods above. Must select at least one.\"\n",
    "        )\n",
    "        # Set lookback_list_UFR based on selection\n",
    "        lookback_list_UFR = lookback_list_UFR_selected if lookback_list_UFR_selected else None\n",
    "    else:\n",
    "        st.info(\"Please select lookback periods first to enable UFR options.\")\n",
    "        lookback_list_UFR = None\n",
    "else:\n",
    "    # UFR logic is disabled, so set to None (equivalent to commenting out the assignment)\n",
    "    lookback_list_UFR = None\n",
    "\n",
    "# Input amount selection\n",
    "input_amount = st.selectbox(\n",
    "    \"Select input amount type:\",\n",
    "    options=[\"ARR\", \"MRR\"]\n",
    ")\n",
    "\n",
    "# Run at levels\n",
    "run_at_levels = st.multiselect(\n",
    "    \"Select run at levels:\",\n",
    "    options=[\"ARR\", \"MRR\", \"T3M\", \"TTM\", \"T3M (Annualized)\"],\n",
    "    default=[\"ARR\", \"MRR\"]\n",
    ")\n",
    "\n",
    "# Retention levels\n",
    "retention_levels = st.multiselect(\n",
    "    \"Select retention level(s):\",\n",
    "    options=[\n",
    "        \"Customer_level\",\n",
    "        \"Customer_Product_level\", \n",
    "        \"Customer_Product_RetentionType_level\",\n",
    "        \"Level4\"\n",
    "    ],\n",
    "    default=[\n",
    "        \"Customer_level\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Check if all required inputs are provided ---\n",
    "if (\n",
    "    lookback_list and \n",
    "    input_amount and \n",
    "    run_at_levels and \n",
    "    retention_levels\n",
    "):\n",
    "    st.success(\"All required inputs selected. Ready to run!\")\n",
    "    \n",
    "    # --- Add your business logic or function call below ---\n",
    "    st.write(\"### Selected Parameters:\")\n",
    "    st.write(f\"Lookback List: {lookback_list}\")\n",
    "    \n",
    "    # Display UFR status clearly\n",
    "    if use_ufr_logic and lookback_list_UFR:\n",
    "        st.write(f\"UFR Lookback List: {lookback_list_UFR}\")\n",
    "    else:\n",
    "        st.write(\"UFR Lookback List: None (Not using UFR logic)\")\n",
    "    \n",
    "    st.write(f\"Input Amount: {input_amount}\")\n",
    "    st.write(f\"Run At Levels: {run_at_levels}\")\n",
    "    st.write(f\"Retention Levels: {retention_levels}\")\n",
    "    \n",
    "    # Debug info to show the actual values\n",
    "    # st.write(\"### Debug Info:\")\n",
    "    # st.write(f\"Use UFR Logic: {use_ufr_logic}\")\n",
    "    # st.write(f\"lookback_list_UFR type: {type(lookback_list_UFR)}\")\n",
    "    # st.write(f\"lookback_list_UFR value: {lookback_list_UFR}\")\n",
    "    \n",
    "    # Code representation (like your comment example)\n",
    "    # st.write(\"### Code Representation:\")\n",
    "    # if use_ufr_logic and lookback_list_UFR:\n",
    "    #     st.code(f\"# lookback_list_UFR = None  # Commented out\\nlookback_list_UFR = {lookback_list_UFR}\")\n",
    "    # else:\n",
    "    #     st.code(f\"lookback_list_UFR = None\\n# lookback_list_UFR = [1,3,12]  # Commented out\")\n",
    "    \n",
    "    # You can now call your main function here\n",
    "    # result = your_main_function(\n",
    "    #     lookback_list=lookback_list,\n",
    "    #     lookback_list_UFR=lookback_list_UFR,\n",
    "    #     input_amount=input_amount,\n",
    "    #     run_at_levels=run_at_levels,\n",
    "    #     retention_levels=retention_levels\n",
    "    # )\n",
    "    # st.write(result)\n",
    "else:\n",
    "    st.warning(\"Please select all required parameters to proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e61920-8f51-4a44-bf9a-c8c95606a478",
   "metadata": {
    "language": "python",
    "name": "configuration"
   },
   "outputs": [],
   "source": [
    "# Step 1: Find the JSON file matching *.config.json in current directory\n",
    "config_files = glob.glob(\"*.config.json\")\n",
    "\n",
    "if not config_files:\n",
    "    raise FileNotFoundError(\"No config file matching '*.config.json' found in the current directory.\")\n",
    "elif len(config_files) > 1:\n",
    "    raise RuntimeError(\"Multiple '*.config.json' files found. Expected only one.\")\n",
    "else:\n",
    "    config_file = config_files[0]\n",
    "    print(f\"Loading config from: {config_file}\")\n",
    "\n",
    "# Step 2: Load the config file\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Step 3: Extract input and pbi table names\n",
    "try:\n",
    "    input_table = config[\"ODBC config\"][\"tables\"][\"input_table\"]\n",
    "    pbi_table = config[\"ODBC config\"][\"tables\"][\"pbi_table\"]\n",
    "    excel_table = config[\"ODBC config\"][\"tables\"][\"excel_table\"]\n",
    "    fact_table = config[\"ODBC config\"][\"tables\"][\"fact_table\"]\n",
    "except KeyError as e:\n",
    "    raise KeyError(f\"Missing expected key in config: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b52c56-7c20-4a99-ae64-0d79e2532ab6",
   "metadata": {
    "collapsed": false,
    "name": "markdown_user_input"
   },
   "source": [
    "# User input variables & levers\n",
    "\n",
    "**Input Variables**\n",
    "\n",
    "- `lookback_list`\n",
    "- `lookback_list_UFR`\n",
    "- `input_amount`\n",
    "- `run_at_levels`\n",
    "- `retention_levels`\n",
    "- `input_file_path`\n",
    "- `input_table_templogic`\n",
    "- `input_table_product_bundle`\n",
    "- `pbi_retention_output_path`\n",
    "- `excel_retention_output_path`\n",
    "- `fact_table_output_path`\n",
    "\n",
    "**Mapping columns**\n",
    "- `column_mapping_file`\n",
    "- `Column_mapping_UFR`\n",
    "\n",
    "**Filter Condition**\n",
    "- Can only be done on base columns like  `CUSTOMERID, PRODUCT, REVENUETYPE, CURRENTPERIOD, ARR` \n",
    "- SAMPLE: col(\"CustomerID\") == \"99999_TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19632869-2b70-47fb-8b9c-cb5a79ff3e16",
   "metadata": {
    "language": "python",
    "name": "user_input"
   },
   "outputs": [],
   "source": [
    "# #####################################################################\n",
    "# ### defining lookbacks function, default is 3 for quarter lookbacks\n",
    "# ### for monthly ARR data, YoY = 12, QoQ = 3, MoM = 1\n",
    "# ### for quarterly ARR data, YoY = 4, QoQ = 1\n",
    "# #####################################################################\n",
    "# # Period Comparison. 1 = Month, 3 = Quarter, 12 = Year\n",
    "# lookback_list= [1,3,12]\n",
    "\n",
    "# # Paremeter for UFR logic only\n",
    "# # Period Comparison. 1 = Month, 3 = Quarter, 12 = Year\n",
    "# # Choose only one of MoM, QoQ or YoY for UFR tag\n",
    "# # NOTE: If not to use UFR tag logic, make {lookback_list_UFR= None}.\n",
    "# # NOTE: (lookback_list_UFR =< lookback_list)\n",
    "# lookback_list_UFR= None\n",
    "# # lookback_list_UFR= [1,3,12]\n",
    "\n",
    "# # Input amount - \"ARR\" or \"MRR\"\n",
    "# input_amount = \"ARR\"\n",
    "\n",
    "# # run_at_levels - [\"ARR\", \"MRR\", \"T3M\", \"TTM\", \"T3M (Annualized)\"]\n",
    "# run_at_levels = [\"ARR\", \"MRR\", \"T3M\", \"TTM\", \"T3M (Annualized)\"]\n",
    "\n",
    "# # Retention_level | \n",
    "# # Valid values [\"Customer_level\",\"Customer_Product_level\",\"Customer_Product_RetentionType_level\",\"Level4\"]\n",
    "# retention_levels = [\"Customer_level\",\"Customer_Product_level\",\"Customer_Product_RetentionType_level\"]\n",
    "\n",
    "# Input Tables\n",
    "input_file_path = input_table\n",
    " \n",
    "# Input is a small sample of SUBSCRIPTION_ACCEL.PYTHON_TESTING.INPUT_BOOKING_TEST\n",
    "# Output Tables Path\n",
    "## Note: Do not add _C, _CP and _CPR at the end of the paths. This is handled internally!\n",
    "pbi_retention_output_path = pbi_table\n",
    "excel_retention_output_path = excel_table\n",
    "fact_table_output_path = fact_table\n",
    "\n",
    "# Input Table for Temp logic. \n",
    "# NOTE: If not to use Temp logic, make it \"None\". \n",
    "# input_table_templogic = pbi_retention_output_path + \"_CP\"\n",
    "input_table_templogic = None\n",
    "\n",
    "# Input Table for product bundling\n",
    "input_table_product_bundle = pbi_retention_output_path + \"_CP\"\n",
    "\n",
    "# Mapping File with respect to columns available in input file\n",
    "column_mapping_file = {\n",
    "    \"CUSTOMERID\":'\"CUSTOMERID\"'\n",
    "    ,\"PRODUCT\": '\"PRODUCT\"'\n",
    "    ,\"REVENUETYPE\": '\"REVENUETYPE\"'\n",
    "    ,\"CURRENTPERIOD\":'\"CURRENTPERIOD\"'\n",
    "    ,\"VALUE\":'\"VALUE\"'\n",
    "    ,\"Account Size\": None\n",
    "    ,\"Region\": None\n",
    "    ,\"Industry\": None\n",
    "    ,\"Channel\": None}\n",
    "\n",
    "# Mapping File with respect to columns available in input file for UFR tagging\n",
    "# (Note: \"#Must column\" should be present or could not be None in input table to use UFR.)\n",
    "Column_mapping_UFR = {\n",
    "    \"DATE\": '\"CURRENTPERIOD\"', #Must column\n",
    "    \"START_DATE\": '\"MIN_START_DATE\"', #Must column\n",
    "    \"END_DATE\": '\"MAX_END_DATE\"', #Must column\n",
    "    \"CUSTOMERID\": '\"CUSTOMERID\"', #Must column\n",
    "    \"UP_SNL_ID\": None,\n",
    "    \"PRODUCT\": '\"PRODUCT\"', #Must column\n",
    "    \"PRODUCT_GROUP\": None,\n",
    "    \"OPPORTUNITY_NUMBER\": None,\n",
    "    \"REVENUE_TYPE\": '\"REVENUETYPE\"', #Must column\n",
    "    \"TAGGING\": None,\n",
    "    \"TIER\": None,\n",
    "    \"SEGMENT\": None,\n",
    "    \"Region\": None,\n",
    "    \"MRR\": '\"VALUE\"', #Must column (VALUE)\n",
    "    \"CARR\": None,\n",
    "    \"ADJUSTED_ARR\": None\n",
    "}\n",
    "\n",
    "####################### Filter condition ############################\n",
    "# Can only be done on base columns like \n",
    "# CUSTOMERID, PRODUCT, REVENUETYPE, CURRENTPERIOD, ARR. \n",
    "# SAMPLE: col(\"CustomerID\") == \"99999_TEST\"\n",
    "\n",
    "filter_condition: None # default\n",
    "# filter_condition = col(\"CUSTOMERID\") == \"Customer 002651\"\n",
    "# filter_condition = col(\"CURRENTPERIOD\").between(\"2020-01-01\", \"2020-06-31\")\n",
    "\n",
    "# Add column names here to run the all_qa_check function\n",
    "qa_columns = ['ARR_ROLLCHECK', 'Count_RollCheck', 'Cohort_Max_Dates_Check']\n",
    "\n",
    "# Thresholds\n",
    "qa_check_thresholds = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f632b52-2334-4ed8-9356-2898103c7386",
   "metadata": {
    "collapsed": false,
    "name": "markdown_utils"
   },
   "source": [
    "**This pipeline offers a comprehensive framework for customer retention analysis, valuable for data analysts and business intelligence professionals.**\n",
    "\n",
    "## Key Functions\n",
    "\n",
    "1. **`determine_account_size(arr_value)`**: Classifies account sizes based on ARR values.\n",
    "2. **`add_rename_cols(df, mapping_file)`**: Adds and renames columns in a DataFrame based on a mapping.\n",
    "3. **`add_fact_table_cols(df)`**: Adds dummy columns for reporting in a fact table.\n",
    "4. **`data_loading(session, path, mapping_file, filter_condition, type, input_amount, retention_level)`**: Loads and processes data based on specified analysis type.\n",
    "5. **`generate_months_snowpark(session, input_df)`**: Generates monthly records for all customers.\n",
    "6. **`credit_df_prepare(df)`**: Prepares DataFrame for credit analysis by adjusting ARR values.\n",
    "7. **`window_cols(df)`**: Adds windowed calculations for customer metrics.\n",
    "8. **`base_table_creation(session, df)`**: Creates a base table for further analysis.\n",
    "9. **`retention(df, l)`**: Applies retention logic to calculate various metrics.\n",
    "10. **`save_results(result, path, type)`**: Saves processed results to a specified Snowflake table.\n",
    "\n",
    "## Outputs\n",
    "- DataFrames with retention metrics.\n",
    "- Aggregated results saved in Snowflake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9bf9c-e2ac-4ade-920d-9e14be60ec2d",
   "metadata": {
    "language": "python",
    "name": "utils"
   },
   "outputs": [],
   "source": [
    "# Function to determine account size based on ARR value\n",
    "def determine_account_size(arr_value):\n",
    "    if arr_value < 10000:\n",
    "        return \"1. <10K\"\n",
    "    elif 10000 <= arr_value <= 50000:\n",
    "        return \"2. 10K - 50K\"\n",
    "    elif 50000 < arr_value <= 100000:\n",
    "        return \"3. 50K - 100K\"\n",
    "    elif 100000 < arr_value <= 250000:\n",
    "        return \"4. 100K - 250K\"\n",
    "    elif 250000 < arr_value <= 500000:\n",
    "        return \"5. 250K - 500K\"\n",
    "    elif 500000 < arr_value < 1000000:\n",
    "        return \"6. 500K-1M\"\n",
    "    else:\n",
    "        return \"7. >1M\"\n",
    "\n",
    "# To add missing columns and rename them is required format\n",
    "def add_rename_cols(df: DataFrame, mapping_file: dict) -> DataFrame:\n",
    "    for key, value in mapping_file.items():\n",
    "        if key == \"Account Size\" and \"Account Size\" in df.columns:\n",
    "            # Skip this to preserve our calculated Account Size\n",
    "            continue\n",
    "        elif value is None:\n",
    "            df = df.with_column(key,lit(\"NA\"))\n",
    "        else:\n",
    "            df = df.with_column_renamed(value,key)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Add dummy columns to fact table for PBI template \n",
    "def add_fact_table_cols(df: DataFrame) -> DataFrame:\n",
    "    # dummy colsr\n",
    "    df = df.withColumn('\"Boomerang flag\"', lit('\"No Boomerang\"'))\n",
    "    df = df.withColumn(\"Cust+Prdt\", concat(col(\"CUSTOMERID\"), lit(\"_\"), col('\"Product\"')))\n",
    "    df = df.withColumn('\"Cust+Prdt+RevType\"', concat(col(\"CUSTOMERID\"), lit(\"_\"), col('\"Product\"'), lit(\"_\"), col('\"Revenue Type\"')))\n",
    "    df = df.withColumn('\"Cust+Prdt+RevType+Lv4\"', concat(col(\"CUSTOMERID\"), lit(\"_\"), col('\"Product\"'), lit(\"_\"), col('\"Revenue Type\"'), lit(\"_\"), col('\"LEVEL4\"')))\n",
    "    df = df.withColumn('\"Product Category\"', lit(\"NA\"))\n",
    "    df = df.withColumn('\"Product Level 2\"', lit(\"NA\"))\n",
    "\n",
    "    # index col\n",
    "    window_spec = Window.order_by(lit(1))\n",
    "    df = df.withColumn('\"Index\"', row_number().over(window_spec))\n",
    "\n",
    "    # cohort cols\n",
    "    df = df.withColumn(\"CM\", lit(1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generic method to loading data from table\n",
    "def data_loading(session: Session, path: str, mapping_file: dict, filter_condition:str = None, \n",
    "                 type:str =\"retention\", input_amount:str =\"MRR\", retention_level:str =\"Customer_Product_RetentionType_level\"):\n",
    "    \n",
    "    df = session.table(path)\n",
    "    \n",
    "    df = add_rename_cols(df, mapping_file)\n",
    "\n",
    "    if type == \"retention\":\n",
    "\n",
    "        # df = df.with_column(\"CURRENTPERIOD\", to_date(col(\"CURRENTPERIOD\"), \"yyyy-MM-dd\"))\n",
    "        if retention_level == \"Customer_level\":\n",
    "            df = df.group_by(col(\"CUSTOMERID\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('CURRENTPERIOD')\n",
    "            ).agg(sum(col(\"VALUE\")).alias(\"VALUE\"))\n",
    "            \n",
    "            df = df.with_column(\"PRODUCT\", lit(\"NA\"))\n",
    "            df = df.with_column(\"REVENUETYPE\", lit(\"NA\"))\n",
    "            df = df.with_column(\"LEVEL4\", lit(\"NA\"))\n",
    "            \n",
    "        elif retention_level == \"Customer_Product_level\":\n",
    "            df = df.group_by(col(\"CUSTOMERID\")\n",
    "                , col(\"PRODUCT\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('CURRENTPERIOD')\n",
    "            ).agg(sum(col(\"VALUE\")).alias(\"VALUE\"))\n",
    "            \n",
    "            df = df.with_column(\"REVENUETYPE\", lit(\"NA\"))\n",
    "            df = df.with_column(\"LEVEL4\", lit(\"NA\"))\n",
    "            \n",
    "        elif retention_level == \"Customer_Product_RetentionType_level\":\n",
    "            df = df.group_by(col(\"CUSTOMERID\")\n",
    "                , col(\"PRODUCT\")\n",
    "                , col(\"REVENUETYPE\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('CURRENTPERIOD')\n",
    "            ).agg(sum(col(\"VALUE\")).alias(\"VALUE\"))\n",
    "            \n",
    "            df = df.with_column(\"LEVEL4\", lit(\"NA\"))\n",
    "            \n",
    "        elif retention_level == \"Level4\":\n",
    "            df = df.group_by(col(\"CUSTOMERID\")\n",
    "                , col(\"PRODUCT\")\n",
    "                , col(\"REVENUETYPE\")\n",
    "                , col(\"LEVEL4\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('CURRENTPERIOD')\n",
    "            ).agg(sum(col(\"VALUE\")).alias(\"VALUE\"))       \n",
    "    \n",
    "        if not filter_condition is None:\n",
    "            df = df.filter(filter_condition)\n",
    "            \n",
    "    elif type == \"fact\":\n",
    "        # df = df.with_column(\"CURRENTPERIOD\", to_date(col(\"CURRENTPERIOD\"), \"yyyy-MM-dd\"))\n",
    "        \n",
    "        if retention_level == \"Customer_level\":\n",
    "            df = df.group_by(\n",
    "                col(\"CUSTOMERID\")\n",
    "                , col(\"Account Size\")\n",
    "                , col(\"Region\")\n",
    "                , col(\"Industry\")\n",
    "                , col(\"Channel\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('\"Date\"')\n",
    "            ).agg(\n",
    "                count(col(\"CUSTOMERID\")).alias(\"Count of rows\"),\n",
    "                sum(col(\"VALUE\")).alias(\"VALUE\") \n",
    "            )\n",
    "            \n",
    "            df = df.with_column('\"Product\"', lit(\"NA\"))            \n",
    "            df = df.with_column('\"Revenue Type\"', lit(\"NA\"))\n",
    "            df = df.with_column(\"LEVEL4\", lit(\"NA\"))\n",
    "            \n",
    "        elif retention_level == \"Customer_Product_level\":\n",
    "            df = df.group_by(\n",
    "                col(\"CUSTOMERID\")\n",
    "                , col(\"Account Size\")\n",
    "                , col(\"Region\")\n",
    "                , col(\"Industry\")\n",
    "                , col(\"Channel\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('\"Date\"')\n",
    "                , col(\"PRODUCT\" ).alias('\"Product\"')\n",
    "            ).agg(\n",
    "                count(col(\"CUSTOMERID\")).alias(\"Count of rows\"),\n",
    "                sum(col(\"VALUE\")).alias(\"VALUE\") \n",
    "            )\n",
    "            \n",
    "            df = df.with_column('\"Revenue Type\"', lit(\"NA\"))\n",
    "            df = df.with_column(\"LEVEL4\", lit(\"NA\"))\n",
    "            \n",
    "        elif retention_level == \"Customer_Product_RetentionType_level\":\n",
    "            df = df.group_by(\n",
    "                col(\"CUSTOMERID\")\n",
    "                , col(\"Account Size\")\n",
    "                , col(\"Region\")\n",
    "                , col(\"Industry\")\n",
    "                , col(\"Channel\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('\"Date\"')\n",
    "                , col(\"PRODUCT\" ).alias('\"Product\"')\n",
    "                , col(\"REVENUETYPE\").alias('\"Revenue Type\"')\n",
    "            ).agg(\n",
    "                count(col(\"CUSTOMERID\")).alias(\"Count of rows\"),\n",
    "                sum(col(\"VALUE\")).alias(\"VALUE\") \n",
    "            )\n",
    "            \n",
    "            df = df.with_column(\"LEVEL4\", lit(\"NA\"))\n",
    "            \n",
    "        elif retention_level == \"Level4\":\n",
    "            df = df.group_by(\n",
    "                col(\"CUSTOMERID\")\n",
    "                , col(\"Account Size\")\n",
    "                , col(\"Region\")\n",
    "                , col(\"Industry\")\n",
    "                , col(\"Channel\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('\"Date\"')\n",
    "                , col(\"PRODUCT\" ).alias('\"Product\"')\n",
    "                , col(\"REVENUETYPE\").alias('\"Revenue Type\"')\n",
    "                , col(\"LEVEL4\")\n",
    "            ).agg(\n",
    "                count(col(\"CUSTOMERID\")).alias(\"Count of rows\"),\n",
    "                sum(col(\"VALUE\")).alias(\"VALUE\") \n",
    "            )\n",
    "\n",
    "        df = add_fact_table_cols(df) # adding new cols\n",
    "\n",
    "        if not filter_condition is None:\n",
    "            df = df.filter(filter_condition)\n",
    "\n",
    "        df = df.with_column_renamed(\"CUSTOMERID\",'\"CustomerID\"')\n",
    "  \n",
    "    return df\n",
    "\n",
    "\n",
    "# Generating all the months rows for all the customers\n",
    "def generate_months_snowpark(session: Session, input_df: DataFrame) -> DataFrame:    \n",
    "    print(\"generate_months_snowpark started...\")\n",
    "    cache_s_time = time.time()\n",
    "    # Get date boundaries\n",
    "    date_boundaries = input_df.agg(min(col(\"CURRENTPERIOD\")), max(col(\"CURRENTPERIOD\"))).collect()\n",
    "    min_date = date_boundaries[0][0]\n",
    "    max_date = date_boundaries[0][1]\n",
    "\n",
    "    # Generate month-end dates using Snowpark\n",
    "    date_range_df = session.range((max_date - min_date).days + 1).select(\n",
    "        (to_date(lit(min_date)) + col(\"id\")).alias(\"date\")\n",
    "    ).filter(\n",
    "        dayofmonth(col(\"date\")) == 1\n",
    "    ).select(\n",
    "        col(\"date\").alias(\"month_date\")\n",
    "    ).distinct()\n",
    "\n",
    "    \n",
    "    # Cross join\n",
    "    cross_join = input_df.cross_join(date_range_df)\n",
    "    \n",
    "    # Update ARR column\n",
    "    cross_join = cross_join.with_column(\n",
    "        \"VALUE\",\n",
    "        when(col(\"CURRENTPERIOD\") == col(\"month_date\"), col(\"VALUE\")).otherwise(lit(0))\n",
    "    )\n",
    "    \n",
    "    # Group by and sum ARR\n",
    "    final_table = cross_join.group_by(\n",
    "        *[col for col in cross_join.columns if col not in ['CURRENTPERIOD', \"VALUE\"]]\n",
    "    ).agg(sum(col(\"VALUE\")).alias(\"VALUE\"))\n",
    "    \n",
    "    # Rename column\n",
    "    final_table = final_table.with_column_renamed(\"month_date\", \"CURRENTPERIOD\")\n",
    "    final_table = final_table.with_column(\"Max Date\", lit(max_date))\n",
    "    cache_e_time = time.time()\n",
    "    print(f\"⏱️ Caching df generate_months_snowpark done: {cache_e_time - cache_s_time:.2f} seconds\")\n",
    "    return final_table\n",
    "    \n",
    "\n",
    "# Generating rows for credit\n",
    "def credit_df_prepare(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    # Split into positive and negative ARR\n",
    "    positive_arr = df.filter(col(\"VALUE\") >= 0)\n",
    "    negative_arr = df.filter(col(\"VALUE\") < 0)\n",
    "    \n",
    "    # Add \"-CR$\" to CUSTOMERID\n",
    "    neg_arr_w_name_change = negative_arr.with_column(\n",
    "        \"CUSTOMERID\", concat(col(\"CUSTOMERID\"), lit(\"-CR$\"))\n",
    "    )\n",
    "    \n",
    "    # Adding back the row with ARR as 0\n",
    "    zero_arr = negative_arr.with_column(\"VALUE\", lit(0.0))\n",
    "\n",
    "    df_final = positive_arr.union_by_name(neg_arr_w_name_change).union_by_name(zero_arr)\n",
    "    \n",
    "    df_final = df_final.sort(col('CURRENTPERIOD'))\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# Creating date columns\n",
    "def window_cols(df: DataFrame) -> DataFrame:\n",
    "\n",
    "    ### Define the window specifications for each set of operations (Group By Summarizations)\n",
    "    customer_window = Window.partitionBy(\"CustomerID\")\n",
    "    customer_date_window = Window.partitionBy(\"CustomerID\",\"CurrentPeriod\")\n",
    "    \n",
    "    customer_product_window = Window.partitionBy(\"CustomerID\", \"Product\")\n",
    "    \n",
    "    customer_product_revenue_window = Window.partitionBy(\"CustomerID\", \"Product\", \"RevenueType\")\n",
    "    \n",
    "    customer_product_revenue_level4_window = Window.partitionBy(\"CustomerID\", \"Product\", \"RevenueType\", \"Level4\")\n",
    "\n",
    "    min_date_w_non_zero_arr = min(when(col('VALUE') != 0, col(\"CurrentPeriod\")))\n",
    "    max_date_w_non_zero_arr = max(when(col('VALUE') != 0, col(\"CurrentPeriod\")))\n",
    "\n",
    "    # Calculate max ARR for each customer (use ARR*12 if the input is MRR)\n",
    "    max_value = max(col(\"VALUE\")).over(customer_window)\n",
    "    df = df.withColumn(\"Customer_Max_ARR\", max_value)\n",
    "    \n",
    "    # Determine account size based on max ARR\n",
    "    df = df.withColumn(\"Account Size\", \n",
    "        when(col(\"Customer_Max_ARR\") < 10000, \"1. <10K\")\n",
    "        .when((col(\"Customer_Max_ARR\") >= 10000) & (col(\"Customer_Max_ARR\") <= 50000), \"2. 10K - 50K\")\n",
    "        .when((col(\"Customer_Max_ARR\") > 50000) & (col(\"Customer_Max_ARR\") <= 100000), \"3. 50K - 100K\")\n",
    "        .when((col(\"Customer_Max_ARR\") > 100000) & (col(\"Customer_Max_ARR\") <= 250000), \"4. 100K - 250K\")\n",
    "        .when((col(\"Customer_Max_ARR\") > 250000) & (col(\"Customer_Max_ARR\") <= 500000), \"5. 250K - 500K\")\n",
    "        .when((col(\"Customer_Max_ARR\") > 500000) & (col(\"Customer_Max_ARR\") < 1000000), \"6. 500K-1M\")\n",
    "        .otherwise(\"7. >1M\")\n",
    "    )\n",
    "    \n",
    "    ## Add minimum and maximum dates for each customer with ARR > 0 (Cohort and MaxARR Dates)\n",
    "    df = df.withColumn(\"Cust_MinDate\", min_date_w_non_zero_arr.over(customer_window))\n",
    "    df = df.withColumn(\"Cust_MaxDate\", max_date_w_non_zero_arr.over(customer_window))\n",
    "\n",
    "    ### Add minimum and maximum dates for each customer-product combination with ARR > 0 (Cohort and MaxARR Dates, 2nd lvl)\n",
    "    df = df.withColumn(\"Cust_Prod_MinDate\", min_date_w_non_zero_arr.over(customer_product_window))\n",
    "    df = df.withColumn(\"Cust_Prod_MaxDate\", max_date_w_non_zero_arr.over(customer_product_window))\n",
    "\n",
    "    ### Add minimum and maximum dates for each customer-product-revtype combination with ARR > 0 (Cohort and MaxARR Dates, 3rd lvl)\n",
    "    df = df.withColumn(\"Cust_Prod_Rev_MinDate\", min_date_w_non_zero_arr.over(customer_product_revenue_window))\n",
    "    df = df.withColumn(\"Cust_Prod_Rev_MaxDate\", max_date_w_non_zero_arr.over(customer_product_revenue_window))\n",
    "    \n",
    "    ### Add minimum and maximum dates for each customer-product-revtype combination with ARR > 0 (Cohort and MaxARR Dates, 3rd lvl)\n",
    "    df = df.withColumn(\"Cust_Prod_Rev_Lv4_MinDate\", min_date_w_non_zero_arr.over(customer_product_revenue_level4_window))\n",
    "    df = df.withColumn(\"Cust_Prod_Rev_Lv4_MaxDate\", max_date_w_non_zero_arr.over(customer_product_revenue_level4_window))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Base table creation: calling function for all row generating logic\n",
    "def base_table_creation(session, df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    df = window_cols(df)\n",
    "    \n",
    "    ### Credit treatment \n",
    "    df = credit_df_prepare(df)\n",
    "    \n",
    "    df = generate_months_snowpark(session, df)\n",
    "\n",
    "    ### add column for total revenue for a customer in a specific period\n",
    "    customer_date_window = Window.partitionBy(\"CustomerID\",\"CurrentPeriod\")\n",
    "    df = df.withColumn(\"Total_ARR\", sum(col(\"VALUE\")).over(customer_date_window))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Creating credit related columns\n",
    "def credit_cols(df: DataFrame) -> DataFrame:\n",
    "    ### Define Credit ARR\n",
    "    df = df.withColumn(\"Credit\",\n",
    "        when(\n",
    "            col(\"VALUE\") < 0\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "\n",
    "    ### Define Credit Reversal ARR\n",
    "    df = df.withColumn(\"Credit_Reversal\",\n",
    "        when(\n",
    "            (col(\"Prior_ARR\") < 0) & (col(\"VALUE\") >= 0)\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "        \n",
    "    ### Define Net Credit  ARR\n",
    "    df = df.withColumn(\"Net_Credit\",\n",
    "                       col('Credit') + col('Credit_Reversal') \n",
    "                      )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Creating ARR related columns\n",
    "def arr_cols(df: DataFrame,l) -> DataFrame:\n",
    "    customer_product_revenue_date_order_window = Window.partitionBy(\"CustomerID\", \"Product\", \"RevenueType\").orderBy(col(\"CurrentPeriod\").asc())\n",
    "    ARR_positive_check = (col(\"Prior_ARR\") == 0) & (col(\"VALUE\") > 0)\n",
    "    Prior_ARR_positive_check = (col(\"VALUE\") == 0) & (col(\"Prior_ARR\") > 0)\n",
    "    \n",
    "    ### Assign Lookback by customer by product by revenue type for ARR (Prior ARR)\n",
    "    df = df.withColumn(\"Prior_ARR\"\n",
    "        , lag(col(\"VALUE\"), offset=l, default_value=0)\n",
    "        .over(customer_product_revenue_date_order_window))\n",
    "    \n",
    "    ### Calculate ARR Change Period of Period (PoP Variance)\n",
    "    df = df.withColumn(\"ARR_Variance\", col(\"VALUE\") - col(\"Prior_ARR\"))\n",
    "\n",
    "    ### Define New Customer ARR\n",
    "    df = df.withColumn(\"NewCust_ARR\",\n",
    "        when(\n",
    "            ARR_positive_check \n",
    "            & (col(\"CurrentPeriod\") < add_months(col(\"Cust_MinDate\"), l)) \n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    ### Define New Product ARR (Cross-sell)\n",
    "    df = df.withColumn(\"NewProd_ARR\",\n",
    "        when(\n",
    "            ARR_positive_check\n",
    "            & (col(\"NewCust_ARR\") == 0)\n",
    "            & (col(\"CurrentPeriod\") < add_months(col(\"Cust_Prod_MinDate\"), l))\n",
    "            , col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    ### Define New Revenue Type ARR (Cross-sell Rev Type)\n",
    "    df = df.withColumn(\"NewRev_ARR\",\n",
    "        when(\n",
    "            ARR_positive_check\n",
    "            & (col(\"NewCust_ARR\") == 0)\n",
    "            & (col(\"NewProd_ARR\") == 0)\n",
    "            & (col(\"CurrentPeriod\") < add_months(col(\"Cust_Prod_Rev_MinDate\"), l))\n",
    "            , col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    ### Define New Lv4 ARR\n",
    "    df = df.withColumn(\"NewLv4_ARR\",\n",
    "        when(\n",
    "            ARR_positive_check\n",
    "            & (col(\"NewCust_ARR\") == 0)\n",
    "            & (col(\"NewProd_ARR\") == 0)\n",
    "            & (col(\"NewRev_ARR\") == 0)\n",
    "            & (col(\"CurrentPeriod\") < add_months(col(\"Cust_Prod_Rev_Lv4_MinDate\"), l))\n",
    "            , col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "        \n",
    "    ### Define Temp New ARR     \n",
    "    df = df.withColumn(\"TempNew_ARR\",\n",
    "        when(\n",
    "            ARR_positive_check\n",
    "            & (col(\"NewCust_ARR\") == 0)\n",
    "            & (col(\"NewProd_ARR\") == 0)\n",
    "            & (col(\"NewRev_ARR\") == 0)\n",
    "            & (col(\"NewLv4_ARR\") == 0)\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    ### Define Upsell ARR       \n",
    "    df = df.withColumn(\"Upsell_ARR\",\n",
    "        when(\n",
    "            (col(\"VALUE\") > col(\"Prior_ARR\"))\n",
    "            & (col(\"Prior_ARR\") > 0)\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "   \n",
    "    ### Define Customer Churn ARR        \n",
    "    df = df.withColumn(\"ChurnCust_ARR\",\n",
    "        when(\n",
    "            Prior_ARR_positive_check\n",
    "            & (col(\"CurrentPeriod\") > col(\"Cust_MaxDate\"))\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    ### Define Product Churn ARR\n",
    "    df = df.withColumn(\"ChurnProd_ARR\",\n",
    "        when(\n",
    "            Prior_ARR_positive_check\n",
    "            & (col(\"ChurnCust_ARR\") == 0)\n",
    "            & (col(\"CurrentPeriod\") > col(\"Cust_Prod_MaxDate\"))\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    ### Define Revenue Type Churn ARR\n",
    "    df = df.withColumn(\"ChurnRev_ARR\",\n",
    "        when(\n",
    "            Prior_ARR_positive_check\n",
    "            & (col(\"ChurnCust_ARR\") == 0)\n",
    "            & (col(\"ChurnProd_ARR\") == 0)\n",
    "            & (col(\"CurrentPeriod\") > col(\"Cust_Prod_Rev_MaxDate\"))\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    ### Define Lv4 Churn ARR\n",
    "    df = df.withColumn(\"ChurnLv4_ARR\",\n",
    "        when(\n",
    "            Prior_ARR_positive_check\n",
    "            & (col(\"ChurnCust_ARR\") == 0)\n",
    "            & (col(\"ChurnProd_ARR\") == 0)\n",
    "            & (col(\"ChurnRev_ARR\") == 0)\n",
    "            & (col(\"CurrentPeriod\") > col(\"Cust_Prod_Rev_Lv4_MaxDate\"))\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    ### Define Temp Loss ARR\n",
    "    df = df.withColumn(\"TempLoss_ARR\",\n",
    "        when(\n",
    "            Prior_ARR_positive_check\n",
    "            & (col(\"ChurnCust_ARR\") == 0)\n",
    "            & (col(\"ChurnProd_ARR\") == 0)\n",
    "            & (col(\"ChurnRev_ARR\") == 0)\n",
    "            & (col(\"ChurnLv4_ARR\") == 0)\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "\n",
    "    ### Define Downsell ARR       \n",
    "    df = df.withColumn(\"Downsell_ARR\",\n",
    "        when(\n",
    "            (col(\"VALUE\") < col(\"Prior_ARR\"))\n",
    "            & (col(\"VALUE\") > 0)\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    ) \n",
    "    return df\n",
    "\n",
    "# Creating QA checks related columns   \n",
    "def qa_cols(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    ### Define Roll Forward Check      \n",
    "    df = df.withColumn(\n",
    "        \"ARR_RollCheck\",\n",
    "        (col('NewCust_ARR') + col('NewProd_ARR') \n",
    "        + col('NewRev_ARR') + col('TempNew_ARR')\n",
    "        + col('Upsell_ARR') + col('ChurnCust_ARR')\n",
    "        + col('ChurnProd_ARR') + col('ChurnRev_ARR') \n",
    "        + col('TempLoss_ARR') + col('Downsell_ARR') \n",
    "        + col('Prior_ARR') + col('Net_Credit')\n",
    "        - col(\"VALUE\")).cast(\"float\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"Count_RollCheck\",\n",
    "        col('BoP_count')\n",
    "        + col('NewCust_count') + col('NewProd_count')\n",
    "        + col('NewRev_count') + col('TempNew_count') \n",
    "        + col('ChurnCust_count') + col('ChurnProd_count') \n",
    "        + col('ChurnRev_count') + col('TempLoss_count') \n",
    "        - col('EoP_count')\n",
    "    )\n",
    "    \n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"Cohort_Max_Dates_Check\",\n",
    "        when( (col(\"VALUE\")!=0 ) &\n",
    "             (\n",
    "                 (col('CurrentPeriod') > col('Cust_MaxDate')) | \n",
    "                 (col('CurrentPeriod') > col('Cust_Prod_MaxDate')) | \n",
    "                 (col('CurrentPeriod') > col('Cust_Prod_Rev_MaxDate'))\n",
    "             ),1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Creating count related columns\n",
    "def count_cols(df: DataFrame) -> DataFrame:\n",
    "\n",
    "    ### Define New Customer Count\n",
    "    df = df.withColumn(\"NewCust_count\", when(col(\"NewCust_ARR\") != 0, 1).otherwise(0))\n",
    "\n",
    "    ### Define New Product Count     \n",
    "    df = df.withColumn(\"NewProd_count\", when(col(\"NewProd_ARR\") != 0, 1).otherwise(0))\n",
    "\n",
    "    ### Define New Revenue Type Count       \n",
    "    df = df.withColumn(\"NewRev_count\", when(col(\"NewRev_ARR\") > 0, 1).otherwise(0))\n",
    "\n",
    "    ### Define Temp New Customer Count          \n",
    "    df = df.withColumn(\"TempNew_count\", when(col(\"TempNew_ARR\") > 0, 1).otherwise(0)) \n",
    "\n",
    "    ### Define Upsell Customer Count        \n",
    "    df = df.withColumn(\"Upsell_count\", when(col(\"Upsell_ARR\") > 0, 1).otherwise(0))\n",
    "\n",
    "    ### Define Churn Customer Count            \n",
    "    df = df.withColumn(\"ChurnCust_count\", when(col(\"ChurnCust_ARR\") != 0, -1).otherwise(0))\n",
    "        \n",
    "    ### Define Churn Product Count        \n",
    "    df = df.withColumn(\"ChurnProd_count\", when(col(\"ChurnProd_ARR\") != 0, -1).otherwise(0))\n",
    "\n",
    "    ### Define Churn Revenue Type Count        \n",
    "    df = df.withColumn(\"ChurnRev_count\", when(col(\"ChurnRev_ARR\") < 0, -1).otherwise(0))\n",
    "\n",
    "    ### Define Downsell Customer Count\n",
    "    df = df.withColumn(\"Downsell_count\", when(col(\"Downsell_ARR\") < 0, -1).otherwise(0))\n",
    "\n",
    "    ### Define Temp Loss Customer Count     \n",
    "    df = df.withColumn(\"TempLoss_count\", when(col(\"TempLoss_ARR\") < 0, -1).otherwise(0))\n",
    "\n",
    "    ### EoP Customer Count\n",
    "    df = df.withColumn(\"EoP_count\", when(col(\"VALUE\") > 0, 1).otherwise(0))\n",
    "\n",
    "    ### BoP Customer Count\n",
    "    df = df.with_column(\"BoP_count\", when(col(\"Prior_ARR\") > 0, 1).otherwise(0))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Creating columns not falling in any category for example, RetentionCategory and Period\n",
    "def extra_cols(df: DataFrame,l) -> DataFrame:\n",
    "\n",
    "    #### Define Retention Categories ####  \n",
    "    df = df.with_column(\"RetentionCategory\",\n",
    "        when(col(\"Credit\") != 0,'Credit')\n",
    "        .when(col(\"Credit_Reversal\") != 0,'Credit Reversal')\n",
    "        .when(col(\"NewCust_ARR\") != 0, 'New Cust')\n",
    "        .when(col(\"NewProd_ARR\") != 0, 'New Prod')\n",
    "        .when(col(\"NewRev_ARR\") != 0, 'New Rev Type')\n",
    "        .when(col(\"NewLv4_ARR\") != 0, 'New Lv4')\n",
    "        .when(col(\"TempNew_ARR\") != 0, 'Increase_TempNew')\n",
    "        .when(col(\"Upsell_ARR\") != 0, 'Increase')\n",
    "        .when(col(\"ChurnCust_ARR\") != 0, 'Churn Cust')\n",
    "        .when(col(\"ChurnProd_ARR\") != 0, 'Churn Prod')\n",
    "        .when(col(\"ChurnRev_ARR\") != 0, 'Churn Rev Type')\n",
    "        .when(col(\"ChurnLv4_ARR\") != 0, 'Churn Lv4')\n",
    "        .when(col(\"TempLoss_ARR\") != 0, 'Decrease_TempLost')\n",
    "        .when(col(\"Downsell_ARR\") != 0, 'Decrease')\n",
    "        .otherwise('NoChange')\n",
    "    )\n",
    "\n",
    "    ### add lookback column as a tag\n",
    "    df = df.with_column(\"Period\", lit(l).cast(\"string\"))\n",
    "    \n",
    "    df = df.with_column(\"Period\",\n",
    "        when(col(\"Period\") == \"1\", \"Month\")\n",
    "        .when(col(\"Period\") == \"3\", \"Quarter\")\n",
    "        .when(col(\"Period\") == \"12\", \"Year\")\n",
    "        .otherwise(col(\"Period\")) \n",
    "    )\n",
    "\n",
    "    df = df.with_column(\n",
    "        \"Retained100%$\",\n",
    "        when(col(\"RetentionCategory\") == 'NoChange'\n",
    "             ,col(\"ARR_Variance\")\n",
    "            )\n",
    "    )\n",
    "\n",
    "    df = df.with_column(\n",
    "        \"Date SOM\", date_trunc('month', col('CURRENTPERIOD'))\n",
    "    )\n",
    "\n",
    "    df = df.with_column(\n",
    "        \"TTM Date\",when(col('Date SOM') > col('Max Date'), dateadd('year',lit(1),'Date SOM'))\n",
    "    .otherwise(col('Date SOM'))\n",
    "    )\n",
    "\n",
    "    ## Placeholder columns\n",
    "    df = df.with_column(\"UFR Amount\", lit(0.0))\n",
    "    df = df.with_column(\"UFR Tag\", lit(\"Not UFR\"))\n",
    "    df = df.with_column(\"Current Winback Tag\", lit(\"\"))\n",
    "    df = df.with_column(\"UFR Date\", lit(\"\"))\n",
    "    df = df.with_column(\"WinbackTag\", lit(\"No Winback\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calling function for Retention logic \n",
    "def retention(df: DataFrame,l) -> DataFrame: \n",
    "    new_df = arr_cols(df,l)\n",
    "    new_df = credit_cols(new_df)\n",
    "    new_df = count_cols(new_df)\n",
    "    new_df = extra_cols(new_df,l)\n",
    "    new_df = qa_cols(new_df)\n",
    "    return new_df\n",
    "\n",
    "# Syncing columns name to Alteryx version\n",
    "def rename_cols(df: DataFrame) -> DataFrame:\n",
    "    alyterx_col_mapping = {\n",
    "        \"ARR_VARIANCE\": '\"YoY Variance\"',\n",
    "        \"VALUE\": '\"EoP$\"',\n",
    "        \"PRIOR_ARR\": '\"BoP$\"',\n",
    "        \"NEWCUST_ARR\": '\"NewCust$\"',\n",
    "        \"NEWPROD_ARR\": '\"NewProd$\"',\n",
    "        \"NEWREV_ARR\": '\"NewRevType$\"',\n",
    "        \"TEMPNEW_ARR\": '\"Increase_TempNew\"',\n",
    "        \"UPSELL_ARR\": '\"Increase$\"',\n",
    "        \"CHURNCUST_ARR\": '\"ChurnCust$\"',\n",
    "        \"CHURNPROD_ARR\": '\"ChurnProd$\"',\n",
    "        \"CHURNREV_ARR\": '\"ChurnRevType$\"',\n",
    "        \"TEMPLOSS_ARR\": '\"Decrease_TempLost\"',\n",
    "        \"DOWNSELL_ARR\": '\"Decrease$\"',\n",
    "        \"NET_CREDIT\": '\"Net Credit\"',\n",
    "        \"NEWCUST_COUNT\": '\"NewCust_count\"',\n",
    "        \"NEWPROD_COUNT\": '\"NewProd_count\"',\n",
    "        \"NEWREV_COUNT\": '\"NewRevType_count\"',\n",
    "        \"TEMPNEW_COUNT\": '\"Increases_TempNew_count\"', #Added extra 's'\n",
    "        \"UPSELL_COUNT\": '\"Increase_count\"',\n",
    "        \"CHURNCUST_COUNT\": '\"ChurnCust_count\"',\n",
    "        \"CHURNPROD_COUNT\": '\"ChurnProd_count\"',\n",
    "        \"CHURNREV_COUNT\": '\"ChurnRevType_count\"',\n",
    "        \"DOWNSELL_COUNT\": '\"Decreases_count\"',\n",
    "        \"TEMPLOSS_COUNT\": '\"Decreases_TempLost_count\"',\n",
    "        \"CURRENTPERIOD\": '\"Current Period\"',\n",
    "        \"CUSTOMERID\": '\"CustName\"',\n",
    "        \"PERIOD\": '\"Period\"',\n",
    "        \"PRODUCT\": '\"Product\"',\n",
    "        \"RETENTIONCATEGORY\": '\"RetentionCategory\"',\n",
    "        \"REVENUETYPE\": '\"Revenue Type\"',\n",
    "        \"CUST_MINDATE\": '\"Cohort Date\"',\n",
    "        \"CUST_MAXDATE\": '\"Max Date w ARR\"',\n",
    "        \"CUST_PROD_MAXDATE\": '\"Max Date w ARR_2nd lv\"',\n",
    "        \"CUST_PROD_MINDATE\": '\"Cohort Date_2nd lv\"',\n",
    "        \"CUST_PROD_REV_MAXDATE\": '\"Max Date w ARR_3rd lv\"',\n",
    "        \"CUST_PROD_REV_MINDATE\": '\"Cohort Date_3rd lv\"',\n",
    "        \"CUST_PROD_REV_Lv4_MAXDATE\": '\"Max Date w ARR_4th lv\"',\n",
    "        \"CUST_PROD_REV_Lv4_MINDATE\": '\"Cohort Date_4th lv\"',\n",
    "        \"Credit\": '\"Credit_$\"',\n",
    "        \"CREDIT_REVERSAL\":'\"Credit_Reversal$\"',\n",
    "        \"EOP_COUNT\": '\"EoP_count\"',\n",
    "        \"BOP_COUNT\": '\"BoP_count\"',\n",
    "        \"ARR_ROLLCHECK\": '\"Check\"',\n",
    "        \"COUNT_ROLLCHECK\": '\"Check_Count\"'\n",
    "    }\n",
    "\n",
    "    for python_name, alyterx_name in alyterx_col_mapping.items():\n",
    "        df = df.withColumnRenamed(python_name, alyterx_name)\n",
    "\n",
    "    return df\n",
    "    \n",
    "# Cleaning step to make output equivalent to Alteryx version\n",
    "def post_cleaning(df_retention: DataFrame) -> DataFrame:\n",
    "    \n",
    "    ## Returning to original customer name/ remove '-CR$' from end\n",
    "    df_retention = df_retention.with_column(\n",
    "        \"CUSTOMERID\", regexp_replace(col(\"CUSTOMERID\"), r\"-CR\\$$\", \"\")\n",
    "    )\n",
    "\n",
    "    ## Removing row with ARR = 0 and PRIOR_ARR = 0\n",
    "    non_zero_arr_prior_arr = ~((col(\"ARR\") == 0) & (col(\"PRIOR_ARR\") == 0))\n",
    "    df_retention = df_retention.filter(non_zero_arr_prior_arr).sort(col('CURRENTPERIOD'))\n",
    "\n",
    "    df_retention = rename_cols(df_retention)\n",
    "    \n",
    "    return df_retention\n",
    "\n",
    "# Custom sum function to add list of columns\n",
    "def sum_columns(columns):\n",
    "    return reduce(lambda a, b: a + b, columns)\n",
    "\n",
    "# Creating amount columns\n",
    "def add_amount_columns(df: DataFrame, input_amount) -> DataFrame:\n",
    "    customer_product_revenue_date_order_window = Window.partitionBy(\"CustomerID\", \"Product\", \"RevenueType\").orderBy(col(\"CurrentPeriod\").asc())\n",
    "\n",
    "    ## Adding new amounts\n",
    "    if input_amount == \"MRR\":\n",
    "        df = df.withColumn(\"ARR\",col(\"VALUE\")*12)\n",
    "        df = df.withColumn(\"MRR\",col(\"VALUE\"))\n",
    "    elif input_amount == \"ARR\":\n",
    "        df = df.withColumn(\"MRR\",col(\"VALUE\")/12)\n",
    "        df = df.withColumn(\"ARR\",col(\"VALUE\"))\n",
    "    else:\n",
    "        raise ValueError(\"Input amount not valid!\")\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"T3M\", sum_columns([lag(col(\"MRR\"), offset=i, default_value=0).over(customer_product_revenue_date_order_window) for i in range(3)]))\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"TTM\", sum_columns([lag(col(\"MRR\"), offset=i, default_value=0).over(customer_product_revenue_date_order_window) for i in range(12)]))\n",
    "\n",
    "    df = df.withColumn(\"T3M (Annualized)\", col(\"T3M\")*4)\n",
    "    return df\n",
    "\n",
    "# Main pipeline function\n",
    "def retention_pipeline_v2(session, df: DataFrame, lb_periods = [3], input_amount=\"MRR\", run_at_levels=[\"ARR\"]) -> DataFrame:\n",
    "    print(\"retention_pipeline_v2 started...\")\n",
    "    cache_s_time = time.time()\n",
    "    base_table = base_table_creation(session, df)\n",
    "    base_table_W_amount = add_amount_columns(base_table, input_amount)\n",
    "    base_table_W_amount_cols = base_table_W_amount.cache_result()\n",
    "    \n",
    "    # Process each run_at_level and collect results\n",
    "    run_at_level_dfs = []\n",
    "    \n",
    "    for run_at_level in run_at_levels:\n",
    "        if run_at_level not in [\"ARR\", \"MRR\", \"T3M\", \"TTM\", \"T3M (Annualized)\"]:\n",
    "            raise ValueError(\"Incorrect Run_at_level. Check user inputs.\")\n",
    "        \n",
    "        df_with_value = base_table_W_amount_cols.withColumn(\"VALUE\", col(run_at_level))\n",
    "        \n",
    "        # Process each lookback period within this run_at_level\n",
    "        retention_dfs = []\n",
    "        for lb in lb_periods:\n",
    "            if lb not in [1, 3, 12, 4]:\n",
    "                raise ValueError(\"Incorrect lookback period. Check user inputs.\")\n",
    "            \n",
    "            retention_df = retention(df_with_value, lb)\n",
    "            retention_dfs.append(retention_df.select(*retention_df.columns))\n",
    "        \n",
    "        # Union all retention_dfs for this run_at_level\n",
    "        if retention_dfs:\n",
    "            df_retention = retention_dfs[0]\n",
    "            for df in retention_dfs[1:]:\n",
    "                df_retention = df_retention.union_all(df)\n",
    "            \n",
    "            df_retention = post_cleaning(df_retention)\n",
    "            df_retention = df_retention.withColumn('\"Amount\"', lit(run_at_level))\n",
    "            run_at_level_dfs.append(df_retention.select(*df_retention.columns))\n",
    "    \n",
    "    # Union all run_at_level results\n",
    "    if run_at_level_dfs:\n",
    "        df_retention_all_amounts = run_at_level_dfs[0]\n",
    "        for df in run_at_level_dfs[1:]:\n",
    "            df_retention_all_amounts = df_retention_all_amounts.union_all(df)\n",
    "    else:\n",
    "        # Handle empty case to match original behavior\n",
    "        df_retention_all_amounts = None\n",
    "    cache_e_time = time.time()\n",
    "    print(f\"⏱️ retention_pipeline_v2 done: {cache_e_time - cache_s_time:.2f} seconds\")\n",
    "    return df_retention_all_amounts\n",
    "\n",
    "  \n",
    "# Data prep for period on period qa check\n",
    "def prep_data_pop_test_v2(df: DataFrame) -> DataFrame:\n",
    "    agg_df_2 = df.group_by(\"Current Period\").agg(\n",
    "        sum('\"EoP$\"').alias(\"EoP$\"),\n",
    "        sum('\"BoP$\"').alias(\"BoP$\")\n",
    "    )\n",
    "    \n",
    "    melted_df = agg_df_2.select(\n",
    "        col(\"Current Period\"),\n",
    "        col(\"EoP$\").alias(\"Value\"),\n",
    "        lit(\"EoP$\").alias(\"Metric\")\n",
    "    ).union_all(\n",
    "        agg_df_2.select(\n",
    "            col(\"Current Period\"),\n",
    "            col(\"BoP$\").alias(\"Value\"),\n",
    "            lit(\"BoP$\").alias(\"Metric\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    pivot_df_2 = melted_df.group_by(\"Metric\").pivot(\"Current Period\").agg(sum(\"Value\"))\n",
    "    pivot_df_2 = pivot_df_2.rename({col(\"Metric\"):\"Source\"})\n",
    "    pivot_df_2 = pivot_df_2.sort(col(\"Source\"))\n",
    "\n",
    "    return pivot_df_2\n",
    "    \n",
    "# Verifying QA check\n",
    "def verify_qa_check(input_df: DataFrame, columns_to_check: list, qa_check_thresholds: float) -> DataFrame:\n",
    "    \"\"\"Fliters the dataframe based on given column and threshold \n",
    "\n",
    "    Args:\n",
    "        input_df (DataFrame): _description_\n",
    "        columns_to_check (list): _description_\n",
    "        qa_check_thresholds (float): _description_\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: _description_\n",
    "    \"\"\"\n",
    "    filter_condition = None\n",
    "    for column in columns_to_check:\n",
    "        if filter_condition is None:\n",
    "            filter_condition = abs(col(column)) >= qa_check_thresholds\n",
    "        else:\n",
    "            filter_condition |= abs(col(column)) >= qa_check_thresholds\n",
    "    \n",
    "    # Apply the filter condition to the DataFrame\n",
    "    filtered_df = input_df.filter(filter_condition)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Save result in table\n",
    "def save_results(result: DataFrame, path: str, type:str = 'overwrite'):\n",
    "    \"\"\"Saves results in a table on the given patj\n",
    "\n",
    "    Args:\n",
    "        result (DataFrame): _description_\n",
    "        path (str): _description_\n",
    "        type (str): _description_\n",
    "    \"\"\"\n",
    "    #print(\"__Writing to table started...\")\n",
    "    start_time = time.time()\n",
    "    print(\"___Saving on path: \", path)\n",
    "    result.write.mode(type).save_as_table(path)\n",
    "    end_time = time.time()\n",
    "    print(\"_save_results.\")\n",
    "    print(f\"⏱️ Save result: {end_time - start_time:.2f} seconds\")\n",
    "    #print(\"__Writing to table completed!\")\n",
    "\n",
    "\n",
    "# Clear console output\n",
    "def clear_console():\n",
    "    \"\"\" Clears console\n",
    "    \"\"\"\n",
    "    if os.name == 'nt':  # For Windows\n",
    "        os.system('cls')\n",
    "    else:  # For macOS and Linux\n",
    "        os.system('clear')\n",
    "\n",
    "# To convert seconds into hours, mins and seconds\n",
    "def cal_time(start_time: int, end_time: int):\n",
    "    \"\"\" Take start and end time. Calculate hours, minutes, seconds\n",
    "\n",
    "    Args:\n",
    "        start_time: start time in secs\n",
    "        end_time: end time in secs\n",
    "\n",
    "    Returns:\n",
    "        hours, minutes, seconds\n",
    "    \"\"\"\n",
    "    elapsed_time = (end_time - start_time)\n",
    "    hours = elapsed_time // 3600\n",
    "    minutes = (elapsed_time % 3600) // 60\n",
    "    seconds = elapsed_time % 60\n",
    "    \n",
    "    return hours, minutes, seconds\n",
    "\n",
    "# To add suffix to file path based on retention level\n",
    "def get_file_path(retention_level: str, output_path: str) -> str:\n",
    "    if retention_level == \"Customer_level\":\n",
    "        updated_output_path = output_path + \"_C_NOTEBOOK\"\n",
    "    elif retention_level == \"Customer_Product_level\":\n",
    "        updated_output_path = output_path + \"_CP\"\n",
    "    elif retention_level == \"Customer_Product_RetentionType_level\":\n",
    "        updated_output_path = output_path + \"_CPR\"\n",
    "    elif retention_level == \"Level4\":\n",
    "        updated_output_path = output_path + \"_L4\"\n",
    "    return updated_output_path\n",
    "\n",
    "\n",
    "\n",
    "def transform_retention_data(session, df1: DataFrame, table2: str,  retention_level: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform retention data using Snowpark\n",
    "    \n",
    "    Args:\n",
    "        session: Snowpark session object\n",
    "        table1: Name of the first table (CUST_PROD_RETENTION_OLD)\n",
    "        table2: Name of the second table (CUST_RETENTION)\n",
    "    \n",
    "    Returns:\n",
    "        Snowpark DataFrame with transformed data\n",
    "    \"\"\"\n",
    "    # Load tables as DataFrames\n",
    "    print(\"**********In Tranform Retention Data***********\")\n",
    "    df2 = session.table(table2)\n",
    "\n",
    "    # Filter rows where RetentionCategory is in the list\n",
    "    filtered_df = df2.filter(\n",
    "        col('\"RetentionCategory\"').isin([\"Decrease_TempLost\", \"Increase_TempNew\"])\n",
    "    )\n",
    "\n",
    "    # Select the required columns\n",
    "    selected_df = filtered_df.select(\n",
    "        col('\"Amount\"'),\n",
    "        col('\"Current Period\"'),\n",
    "        col('\"CustName\"'),\n",
    "        col('\"RetentionCategory\"'),\n",
    "        col('\"Period\"')\n",
    "    )\n",
    "    # Group by all selected columns to mimic GROUP BY ALL\n",
    "    grouped_df = selected_df.group_by(\n",
    "        '\"CustName\"', '\"Current Period\"', '\"RetentionCategory\"', '\"Amount\"', '\"Period\"'\n",
    "    ).agg()\n",
    "\n",
    "    df2 = grouped_df\n",
    "    print(df2.schema)\n",
    "    # grouped_df now contains distinct rows matching the filter\n",
    "    # Columns to exclude\n",
    "    exclude_cols = [\n",
    "        '\"RetentionCategory\"', \"Credit_Reversal$\", \"Credit_$\", \"NewCust$\", \"ChurnCust$\", \"Increase$\", \n",
    "        \"Decrease$\", \"Decrease_TempLost$\", \"Increase_TempNew$\", \"Retained100%$\", \"NewProd$\", \n",
    "        \"ChurnProd$\", \"BoP_count\", \"ChurnCust_count\", \"ChurnProd_count\", \"Decreases_count\", \n",
    "        \"Decreases_TempLost_count\", \"Increases_TempNew_count\", \"Increase_count\", \"NewCust_count\", \n",
    "        \"NewProd_count\", \"EoP_count\",'\"Current Period\"','\"CustName\"','\"Period\"','\"Amount\"'\n",
    "    ]\n",
    "    new_cols = ['\"NewRevType$\"', '\"ChurnRevType$\"', '\"NewRevType_count\"', '\"ChurnRevType_count\"']\n",
    "\n",
    "    if retention_level == 'Customer_Product_RetentionType_level':\n",
    "        exclude_cols.extend(new_cols)\n",
    "\n",
    "    # Select all columns from T1 except excluded ones\n",
    "    select_cols = [col(c) for c in df1.columns if c not in exclude_cols]\n",
    "    print(df1.columns)\n",
    "    print(select_cols)\n",
    "    # Define new RetentionCategory logic (Tool ID 3438)\n",
    "    retention_category_expr = when(\n",
    "        (df1['\"RetentionCategory\"'] == \"Increase_TempNew\") & (df2['\"RetentionCategory\"'].is_null()),\n",
    "        lit(\"New Prod\")\n",
    "    ).when(\n",
    "        (df1['\"RetentionCategory\"'] == \"Decrease_TempLost\") & (df2['\"RetentionCategory\"'].is_null()),\n",
    "        lit(\"Churn Prod\")\n",
    "    ).when(\n",
    "        df2['\"RetentionCategory\"'].is_not_null(),\n",
    "        df2['\"RetentionCategory\"']\n",
    "    ).otherwise(df1['\"RetentionCategory\"'])\n",
    "\n",
    "    # Define YOY_VARIANCE-based columns (Tool ID 3439)\n",
    "    credit_reversal = when(col('\"RetentionCategory\"') == \"Credit Reversal\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    credit = when(col('\"RetentionCategory\"') == \"Credit\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    new_cust = when(col('\"RetentionCategory\"') == \"New Cust\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    churn_cust = when(col('\"RetentionCategory\"') == \"Churn Cust\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    increase = when(col('\"RetentionCategory\"') == \"Increase\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    decrease = when(col('\"RetentionCategory\"') == \"Decrease\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    decrease_temp_lost = when(col('\"RetentionCategory\"') == \"Decrease_TempLost\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    increase_temp_new = when(col('\"RetentionCategory\"') == \"Increase_TempNew\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    retained_100 = when(col('\"RetentionCategory\"') == \"NoChange\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    new_prod = when(col('\"RetentionCategory\"') == \"New Prod\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    churn_prod = when(col('\"RetentionCategory\"') == \"Churn Prod\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    if retention_level == 'Customer_Product_RetentionType_level':\n",
    "        new_rev_type = when(col('\"RetentionCategory\"') == \"New Rev Type\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "        churn_rev_type = when(col('\"RetentionCategory\"') == \"Churn Rev Type\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    # Compute CHECK column\n",
    "    check_col = (\n",
    "        df1['\"BoP$\"']+ \n",
    "        new_cust + \n",
    "        churn_cust + \n",
    "        increase + \n",
    "        increase_temp_new + \n",
    "        decrease + \n",
    "        decrease_temp_lost + \n",
    "        new_prod + \n",
    "        churn_prod + \n",
    "        retained_100 - \n",
    "        df1['\"EoP$\"']\n",
    "    )\n",
    "\n",
    "    # Define count columns (Tool ID 3441)\n",
    "    bop_count = when(df1['\"BoP$\"'] != 0, lit(1)).otherwise(lit(0))\n",
    "    churn_cust_count = when(churn_cust != 0, lit(-1)).otherwise(lit(0))\n",
    "    churn_prod_count = when(churn_prod != 0, lit(-1)).otherwise(lit(0))\n",
    "    decreases_count = when(decrease != 0, lit(-1)).otherwise(lit(0))\n",
    "    decreases_temp_lost_count = when(decrease_temp_lost != 0, lit(-1)).otherwise(lit(0))\n",
    "    increases_temp_new_count = when(increase_temp_new != 0, lit(1)).otherwise(lit(0))\n",
    "    increase_count = when(increase != 0, lit(1)).otherwise(lit(0))\n",
    "    new_cust_count = when(new_cust != 0, lit(1)).otherwise(lit(0))\n",
    "    new_prod_count = when(new_prod != 0, lit(1)).otherwise(lit(0))\n",
    "    eop_count = when(df1['\"EoP$\"'] != 0, lit(1)).otherwise(lit(0))\n",
    "    if retention_level == 'Customer_Product_RetentionType_level':\n",
    "        new_rev_type_count = when(new_rev_type != 0, lit(1)).otherwise(lit(0))\n",
    "        churn_rev_type_count = when(churn_rev_type != 0, lit(1)).otherwise(lit(0))\n",
    "    \n",
    "\n",
    "    # Perform the full outer join\n",
    "    result_df = df1.join(\n",
    "        df2,\n",
    "        (df1['\"Current Period\"'] == df2['\"Current Period\"']) &\n",
    "        (df1['\"CustName\"'] == df2['\"CustName\"']) &\n",
    "        (df1['\"Period\"'] == df2['\"Period\"']) &\n",
    "        (df1['\"Amount\"'] == df2['\"Amount\"']),\n",
    "        \"full_outer\"\n",
    "    )\n",
    "\n",
    "    # Select final columns\n",
    "    if retention_level == 'Customer_Product_RetentionType_level':\n",
    "        result_df = result_df.select(\n",
    "            *select_cols,\n",
    "            df1['\"Current Period\"'].alias('\"Current Period\"'),df1['\"Period\"'].alias('\"Period\"'),\n",
    "            df1['\"CustName\"'].alias('\"CustName\"'),df1['\"Amount\"'].alias('\"Amount\"'),\n",
    "            retention_category_expr.alias('\"RetentionCategory\"'), credit_reversal.alias('\"Credit_Reversal$\"'),\n",
    "            credit.alias('\"Credit_$\"'),new_cust.alias('\"NewCust$\"'),\n",
    "            churn_cust.alias('\"ChurnCust$\"'),increase.alias('\"Increase$\"'),\n",
    "            decrease.alias('\"Decrease$\"'),decrease_temp_lost.alias('\"Decrease_TempLost$\"'),\n",
    "            increase_temp_new.alias('\"Increase_TempNew$\"'),retained_100.alias('\"Retained100%$\"'),\n",
    "            new_prod.alias('\"NewProd$\"'),churn_prod.alias('\"ChurnProd$\"'),\n",
    "            new_rev_type.alias('\"NewRevType$\"'),churn_rev_type.alias('\"ChurnRevType$\"'),\n",
    "            check_col.alias('\"Check\"'),bop_count.alias('\"BoP_count\"'),\n",
    "            churn_cust_count.alias('\"ChurnCust_count\"'),churn_prod_count.alias('\"ChurnProd_count\"'),\n",
    "            decreases_count.alias('\"Decreases_count\"'),decreases_temp_lost_count.alias('\"Decreases_TempLost_count\"'),\n",
    "            increases_temp_new_count.alias('\"Increases_TempNew_count\"'),increase_count.alias('\"Increase_count\"'),\n",
    "            new_cust_count.alias('\"NewCust_count\"'),new_prod_count.alias('\"NewProd_count\"'),\n",
    "            eop_count.alias('\"EoP_count\"'), new_rev_type_count.alias('\"NewRevType_count\"'),\n",
    "            churn_rev_type_count.alias('\"ChurnRevType_count\"')\n",
    "        )\n",
    "        result_df = result_df.drop(df1['\"ChurnCust$\"'],df1['\"Credit_Reversal$\"'],\n",
    "                                   df1['\"Credit_$\"'],df1['\"NewCust$\"'],df1['\"NewProd$\"'],\n",
    "                                   df1['\"Increase$\"'],df1['\"Decrease$\"'],df1['\"Retained100%$\"'],\n",
    "                                   df1['\"NewProd$\"'],df1['\"ChurnProd$\"'],df1['\"Check\"'],\n",
    "                                   df1['\"BoP_count\"'],df1['\"ChurnCust_count\"'],df1['\"ChurnProd_count\"'],\n",
    "                                   df1['\"Decreases_count\"'],df1['\"Decreases_TempLost_count\"'],\n",
    "                                   df1['\"Increases_TempNew_count\"'],df1['\"Increase_count\"'],\n",
    "                                   df1['\"NewCust_count\"'],df1['\"NewProd_count\"'],df1['\"EoP_count\"'])\n",
    "    else:\n",
    "        result_df = result_df.select(\n",
    "            *select_cols,\n",
    "            df1['\"Current Period\"'].alias('\"Current Period\"'),df1['\"Period\"'].alias('\"Period\"'),\n",
    "            df1['\"CustName\"'].alias('\"CustName\"'),df1['\"Amount\"'].alias('\"Amount\"'),\n",
    "            retention_category_expr.alias('\"RetentionCategory\"'), credit_reversal.alias('\"Credit_Reversal$\"'),\n",
    "            credit.alias('\"Credit_$\"'),new_cust.alias('\"NewCust$\"'),\n",
    "            churn_cust.alias('\"ChurnCust$\"'),increase.alias('\"Increase$\"'),\n",
    "            decrease.alias('\"Decrease$\"'),decrease_temp_lost.alias('\"Decrease_TempLost$\"'),\n",
    "            increase_temp_new.alias('\"Increase_TempNew$\"'),retained_100.alias('\"Retained100%$\"'),\n",
    "            new_prod.alias('\"NewProd$\"'),churn_prod.alias('\"ChurnProd$\"'),\n",
    "            check_col.alias('\"Check\"'),bop_count.alias('\"BoP_count\"'),\n",
    "            churn_cust_count.alias('\"ChurnCust_count\"'),churn_prod_count.alias('\"ChurnProd_count\"'),\n",
    "            decreases_count.alias('\"Decreases_count\"'),decreases_temp_lost_count.alias('\"Decreases_TempLost_count\"'),\n",
    "            increases_temp_new_count.alias('\"Increases_TempNew_count\"'),increase_count.alias('\"Increase_count\"'),\n",
    "            new_cust_count.alias('\"NewCust_count\"'),new_prod_count.alias('\"NewProd_count\"'),\n",
    "            eop_count.alias('\"EoP_count\"')\n",
    "        )\n",
    "        result_df = result_df.drop(df1['\"ChurnCust$\"'],df1['\"Credit_Reversal$\"'],\n",
    "                                   df1['\"Credit_$\"'],df1['\"NewCust$\"'],df1['\"NewProd$\"'],\n",
    "                                   df1['\"Increase$\"'],df1['\"Decrease$\"'],df1['\"Retained100%$\"'],\n",
    "                                   df1['\"NewProd$\"'],df1['\"ChurnProd$\"'],df1['\"Check\"'],\n",
    "                                   df1['\"BoP_count\"'],df1['\"ChurnCust_count\"'],df1['\"ChurnProd_count\"'],\n",
    "                                   df1['\"Decreases_count\"'],df1['\"Decreases_TempLost_count\"'],\n",
    "                                   df1['\"Increases_TempNew_count\"'],df1['\"Increase_count\"'],\n",
    "                                   df1['\"NewCust_count\"'],df1['\"NewProd_count\"'],df1['\"EoP_count\"'])\n",
    "    \n",
    "    print(result_df.schema)\n",
    "    print(\"**********Out Tranform Retention Data***********\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e8c90-92a4-4754-b363-4e0d2985883f",
   "metadata": {
    "collapsed": false,
    "name": "markdown_arr_mapping_and_round_off"
   },
   "source": [
    "# Fields mapped according to highest ARR\n",
    "**Logic force fields to be 1:1 with customer based on highest ARR. Logic also outputs customers that were affected by the logic so that users can quantify it.**\n",
    "\n",
    "**Mapped Fields**\n",
    "- `REGION`\n",
    "- `INDUSTRY`\n",
    "- `CHANNEL`\n",
    "\n",
    "**IF you do not want to use it, kindly ignore \"ARR_mapping_fields\" block of code and use \"optimized_main\" instead of \"optimized_main_with_mapping_bundling\" to genearte output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00688e60-aa4f-4acb-b244-e579fa6c6c7c",
   "metadata": {
    "language": "python",
    "name": "arr_mapping_and_round_off"
   },
   "outputs": [],
   "source": [
    "def standardize_customer_fields(session: Session, input_table_path: str, column_mapping_file: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizes fields to be 1:1 with customer based on the highest ARR value.\n",
    "    Also creates a report of affected customers.\n",
    "    \n",
    "    Parameters:\n",
    "    session (Session): The Snowpark session object.\n",
    "    input_table_path (str): Path to the input table.\n",
    "    column_mapping_file (dict): Dictionary mapping standard column names to input table column names.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (Standardized DataFrame, DataFrame of affected customers)\n",
    "    \"\"\"\n",
    "    print(\"\\n_Standardizing customer fields started...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read the input table\n",
    "    df = session.table(input_table_path)\n",
    "    \n",
    "    # Determine which fields to standardize based on the column mapping\n",
    "    fields_to_standardize = []\n",
    "    reverse_mapping = {}\n",
    "    \n",
    "    # Create a reverse mapping for easy reference and identify fields to standardize\n",
    "    for standard_col, input_col in column_mapping_file.items():\n",
    "        if input_col is not None:\n",
    "            # Remove quotes if present in the column name\n",
    "            clean_input_col = input_col.replace('\"', '')\n",
    "            reverse_mapping[clean_input_col] = standard_col\n",
    "            \n",
    "            # Add standard fields that should be standardized and exist in the input\n",
    "            if standard_col in ['Region', 'Industry', 'Channel'] and clean_input_col in df.columns:\n",
    "                fields_to_standardize.append(clean_input_col)\n",
    "    \n",
    "    # Get the customer ID and value (ARR) column names from the mapping\n",
    "    customer_id_col = column_mapping_file[\"CUSTOMERID\"].replace('\"', '') if column_mapping_file[\"CUSTOMERID\"] else \"CUSTOMERID\"\n",
    "    value_col = column_mapping_file[\"VALUE\"].replace('\"', '') if column_mapping_file[\"VALUE\"] else \"VALUE\"\n",
    "    \n",
    "    affected_customers = []\n",
    "    \n",
    "    # Create a standardized dataframe\n",
    "    standardized_df = df\n",
    "    \n",
    "    for field in fields_to_standardize:\n",
    "        # Check if we need to standardize this field (if customers have multiple values)\n",
    "        field_count_per_customer = standardized_df.group_by(customer_id_col, field).count()\n",
    "        customers_with_multiple_values = field_count_per_customer.group_by(customer_id_col).count().filter(col('COUNT') > 1)\n",
    "        \n",
    "        # Only proceed if there are customers that need standardization\n",
    "        if customers_with_multiple_values.count() > 0:\n",
    "            # Find dominant value for each customer based on highest total ARR\n",
    "            dominant_values = standardized_df.group_by(customer_id_col, field).agg(\n",
    "                sum(value_col).alias('TOTAL_ARR')\n",
    "            ).with_column(\n",
    "                'RANK', row_number().over(\n",
    "                    Window.partition_by(customer_id_col).order_by(col('TOTAL_ARR').desc())\n",
    "                )\n",
    "            ).filter(col('RANK') == 1).select(customer_id_col, field, 'TOTAL_ARR')\n",
    "            \n",
    "            # Get list of affected customers with their original and new values\n",
    "            affected_field_df = standardized_df.filter(\n",
    "                col(customer_id_col).in_(customers_with_multiple_values.select(customer_id_col))\n",
    "            ).select(customer_id_col, field, value_col)\n",
    "            \n",
    "            affected_field_df = affected_field_df.join(\n",
    "                dominant_values,\n",
    "                [customer_id_col],\n",
    "                suffix='_DOMINANT'\n",
    "            ).filter(\n",
    "                col(field) != col(f'{field}_DOMINANT')\n",
    "            ).select(\n",
    "                customer_id_col, \n",
    "                col(field).alias(f'ORIGINAL_{field}'),\n",
    "                col(f'{field}_DOMINANT').alias(f'NEW_{field}'),\n",
    "                value_col\n",
    "            ).distinct()\n",
    "            \n",
    "            # Add to affected customers report\n",
    "            if affected_field_df.count() > 0:\n",
    "                affected_customers.append(affected_field_df)\n",
    "            \n",
    "            # Create a map of customerid to dominant value\n",
    "            dominant_map = dominant_values.select(customer_id_col, field).to_pandas().set_index(customer_id_col)[field].to_dict()\n",
    "            \n",
    "            # Update values in the original dataframe\n",
    "            standardized_df = standardized_df.with_column(\n",
    "                field,\n",
    "                when(\n",
    "                    col(customer_id_col).in_(customers_with_multiple_values.select(customer_id_col)),\n",
    "                    # Map function to replace values for affected customers\n",
    "                    when(\n",
    "                        lit(True), \n",
    "                        lit(None)  # Placeholder that will be replaced\n",
    "                    ).otherwise(col(field))\n",
    "                ).otherwise(col(field))\n",
    "            )\n",
    "            \n",
    "            # Use pandas to efficiently apply the mapping\n",
    "            # Convert to pandas, apply the mapping, and convert back to Snowpark DataFrame\n",
    "            pdf = standardized_df.to_pandas()\n",
    "            affected_indices = pdf[customer_id_col].isin(dominant_map.keys())\n",
    "            pdf.loc[affected_indices, field] = pdf.loc[affected_indices, customer_id_col].map(dominant_map)\n",
    "            standardized_df = session.create_dataframe(pdf)\n",
    "            \n",
    "            print(f\"Standardized '{field}' for {customers_with_multiple_values.count()} customers\")\n",
    "    \n",
    "    # Combine all affected customers into a single report\n",
    "    affected_customers_df = None\n",
    "    if affected_customers:\n",
    "        affected_customers_df = reduce(\n",
    "            lambda df1, df2: df1.union(df2), \n",
    "            affected_customers\n",
    "        )\n",
    "        \n",
    "        # Save affected customers to a table\n",
    "        affected_customers_report_path = input_file_path + \"_AFFECTED_CUSTOMERS_REPORT\"\n",
    "        save_results(affected_customers_df, affected_customers_report_path)\n",
    "        print(f\"Saved affected customers report to {affected_customers_report_path}\")\n",
    "    else:\n",
    "        print(\"No customers were affected by standardization\")\n",
    "    \n",
    "    # Save standardized dataframe\n",
    "    output_path = input_file_path + \"_ARR_Mapped\"\n",
    "    save_results(standardized_df, output_path)\n",
    "    print(f\"___Saving on path: {output_path}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"⏱️ Field standardization runtime: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return standardized_df, affected_customers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0956d21-f2b8-40cd-ad23-44afb18d7a3d",
   "metadata": {
    "collapsed": false,
    "name": "markdown_product_bundle"
   },
   "source": [
    "# Product Bundle Analysis\n",
    "**Goal: Analyze customer-product bundles using Snowpark to generate ARR, MRR, and cross-sell insights.**\n",
    "\n",
    "`Load Data`\n",
    "\n",
    "- Read input Snowflake table: input_table_product_bundle.\n",
    "\n",
    "`Map & Validate Columns`\n",
    "\n",
    "- Rename columns using column_mapping_bundling.\n",
    "\n",
    "- Check for required fields like CUSTOMERID, PRODUCT, VALUE, etc.\n",
    "\n",
    "`Customer-Level Metrics`\n",
    "\n",
    "- Group by period & customer to calculate:\n",
    "\n",
    "- ARR, CROSSSELL\n",
    "\n",
    "- PRODUCT_COMBO (distinct products list)\n",
    "\n",
    "`Bundle Calculation`\n",
    "\n",
    "- Compute PRODUCT_BUNDLE as number of products (via comma count in PRODUCT_COMBO).\n",
    "\n",
    "`Final Aggregation`\n",
    "\n",
    "- Group by period, cohort date, bundle info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df75d4e-4b27-439c-90d0-9add49c38535",
   "metadata": {
    "language": "python",
    "name": "product_bundle"
   },
   "outputs": [],
   "source": [
    "def product_bundle_analysis(session: Session):\n",
    "    \"\"\"\n",
    "    Convert SQL query to Snowpark DataFrame for product bundle analysis\n",
    "    Integrates with existing notebook flow\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 3: GENERATING PRODUCT BUNDLE ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Mapping File with respect to columns available in input_table_product_bundle\n",
    "    column_mapping_bundling = {\n",
    "    \"CURRENTPERIOD\": '\"Current Period\"',\n",
    "    \"CUSTOMERID\": '\"CustName\"',\n",
    "    \"TRANSACTION_COHORT_DATE\": '\"Cohort Date\"',\n",
    "    \"VALUE\": '\"EoP$\"',\n",
    "    \"NEWPROD_ARR\": '\"NewRevType$\"',\n",
    "    \"PRODUCT\": '\"Product\"',\n",
    "    \"PERIOD\": '\"Period\"',\n",
    "    \"AMOUNT\": '\"Amount\"'}\n",
    "    \n",
    "    # Read the input data from the standardized output\n",
    "    #input_table = get_file_path(\"Customer_Product_level\", pbi_retention_output_path)\n",
    "    print(f\"Reading data from: {input_table_product_bundle}\")\n",
    "    \n",
    "    # Use Snowpark DataFrame\n",
    "    df = session.table(input_table_product_bundle)\n",
    "    \n",
    "    # Debug: Print column names to see what's available\n",
    "    # print(\"Available columns in source data:\")\n",
    "    # print(df.columns)\n",
    "    \n",
    "    # Create a working copy with mapped columns\n",
    "    work_df = df\n",
    "    \n",
    "    # Rename columns according to mapping\n",
    "    for sql_col, df_col in column_mapping_bundling.items():\n",
    "        if df_col in df.columns:\n",
    "            work_df = work_df.rename(df_col, sql_col)\n",
    "    \n",
    "    # print(\"Columns after mapping:\")\n",
    "    # print(work_df.columns)\n",
    "    \n",
    "    # Check for missing columns\n",
    "    required_columns = ['CURRENTPERIOD', 'CUSTOMERID', 'TRANSACTION_COHORT_DATE', 'VALUE', 'NEWPROD_ARR', 'PRODUCT']\n",
    "    missing_cols = [col for col in required_columns if col not in work_df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        # print(f\"Missing columns: {missing_cols}\")\n",
    "        raise ValueError(f\"Required columns are missing: {missing_cols}\")\n",
    "    \n",
    "    try:\n",
    "        # STEP 1: Equivalent to the customer_level CTE\n",
    "        print(\"Creating customer_level aggregation...\")\n",
    "        \n",
    "        # Using listagg with WITHIN GROUP to get distinct, sorted products\n",
    "        customer_level = work_df.group_by(\n",
    "            'CURRENTPERIOD', 'CUSTOMERID', 'TRANSACTION_COHORT_DATE', 'AMOUNT', 'PERIOD'\n",
    "        ).agg(\n",
    "            sum_sf('VALUE').alias('ARR'),\n",
    "            sum_sf('NEWPROD_ARR').alias('CROSSSELL'),\n",
    "            F.call_builtin(\"LISTAGG\", F.call_builtin(\"DISTINCT\", col('PRODUCT')), lit(',')).alias('PRODUCT_COMBO')\n",
    "        )\n",
    "        \n",
    "        # Calculate MRR from ARR using Snowpark's column operations\n",
    "        customer_level = customer_level.with_column('MRR', col('ARR') / 12)\n",
    "        \n",
    "        # STEP 2: Equivalent to the product_bundle CTE\n",
    "        print(\"Calculating product bundles...\")\n",
    "        \n",
    "        # Count commas in PRODUCT_COMBO to determine bundle size and add 1\n",
    "        # We need to handle NULL values with COALESCE in Snowpark\n",
    "        customer_level = customer_level.with_column(\n",
    "            'PRODUCT_BUNDLE', \n",
    "            regexp_count(col('PRODUCT_COMBO'), ',') + 1\n",
    "        )\n",
    "        \n",
    "        # STEP 3: Equivalent to the final SELECT query\n",
    "        print(\"Performing final aggregation...\")\n",
    "        \n",
    "        # Group by the required fields for sum aggregations using Snowpark\n",
    "        final_result = customer_level.group_by(\n",
    "            'CURRENTPERIOD', 'TRANSACTION_COHORT_DATE', 'PRODUCT_COMBO', 'PRODUCT_BUNDLE'\n",
    "        ).agg(\n",
    "            sum_sf('ARR').alias('ARR'),\n",
    "            sum_sf('MRR').alias('MRR'),\n",
    "            sum_sf('CROSSSELL').alias('CROSSSELL'),\n",
    "            count(when(col('ARR') > 0, col('CUSTOMERID'))).alias('DISTINCT_CUSTOMERS_WITH_ARR')\n",
    "        )\n",
    "        \n",
    "        # Write results to output table using Snowpark's native save method\n",
    "        output_table = input_table_product_bundle + \"_PRODUCT_BUNDLE_ANALYSIS\"\n",
    "        # print(f\"Writing results to: {output_table}\")\n",
    "        \n",
    "        # Save to Snowflake table with mode overwrite\n",
    "        final_result.write.mode(\"overwrite\").save_as_table(output_table)\n",
    "        \n",
    "        print(f\"___Saving on path: {output_table}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in product bundle analysis: {str(e)}\")\n",
    "        print(\"Detailed error information:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417bb747-8630-438f-b84d-48de30bdb511",
   "metadata": {
    "collapsed": false,
    "name": "markdown_round_off"
   },
   "source": [
    "# Float Column Rounding\n",
    "**Goal: Ensure consistency and control over decimal precision by rounding all float-type columns in a Snowflake table to a specified number of decimal places (default: 3).**\n",
    "\n",
    "`Read Input Table`\n",
    "\n",
    "- Loads table from the given table_path.\n",
    "\n",
    "`Identify Float Columns`\n",
    "\n",
    "- Uses DESCRIBE TABLE to detect columns of type: FLOAT, DOUBLE, REAL, DECIMAL, or NUMERIC.\n",
    "\n",
    "`Round Float Values`\n",
    "\n",
    "- Applies rounding to each float column using Snowpark's round() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ce4f2-3116-45f8-a499-92c26af454df",
   "metadata": {
    "language": "python",
    "name": "round_off"
   },
   "outputs": [],
   "source": [
    "def round_float_columns(session: Session, table_path, decimal_places=3):\n",
    "    \"\"\"\n",
    "    Rounds all float columns in a Snowflake table to the specified number of decimal places.\n",
    "    \n",
    "    Parameters:\n",
    "    session (Session): The Snowpark session object.\n",
    "    table_path (str): Path to the input table.\n",
    "    decimal_places (int): Number of decimal places to round to (default: 3).\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with float columns rounded to specified decimal places.\n",
    "    \"\"\"\n",
    "    from snowflake.snowpark.functions import round, col\n",
    "    import time\n",
    "    \n",
    "    print(f\"\\n_Rounding float columns to {decimal_places} decimal places...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read the table first\n",
    "    df = session.table(table_path)\n",
    "    \n",
    "    # Get column information using Snowflake's DESCRIBE TABLE command\n",
    "    table_columns = session.sql(f\"DESCRIBE TABLE {table_path}\").collect()\n",
    "    \n",
    "    # Identify float columns\n",
    "    float_columns = []\n",
    "    for column_info in table_columns:\n",
    "        column_type = str(column_info['type']).upper()\n",
    "        column_name = column_info['name']\n",
    "        if any(float_type in column_type for float_type in ['FLOAT', 'DOUBLE', 'REAL', 'DECIMAL', 'NUMERIC']):\n",
    "            float_columns.append(column_name)\n",
    "    \n",
    "    if not float_columns:\n",
    "        print(\"No float columns found in the table.\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Found {len(float_columns)} float columns to round: {', '.join(float_columns)}\")\n",
    "    \n",
    "    # Apply rounding to each float column using Snowpark DataFrame API\n",
    "    for column_name in float_columns:\n",
    "        df = df.withColumn(column_name, round(col(column_name), decimal_places))\n",
    "    \n",
    "    # Save the rounded data back to a new table\n",
    "    rounded_table_path = table_path + \"_ROUNDED\"\n",
    "    df.write.mode(\"overwrite\").save_as_table(rounded_table_path)\n",
    "    print(f\"___Saving on path: {rounded_table_path}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"⏱️ Float column rounding runtime: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4c35b-9413-43e6-9508-b266c1acfe52",
   "metadata": {
    "language": "python",
    "name": "dimension_date_dim"
   },
   "outputs": [],
   "source": [
    "def create_master_dimension_table(session: Session, source_table_name: str, target_table_name: str):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        session: Snowpark session\n",
    "        source_table_name: Source table name (e.g., \"SUBSCRIPTION_ACCEL.PYTHON_TESTING.FACT_TABLE_AMSTERDAM_INPUT_100K_24JUNE_MASTER\")\n",
    "        target_table_name: Target table name (e.g., \"SUBSCRIPTION_ACCEL.PYTHON_TESTING.AMSTERDAM_INPUT_100K_24JUNE_MASTER_DIMENSION_DIM\")\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The aggregated result that can be saved to a table\n",
    "    \"\"\"\n",
    "    \n",
    "    # Helper function to get column reference\n",
    "    def get_col(df, col_name):\n",
    "        \"\"\"Get column reference, handling both quoted and unquoted identifiers\"\"\"\n",
    "        columns = df.columns\n",
    "        \n",
    "        # Try exact match first\n",
    "        if col_name in columns:\n",
    "            return col(col_name)\n",
    "        \n",
    "        # Try with quotes\n",
    "        quoted_name = f'\"{col_name}\"'\n",
    "        if quoted_name in columns:\n",
    "            return col(quoted_name)\n",
    "        \n",
    "        # Try case variations\n",
    "        for c in columns:\n",
    "            if c.upper() == col_name.upper():\n",
    "                return col(c)\n",
    "        \n",
    "        # If not found, return the original and let Snowflake handle the error\n",
    "        return col(col_name)\n",
    "    \n",
    "    # print(f\"Reading source table: {source_table_name}\")\n",
    "    \n",
    "    # Read the source table\n",
    "    df = session.table(source_table_name)\n",
    "    \n",
    "    # print(\"Available columns:\", df.columns)\n",
    "    \n",
    "    # Group by all columns except \"Boomerang flag\" and aggregate\n",
    "    result_df = df.groupBy(\n",
    "        get_col(df, \"CustomerID\"),\n",
    "        get_col(df, \"Account Size\"),\n",
    "        get_col(df, \"Revenue Type\"),\n",
    "        get_col(df, \"Region\"),\n",
    "        get_col(df, \"Industry\"),\n",
    "        get_col(df, \"Channel\"),\n",
    "        get_col(df, \"Cohort Date\"),\n",
    "        get_col(df, \"Product Category\"),\n",
    "        get_col(df, \"Product\")\n",
    "    ).agg(\n",
    "        spark_max(get_col(df, \"Boomerang flag\")).alias(\"Boomerang flag\")\n",
    "    ).select(\n",
    "        # Explicitly select all columns to match the original SQL order\n",
    "        get_col(df, \"CustomerID\"),\n",
    "        get_col(df, \"Account Size\"),\n",
    "        get_col(df, \"Revenue Type\"),\n",
    "        get_col(df, \"Region\"),\n",
    "        get_col(df, \"Industry\"),\n",
    "        get_col(df, \"Channel\"),\n",
    "        col(\"Boomerang flag\"),  # This comes from the aggregation\n",
    "        get_col(df, \"Cohort Date\"),\n",
    "        get_col(df, \"Product Category\"),\n",
    "        get_col(df, \"Product\")\n",
    "    )\n",
    "    \n",
    "    # print(f\"Creating/replacing table: {target_table_name}\")\n",
    "    \n",
    "    # Write the result to the target table (equivalent to CREATE OR REPLACE TABLE)\n",
    "    result_df.write.mode(\"overwrite\").saveAsTable(target_table_name)\n",
    "    \n",
    "    print(f\"___Saving on path:  {target_table_name}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a session already created\n",
    "\n",
    "# Source and target table names\n",
    "# SOURCE_TABLE = fact_table_output_path\n",
    "# TARGET_TABLE = fact_table_output_path + \"_DIMENSION_DIM\"\n",
    "\n",
    "# Call the function\n",
    "# result_df = create_master_dimension_table(session, SOURCE_TABLE, TARGET_TABLE)\n",
    "\n",
    "# Display the results (optional)\n",
    "# result_df.show()\n",
    "\n",
    "# To get row count\n",
    "# print(f\"Total rows in result: {result_df.count()}\")\n",
    "\n",
    "\n",
    "def create_date_dimension_table(session: Session, source_table_name: str, target_table_name: str, row_count: int = 10000):\n",
    "    \"\"\"\n",
    "    Convert the SQL CREATE TABLE query to Snowpark using Snowflake DataFrames\n",
    "    Creates a date dimension table with sequential dates from min to max date in source table\n",
    "    \n",
    "    Args:\n",
    "        session: Snowpark session\n",
    "        source_table_name: Source table name (e.g., \"SUBSCRIPTION_ACCEL.PYTHON_TESTING.FACT_TABLE_AMSTERDAM_INPUT_100K_24JUNE_MASTER\")\n",
    "        target_table_name: Target table name (e.g., \"SUBSCRIPTION_ACCEL.PYTHON_TESTING.AMSTERDAM_INPUT_100K_24JUNE_MASTER_DATE_DIM\")\n",
    "        row_count: Number of rows to generate in the sequence (default: 10000)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The date dimension result that can be saved to a table\n",
    "    \"\"\"\n",
    "    \n",
    "    # Helper function to get column reference\n",
    "    def get_col(df, col_name):\n",
    "        \"\"\"Get column reference, handling both quoted and unquoted identifiers\"\"\"\n",
    "        columns = df.columns\n",
    "        \n",
    "        # Try exact match first\n",
    "        if col_name in columns:\n",
    "            return col(col_name)\n",
    "        \n",
    "        # Try with quotes\n",
    "        quoted_name = f'\"{col_name}\"'\n",
    "        if quoted_name in columns:\n",
    "            return col(quoted_name)\n",
    "        \n",
    "        # Try case variations\n",
    "        for c in columns:\n",
    "            if c.upper() == col_name.upper():\n",
    "                return col(c)\n",
    "        \n",
    "        # If not found, return the original and let Snowflake handle the error\n",
    "        return col(col_name)\n",
    "    \n",
    "    # print(f\"Reading source table: {source_table_name}\")\n",
    "    \n",
    "    # Read the source table\n",
    "    df = session.table(source_table_name)\n",
    "    \n",
    "    # print(\"Available columns:\", df.columns)\n",
    "    \n",
    "    # Step 1: Min_Date_Summary CTE - Get minimum date from source table\n",
    "    # print(\"Step 1: Getting minimum date...\")\n",
    "    min_date_df = df.select(spark_min(get_col(df, \"Date\")).alias(\"Min_Date\"))\n",
    "    min_date_value = min_date_df.collect()[0][\"MIN_DATE\"]\n",
    "    # print(f\"Minimum date found: {min_date_value}\")\n",
    "    \n",
    "    # Step 2 & 3: CTE_MY_DATE - Generate date sequence using GENERATOR and SEQ4\n",
    "    # print(f\"Step 2-3: Generating date sequence with {row_count} rows...\")\n",
    "    \n",
    "    # Create generator table with specified row count\n",
    "    # In Snowpark, we need to use the table function approach\n",
    "    generator_df = session.sql(f\"SELECT SEQ4() as seq_num FROM TABLE(GENERATOR(ROWCOUNT=>{row_count}))\")\n",
    "    \n",
    "    # Add months to min_date using seq_num values\n",
    "    date_sequence_df = generator_df.select(\n",
    "        dateadd(\"month\", col(\"seq_num\"), lit(min_date_value)).alias(\"DATE\")\n",
    "    )\n",
    "    \n",
    "    # Step 4: Get max date from source where TTM <> 0 for filtering\n",
    "    # print(\"Step 4: Getting maximum TTM date for filtering...\")\n",
    "    max_ttm_date_df = df.filter(get_col(df, \"TTM\") != 0).select(\n",
    "        spark_max(get_col(df, \"Date\")).alias(\"Max_TTM_Date\")\n",
    "    )\n",
    "    max_ttm_date_value = max_ttm_date_df.collect()[0][\"MAX_TTM_DATE\"]\n",
    "    # print(f\"Maximum TTM date found: {max_ttm_date_value}\")\n",
    "    \n",
    "    # Step 4: Date_Dimension CTE - Filter dates and add row numbers\n",
    "    # print(\"Step 4: Creating date dimension with filtering and row numbers...\")\n",
    "    \n",
    "    # Filter dates <= max TTM date\n",
    "    filtered_dates_df = date_sequence_df.filter(col(\"DATE\") <= lit(max_ttm_date_value))\n",
    "    \n",
    "    # Add row number and TTM Date columns\n",
    "    window_spec = Window.orderBy(col(\"DATE\"))\n",
    "    \n",
    "    date_dimension_df = filtered_dates_df.select(\n",
    "        col(\"DATE\").alias(\"Date\"),\n",
    "        row_number().over(window_spec).alias(\"INDEX\"),\n",
    "        col(\"DATE\").alias(\"TTM Date\")\n",
    "    )\n",
    "    \n",
    "    # Final step: Order by Date ASC (equivalent to ORDER BY \"Date\" ASC)\n",
    "    # print(\"Final step: Ordering results by Date...\")\n",
    "    final_result_df = date_dimension_df.orderBy(col(\"Date\").asc())\n",
    "    \n",
    "    # print(f\"Creating/replacing table: {target_table_name}\")\n",
    "    \n",
    "    # Write the result to the target table (equivalent to CREATE TABLE)\n",
    "    final_result_df.write.mode(\"overwrite\").saveAsTable(target_table_name)\n",
    "    \n",
    "    print(f\"___Saving on path:  {target_table_name}\")\n",
    "    \n",
    "    return final_result_df\n",
    "\n",
    "# Example usage:\n",
    "# Source and target table names\n",
    "# SOURCE_TABLE = fact_table_output_path\n",
    "# TARGET_TABLE = fact_table_output_path + \"_DATE_DIM\"\n",
    "\n",
    "# Call the function to create the table\n",
    "# result_df = create_date_dimension_table(session, SOURCE_TABLE, TARGET_TABLE, row_count=10000)\n",
    "\n",
    "# Display the results (optional)\n",
    "# result_df.show(20)  # Show first 20 rows\n",
    "\n",
    "# To get row count\n",
    "# print(f\"Total rows in date dimension: {result_df.count()}\")\n",
    "\n",
    "# You can also customize the row count if needed\n",
    "# result_df = create_date_dimension_table(session, SOURCE_TABLE, TARGET_TABLE, row_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4e86f-046e-4bfe-98c0-a3d6de19877f",
   "metadata": {
    "language": "python",
    "name": "rename_column"
   },
   "outputs": [],
   "source": [
    "def rename_fixed_tables(session):\n",
    "    tables_and_renames = {\n",
    "        fact_table_output_path: [(\"REGION\", \"Region\"), (\"INDUSTRY\", \"Industry\"), (\"CHANNEL\", \"Channel\")],\n",
    "        fact_table_output_path + \"_DATE_DIM\": [(\"DATE\", \"Date\")],\n",
    "        fact_table_output_path + \"_DIMENSION_DIM\": [(\"REGION\", \"Region\"), (\"INDUSTRY\", \"Industry\"), (\"CHANNEL\", \"Channel\")]\n",
    "    }\n",
    "\n",
    "    for table_name, rename_list in tables_and_renames.items():\n",
    "        print(f\"\\n🔧 Renaming columns in: {table_name}\")\n",
    "        df = session.table(table_name)\n",
    "        actual_cols = df.columns\n",
    "        col_map = {}\n",
    "\n",
    "        for old_col, new_col in rename_list:\n",
    "            match = next((ac for ac in actual_cols if ac.upper() == old_col.upper()), None)\n",
    "            if match:\n",
    "                col_map[match] = new_col\n",
    "            else:\n",
    "                print(f\"⚠️ Column '{old_col}' not found in {table_name}\")\n",
    "\n",
    "        for src, tgt in col_map.items():\n",
    "            df = df.with_column_renamed(src, f'\"{tgt}\"')  # Quote to preserve casing\n",
    "\n",
    "        df.write.mode(\"overwrite\").save_as_table(table_name)\n",
    "        print(f\"✅ Updated: {table_name}\")\n",
    "\n",
    "\n",
    "def rename_pbi_tables(session, retention_levels):\n",
    "    for level in retention_levels:\n",
    "        print(f\"\\n🔄 Processing PBI table for: {level}\")\n",
    "        \n",
    "        suffix = {\n",
    "            \"Customer_level\": \"_C_NOTEBOOK\",\n",
    "            \"Customer_Product_level\": \"_CP\",\n",
    "            \"Customer_Product_RetentionType_level\": \"_CPR\"\n",
    "        }.get(level)\n",
    "        \n",
    "        if not suffix:\n",
    "            print(f\"⚠️ Unknown retention level: {level}\")\n",
    "            continue\n",
    "\n",
    "        pbi_table = pbi_retention_output_path + suffix\n",
    "        df = session.table(pbi_table)\n",
    "        actual_cols = df.columns\n",
    "        col_map = {}\n",
    "\n",
    "        # Rename: \"Cohort Date\" → \"Cohort_Date\", WINBACKTAG → WinbackTag\n",
    "        rename_targets = [(\"Cohort Date\", \"Cohort_Date\"), (\"WINBACKTAG\", \"WinbackTag\")]\n",
    "\n",
    "        for old_col, new_col in rename_targets:\n",
    "            match = next((ac for ac in actual_cols if ac.replace('\"', '').upper() == old_col.upper()), None)\n",
    "            if match:\n",
    "                col_map[match] = new_col\n",
    "            else:\n",
    "                print(f\"⚠️ Column '{old_col}' not found in {pbi_table}, skipping.\")\n",
    "\n",
    "        for src, tgt in col_map.items():\n",
    "            df = df.with_column_renamed(src, f'\"{tgt}\"')  # Use quoted column names to preserve case\n",
    "\n",
    "        df.write.mode(\"overwrite\").save_as_table(pbi_table)\n",
    "        print(f\"✅ Updated PBI table: {pbi_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678b9bb-4629-4117-a6a8-07ab59e1025a",
   "metadata": {
    "language": "python",
    "name": "ufr"
   },
   "outputs": [],
   "source": [
    "def UFR_C_MoM(session: Session, table_name: str, column_mapping: dict = None, filter_date: str = \"2024-04-01\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        session: Snowpark session\n",
    "        table_name: Name of the input table\n",
    "        column_mapping: Dictionary mapping standard column names to actual column names in the input table\n",
    "        filter_date: Date filter for final result (format: 'YYYY-MM-DD')\n",
    "    \n",
    "    Returns:\n",
    "        Snowpark DataFrame with the final result\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use default mapping if none provided\n",
    "    if column_mapping is None:\n",
    "        column_mapping = Column_mapping_UFR\n",
    "    \n",
    "    # Read the input table\n",
    "    df = session.table(table_name)\n",
    "    \n",
    "    # Helper function to get column reference with mapping support\n",
    "    def get_mapped_col(standard_col_name, default_value=None):\n",
    "        \"\"\"\n",
    "        Get column reference using the mapping, with support for missing columns\n",
    "        \n",
    "        Args:\n",
    "            standard_col_name: The standard column key from the mapping\n",
    "            default_value: Default value to use if column is missing (default: None -> lit(\"NA\") for strings, 0 for numbers)\n",
    "        \n",
    "        Returns:\n",
    "            Column expression or literal value\n",
    "        \"\"\"\n",
    "        mapped_name = column_mapping.get(standard_col_name)\n",
    "        \n",
    "        if mapped_name is None:\n",
    "            # Column not present in input, return default value\n",
    "            if default_value is not None:\n",
    "                return lit(default_value)\n",
    "            elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "                return lit(0)  # Default for numeric columns\n",
    "            else:\n",
    "                return lit(\"NA\")  # Default for text columns\n",
    "        \n",
    "        # Column is mapped, try to get it from the DataFrame\n",
    "        columns = df.columns\n",
    "        \n",
    "        # Remove quotes for comparison\n",
    "        clean_mapped_name = mapped_name.strip('\"')\n",
    "        \n",
    "        # Try exact match first\n",
    "        if mapped_name in columns:\n",
    "            return col(mapped_name)\n",
    "        elif clean_mapped_name in columns:\n",
    "            return col(clean_mapped_name)\n",
    "        \n",
    "        # Try case-insensitive match\n",
    "        for c in columns:\n",
    "            if c.upper() == clean_mapped_name.upper():\n",
    "                return col(c)\n",
    "        \n",
    "        # If not found, return default value and print warning\n",
    "        print(f\"Warning: Column {mapped_name} not found in table. Using default value.\")\n",
    "        if default_value is not None:\n",
    "            return lit(default_value)\n",
    "        elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "            return lit(0)\n",
    "        else:\n",
    "            return lit(\"NA\")\n",
    "    \n",
    "    def validate_required_columns():\n",
    "        \"\"\"Validate that all required columns are available\"\"\"\n",
    "        required_columns = [\"DATE\", \"START_DATE\", \"END_DATE\", \"CUSTOMERID\", \"MRR\"]\n",
    "        missing_required = []\n",
    "        \n",
    "        for req_col in required_columns:\n",
    "            if column_mapping.get(req_col) is None:\n",
    "                missing_required.append(req_col)\n",
    "        \n",
    "        if missing_required:\n",
    "            raise ValueError(f\"Required columns are missing from column mapping: {missing_required}\")\n",
    "        \n",
    "        print(\"✓ All required columns are mapped\")\n",
    "        return True\n",
    "    \n",
    "    # Validate required columns\n",
    "    validate_required_columns()\n",
    "    \n",
    "    # Print column mapping summary\n",
    "    print(\"\\n=== Column Mapping Summary ===\")\n",
    "    for std_name, mapped_name in column_mapping.items():\n",
    "        status = \"✓ Mapped\" if mapped_name is not None else \"✗ Missing (using default)\"\n",
    "        mapped_display = mapped_name if mapped_name is not None else \"None\"\n",
    "        print(f\"{std_name:20} -> {mapped_display:20} {status}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: cleaned_data CTE\n",
    "    def clean_data(df):\n",
    "        print(\"Step 1: Cleaning data...\")\n",
    "        \n",
    "        cleaned = df.select(\n",
    "            # Convert date columns using mapping\n",
    "            get_mapped_col(\"DATE\").alias(\"Date\"),\n",
    "            get_mapped_col(\"START_DATE\").alias(\"Start_Date\"),\n",
    "            get_mapped_col(\"END_DATE\").alias(\"End_Date\"),\n",
    "            \n",
    "            # Clean and standardize text columns using mapping\n",
    "            (trim(coalesce(get_mapped_col(\"CUSTOMERID\"), lit(\"NA\")))).alias(\"CustomerID\"),\n",
    "            (trim(coalesce(get_mapped_col(\"UP_SNL_ID\"), lit(\"NA\")))).alias(\"up_snl_id\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT\"), lit(\"NA\")))).alias(\"PRODUCT\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT_GROUP\"), lit(\"NA\")))).alias(\"PRODUCT_GROUP\"),\n",
    "            (trim(coalesce(get_mapped_col(\"OPPORTUNITY_NUMBER\"), lit(\"NA\")))).alias(\"opportunity_number\"),\n",
    "            (trim(coalesce(get_mapped_col(\"REVENUE_TYPE\"), lit(\"NA\")))).alias(\"Revenue_Type\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TAGGING\"), lit(\"NA\")))).alias(\"Tagging\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TIER\"), lit(\"NA\")))).alias(\"Tier\"),\n",
    "            (trim(coalesce(get_mapped_col(\"SEGMENT\"), lit(\"NA\")))).alias(\"Segment\"),\n",
    "            (trim(coalesce(get_mapped_col(\"Region\"), lit(\"NA\")))).alias(\"Region\"),\n",
    "            \n",
    "            # Handle numeric columns using mapping\n",
    "            coalesce(get_mapped_col(\"MRR\"), lit(0)).alias(\"MRR\"),\n",
    "            coalesce(get_mapped_col(\"CARR\"), lit(0)).alias(\"CARR\"),\n",
    "            coalesce(get_mapped_col(\"ADJUSTED_ARR\"), lit(0)).alias(\"Adjusted_ARR\")\n",
    "        ).filter(\n",
    "            # Apply filters\n",
    "            (col(\"End_Date\").isNotNull()) & \n",
    "            (col(\"MRR\") != 0)\n",
    "        )\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    # Step 2: contract_summary CTE\n",
    "    def create_contract_summary(cleaned_data):\n",
    "        print(\"Step 2: Creating contract summary...\")\n",
    "        \n",
    "        contract_summary = cleaned_data.groupBy(\"CustomerID\", \"Start_Date\", \"End_Date\").agg(\n",
    "            spark_min(\"Date\").alias(\"Min_Date\"),\n",
    "            spark_max(\"Date\").alias(\"Max_Date\")\n",
    "        )\n",
    "        \n",
    "        # Calculate contract size\n",
    "        contract_summary = contract_summary.withColumn(\n",
    "            \"Contract_Size\",\n",
    "            when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 1, \"1. Less than 1 year\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 2, \"2. 1-2 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 3, \"3. 2-3 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 4, \"4. 3-4 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 5, \"5. 4-5 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) >= 5, \"6. 5+ years\")\n",
    "            .otherwise(\"NA\")\n",
    "        )\n",
    "        \n",
    "        return contract_summary\n",
    "    \n",
    "    # Step 3: summary_a and summary_b CTEs\n",
    "    def create_summaries(cleaned_data):\n",
    "        print(\"Step 3: Creating summaries...\")\n",
    "        \n",
    "        summary_a = cleaned_data.select(\"CustomerID\", \"Start_Date\", \"End_Date\").distinct()\n",
    "        summary_b = cleaned_data.select(\"Date\").distinct()\n",
    "        \n",
    "        return summary_a, summary_b\n",
    "    \n",
    "    # Step 4: cartesian_data CTE\n",
    "    def create_cartesian_data(summary_a, summary_b):\n",
    "        print(\"Step 4: Creating cartesian data...\")\n",
    "        \n",
    "        # Cross join to create cartesian product\n",
    "        cartesian = summary_a.crossJoin(summary_b)\n",
    "        \n",
    "        return cartesian\n",
    "    \n",
    "    # Step 5: summary_mrr CTE\n",
    "    def create_summary_mrr(cleaned_data):\n",
    "        print(\"Step 5: Creating summary MRR...\")\n",
    "        \n",
    "        summary_mrr = cleaned_data.groupBy(\"CustomerID\", \"Start_Date\", \"End_Date\", \"Date\").agg(\n",
    "            spark_sum(\"MRR\").alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return summary_mrr\n",
    "    \n",
    "    # Step 6: joined_data CTE\n",
    "    def create_joined_data(cartesian_data, summary_mrr):\n",
    "        print(\"Step 6: Creating joined data...\")\n",
    "        \n",
    "        joined = cartesian_data.join(\n",
    "            summary_mrr,\n",
    "            [\"CustomerID\", \"Start_Date\", \"End_Date\", \"Date\"],\n",
    "            \"left\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Date\"),\n",
    "            coalesce(col(\"Sum_MRR\"), lit(0)).alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return joined\n",
    "    \n",
    "    # Step 7: joined_data2 CTE\n",
    "    def create_joined_data2(contract_summary, joined_data):\n",
    "        print(\"Step 7: Creating joined data 2...\")\n",
    "        \n",
    "        joined_data2 = joined_data.join(\n",
    "            contract_summary.select(\"CustomerID\", \"Start_Date\", \"End_Date\", \"Max_Date\"),\n",
    "            [\"CustomerID\", \"Start_Date\", \"End_Date\"],\n",
    "            \"inner\"\n",
    "        )\n",
    "        \n",
    "        return joined_data2\n",
    "    \n",
    "    # Step 8: with_next_date CTE\n",
    "    def create_with_next_date(joined_data2):\n",
    "        print(\"Step 8: Adding next date...\")\n",
    "        \n",
    "        with_next_date = joined_data2.withColumn(\n",
    "            \"Next_Date\",\n",
    "            date_trunc(\"month\", dateadd(\"day\", lit(1), col(\"End_Date\")))\n",
    "        ).withColumn(\n",
    "            \"Sum_MRR_Adjusted\",\n",
    "            when(\n",
    "                dateadd(\"month\", lit(1), col(\"Max_Date\")) < col(\"Next_Date\"),\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"Sum_MRR\"))\n",
    "        )\n",
    "        \n",
    "        return with_next_date\n",
    "    \n",
    "    # Step 9: sorted_data CTE\n",
    "    def create_sorted_data(with_next_date):\n",
    "        print(\"Step 9: Sorting data...\")\n",
    "        \n",
    "        sorted_data = with_next_date.orderBy(\"CustomerID\", \"Start_Date\", \"End_Date\", \"Date\")\n",
    "        \n",
    "        return sorted_data\n",
    "    \n",
    "    # Step 10: rolling_sums CTE\n",
    "    def create_rolling_sums(sorted_data):\n",
    "        print(\"Step 10: Creating rolling sums...\")\n",
    "        \n",
    "        window_spec = Window.partitionBy(\"CustomerID\", \"Start_Date\", \"End_Date\").orderBy(\"Date\")\n",
    "        \n",
    "        rolling_sums = sorted_data.withColumn(\n",
    "            \"UFR_TTM\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-11, 0))\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-2, 0))\n",
    "        )\n",
    "        \n",
    "        return rolling_sums\n",
    "    \n",
    "    # Step 11: final_metrics CTE\n",
    "    def create_final_metrics(rolling_sums):\n",
    "        print(\"Step 11: Creating final metrics...\")\n",
    "        \n",
    "        final_metrics = rolling_sums.withColumn(\n",
    "            \"UFR_ARR\", col(\"Sum_MRR_Adjusted\") * 12\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M_Annualized\", col(\"UFR_TTM\") * 4\n",
    "        ).filter(\n",
    "            (col(\"Sum_MRR_Adjusted\") != 0) |\n",
    "            (col(\"UFR_TTM\") != 0) |\n",
    "            (col(\"UFR_T3M\") != 0)\n",
    "        )\n",
    "        \n",
    "        return final_metrics\n",
    "    \n",
    "    # Step 12: joined_data3 CTE\n",
    "    def create_joined_data3(contract_summary, final_metrics):\n",
    "        print(\"Step 12: Creating joined data 3...\")\n",
    "        \n",
    "        # Filter final_metrics where Date equals Max_Date\n",
    "        filtered_metrics = final_metrics.filter(col(\"Date\") == col(\"Max_Date\"))\n",
    "        \n",
    "        joined_data3 = contract_summary.join(\n",
    "            filtered_metrics,\n",
    "            [\"CustomerID\", \"Start_Date\", \"End_Date\", \"Max_Date\"],\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Max_Date\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"Date\"),\n",
    "            col(\"Sum_MRR_Adjusted\").alias(\"UFR_Amount\"),\n",
    "            col(\"UFR_TTM\"),\n",
    "            col(\"UFR_T3M\"),\n",
    "            col(\"UFR_ARR\"),\n",
    "            col(\"UFR_T3M_Annualized\"),\n",
    "            col(\"Next_Date\")\n",
    "        )\n",
    "        \n",
    "        return joined_data3\n",
    "    \n",
    "    # Step 13: unioneddata2 CTE\n",
    "    def create_unioneddata2(joined_data3):\n",
    "        print(\"Step 13: Creating unioned data...\")\n",
    "        \n",
    "        # Create separate DataFrames for each UNION\n",
    "        mrr_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"MRR\"))\n",
    "        \n",
    "        arr_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_ARR\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"ARR\"))\n",
    "        \n",
    "        t3m_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M\"))\n",
    "        \n",
    "        ttm_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_TTM\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"TTM\"))\n",
    "        \n",
    "        t3m_ann_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M_Annualized\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M (Annualized)\"))\n",
    "        \n",
    "        # Union all data\n",
    "        unioneddata2 = mrr_data.unionAll(arr_data).unionAll(t3m_data).unionAll(ttm_data).unionAll(t3m_ann_data)\n",
    "        \n",
    "        return unioneddata2\n",
    "    \n",
    "    # Step 14: main_data1 CTE\n",
    "    def create_main_data1(unioneddata2):\n",
    "        print(\"Step 14: Creating main data...\")\n",
    "        \n",
    "        main_data1 = unioneddata2.withColumn(\n",
    "            \"Min_Date\", date_trunc(\"month\", col(\"Min_Date\"))\n",
    "        ).withColumn(\n",
    "            \"Next_Date\", date_trunc(\"month\", col(\"Next_Date\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount\") >= 0\n",
    "        )\n",
    "        \n",
    "        return main_data1\n",
    "    \n",
    "    # Step 15: aggregated_data CTE\n",
    "    def create_aggregated_data(main_data1):\n",
    "        print(\"Step 15: Creating aggregated data...\")\n",
    "        \n",
    "        aggregated_data = main_data1.groupBy(\"CustomerID\", \"Next_Date\", \"Amount\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        ).withColumnRenamed(\"Next_Date\", \"UFR_Date\")\n",
    "        \n",
    "        return aggregated_data\n",
    "    \n",
    "    # Step 16: Final SELECT\n",
    "    def create_final_result(aggregated_data):\n",
    "        print(\"Step 16: Creating final result...\")\n",
    "        \n",
    "        # Filter UFR Date < filter_date (configurable parameter)\n",
    "        final_result = aggregated_data.filter(\n",
    "            col(\"UFR_Date\") < lit(filter_date)\n",
    "        ).groupBy(\"CustomerID\", \"UFR_Date\", \"Amount\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "\n",
    "        if input_amount == \"ARR\":\n",
    "            final_result = final_result.select(\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                (col(\"UFR_Amount\") / 12).alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "        )\n",
    "        else:  # For MRR\n",
    "            final_result = final_result.select(\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                col(\"UFR_Amount\").alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "            )\n",
    "            \n",
    "        return final_result\n",
    "    \n",
    "    # Execute all steps\n",
    "    print(f\"\\n🚀 Starting UFR_C_MoM processing for table: {table_name}\")\n",
    "    # print(f\"📅 Filter date: {filter_date}\")\n",
    "    \n",
    "    cleaned_data = clean_data(df)\n",
    "    contract_summary = create_contract_summary(cleaned_data)\n",
    "    summary_a, summary_b = create_summaries(cleaned_data)\n",
    "    cartesian_data = create_cartesian_data(summary_a, summary_b)\n",
    "    summary_mrr = create_summary_mrr(cleaned_data)\n",
    "    joined_data = create_joined_data(cartesian_data, summary_mrr)\n",
    "    joined_data2 = create_joined_data2(contract_summary, joined_data)\n",
    "    with_next_date = create_with_next_date(joined_data2)\n",
    "    sorted_data = create_sorted_data(with_next_date)\n",
    "    rolling_sums = create_rolling_sums(sorted_data)\n",
    "    final_metrics = create_final_metrics(rolling_sums)\n",
    "    joined_data3 = create_joined_data3(contract_summary, final_metrics)\n",
    "    unioneddata2 = create_unioneddata2(joined_data3)\n",
    "    main_data1 = create_main_data1(unioneddata2)\n",
    "    aggregated_data = create_aggregated_data(main_data1)\n",
    "    final_result = create_final_result(aggregated_data)\n",
    "    \n",
    "    print(\"✅ UFR_C_MoM processing completed successfully!\")\n",
    "    return final_result\n",
    "\n",
    "# result_df = UFR_C_MoM(session, \"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST\", Column_mapping_UFR)\n",
    "\n",
    "# result_df.write.mode(\"overwrite\").saveAsTable(\"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST_C_MoM\")\n",
    "\n",
    "\n",
    "\n",
    "def UFR_C_QoQ(session: Session, table_name: str, column_mapping: dict = None, filter_date: str = \"2024-04-01\"):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        session: Snowpark session\n",
    "        table_name: Name of the input table\n",
    "        column_mapping: Dictionary mapping standard column names to actual column names in the input table\n",
    "        filter_date: Date filter for final result (format: 'YYYY-MM-DD')\n",
    "    \n",
    "    Returns:\n",
    "        Snowpark DataFrame with the final result\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use default mapping if none provided\n",
    "    if column_mapping is None:\n",
    "        column_mapping = Column_mapping_UFR\n",
    "    \n",
    "    # Read the input table\n",
    "    df = session.table(table_name)\n",
    "    \n",
    "    # Helper function to get column reference with mapping support\n",
    "    def get_mapped_col(standard_col_name, default_value=None):\n",
    "        \"\"\"\n",
    "        Get column reference using the mapping, with support for missing columns\n",
    "        \n",
    "        Args:\n",
    "            standard_col_name: The standard column key from the mapping\n",
    "            default_value: Default value to use if column is missing (default: None -> lit(\"NA\") for strings, 0 for numbers)\n",
    "        \n",
    "        Returns:\n",
    "            Column expression or literal value\n",
    "        \"\"\"\n",
    "        mapped_name = column_mapping.get(standard_col_name)\n",
    "        \n",
    "        if mapped_name is None:\n",
    "            # Column not present in input, return default value\n",
    "            if default_value is not None:\n",
    "                return lit(default_value)\n",
    "            elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "                return lit(0)  # Default for numeric columns\n",
    "            else:\n",
    "                return lit(\"NA\")  # Default for text columns\n",
    "        \n",
    "        # Column is mapped, try to get it from the DataFrame\n",
    "        columns = df.columns\n",
    "        \n",
    "        # Remove quotes for comparison\n",
    "        clean_mapped_name = mapped_name.strip('\"')\n",
    "        \n",
    "        # Try exact match first\n",
    "        if mapped_name in columns:\n",
    "            return col(mapped_name)\n",
    "        elif clean_mapped_name in columns:\n",
    "            return col(clean_mapped_name)\n",
    "        \n",
    "        # Try case-insensitive match\n",
    "        for c in columns:\n",
    "            if c.upper() == clean_mapped_name.upper():\n",
    "                return col(c)\n",
    "        \n",
    "        # If not found, return default value and print warning\n",
    "        print(f\"Warning: Column {mapped_name} not found in table. Using default value.\")\n",
    "        if default_value is not None:\n",
    "            return lit(default_value)\n",
    "        elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "            return lit(0)\n",
    "        else:\n",
    "            return lit(\"NA\")\n",
    "    \n",
    "    def validate_required_columns():\n",
    "        \"\"\"Validate that all required columns are available\"\"\"\n",
    "        required_columns = [\"DATE\", \"START_DATE\", \"END_DATE\", \"CUSTOMERID\", \"MRR\"]\n",
    "        missing_required = []\n",
    "        \n",
    "        for req_col in required_columns:\n",
    "            if column_mapping.get(req_col) is None:\n",
    "                missing_required.append(req_col)\n",
    "        \n",
    "        if missing_required:\n",
    "            raise ValueError(f\"Required columns are missing from column mapping: {missing_required}\")\n",
    "        \n",
    "        print(\"✓ All required columns are mapped\")\n",
    "        return True\n",
    "    \n",
    "    # Validate required columns\n",
    "    validate_required_columns()\n",
    "    \n",
    "    # Print column mapping summary\n",
    "    print(\"\\n=== Column Mapping Summary ===\")\n",
    "    for std_name, mapped_name in column_mapping.items():\n",
    "        status = \"✓ Mapped\" if mapped_name is not None else \"✗ Missing (using default)\"\n",
    "        mapped_display = mapped_name if mapped_name is not None else \"None\"\n",
    "        print(f\"{std_name:20} -> {mapped_display:20} {status}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: cleaned_data CTE\n",
    "    def clean_data(df):\n",
    "        print(\"Step 1: Cleaning data...\")\n",
    "        \n",
    "        cleaned = df.select(\n",
    "            # Convert date columns using mapping\n",
    "            get_mapped_col(\"DATE\").alias(\"Date\"),\n",
    "            get_mapped_col(\"START_DATE\").alias(\"Start_Date\"),\n",
    "            get_mapped_col(\"END_DATE\").alias(\"End_Date\"),\n",
    "            \n",
    "            # Clean and standardize text columns using mapping\n",
    "            (trim(coalesce(get_mapped_col(\"CUSTOMERID\"), lit(\"NA\")))).alias(\"CustomerID\"),\n",
    "            (trim(coalesce(get_mapped_col(\"UP_SNL_ID\"), lit(\"NA\")))).alias(\"up_snl_id\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT\"), lit(\"NA\")))).alias(\"PRODUCT\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT_GROUP\"), lit(\"NA\")))).alias(\"PRODUCT_GROUP\"),\n",
    "            (trim(coalesce(get_mapped_col(\"OPPORTUNITY_NUMBER\"), lit(\"NA\")))).alias(\"opportunity_number\"),\n",
    "            (trim(coalesce(get_mapped_col(\"REVENUE_TYPE\"), lit(\"NA\")))).alias(\"Revenue_Type\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TAGGING\"), lit(\"NA\")))).alias(\"Tagging\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TIER\"), lit(\"NA\")))).alias(\"Tier\"),\n",
    "            (trim(coalesce(get_mapped_col(\"SEGMENT\"), lit(\"NA\")))).alias(\"Segment\"),\n",
    "            (trim(coalesce(get_mapped_col(\"Region\"), lit(\"NA\")))).alias(\"Region\"),\n",
    "            \n",
    "            # Handle numeric columns using mapping\n",
    "            coalesce(get_mapped_col(\"MRR\"), lit(0)).alias(\"MRR\"),\n",
    "            coalesce(get_mapped_col(\"CARR\"), lit(0)).alias(\"CARR\"),\n",
    "            coalesce(get_mapped_col(\"ADJUSTED_ARR\"), lit(0)).alias(\"Adjusted_ARR\")\n",
    "        ).filter(\n",
    "            # Apply filters\n",
    "            (col(\"End_Date\").isNotNull()) & \n",
    "            (col(\"MRR\") != 0)\n",
    "        )\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    # Step 2: contract_summary CTE\n",
    "    def create_contract_summary(cleaned_data):\n",
    "        print(\"Step 2: Creating contract summary...\")\n",
    "        \n",
    "        contract_summary = cleaned_data.groupBy(\"CustomerID\", \"Start_Date\", \"End_Date\").agg(\n",
    "            spark_min(\"Date\").alias(\"Min_Date\"),\n",
    "            spark_max(\"Date\").alias(\"Max_Date\")\n",
    "        )\n",
    "        \n",
    "        # Calculate contract size\n",
    "        contract_summary = contract_summary.withColumn(\n",
    "            \"Contract_Size\",\n",
    "            when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 1, \"1. Less than 1 year\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 2, \"2. 1-2 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 3, \"3. 2-3 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 4, \"4. 3-4 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 5, \"5. 4-5 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) >= 5, \"6. 5+ years\")\n",
    "            .otherwise(\"NA\")\n",
    "        )\n",
    "        \n",
    "        return contract_summary\n",
    "    \n",
    "    # Step 3: summary_a and summary_b CTEs\n",
    "    def create_summaries(cleaned_data):\n",
    "        print(\"Step 3: Creating summaries...\")\n",
    "        \n",
    "        summary_a = cleaned_data.select(\"CustomerID\", \"Start_Date\", \"End_Date\").distinct()\n",
    "        summary_b = cleaned_data.select(\"Date\").distinct()\n",
    "        \n",
    "        return summary_a, summary_b\n",
    "    \n",
    "    # Step 4: cartesian_data CTE\n",
    "    def create_cartesian_data(summary_a, summary_b):\n",
    "        print(\"Step 4: Creating cartesian data...\")\n",
    "        \n",
    "        # Cross join to create cartesian product\n",
    "        cartesian = summary_a.crossJoin(summary_b)\n",
    "        \n",
    "        return cartesian\n",
    "    \n",
    "    # Step 5: summary_mrr CTE\n",
    "    def create_summary_mrr(cleaned_data):\n",
    "        print(\"Step 5: Creating summary MRR...\")\n",
    "        \n",
    "        summary_mrr = cleaned_data.groupBy(\"CustomerID\", \"Start_Date\", \"End_Date\", \"Date\").agg(\n",
    "            spark_sum(\"MRR\").alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return summary_mrr\n",
    "    \n",
    "    # Step 6: joined_data CTE\n",
    "    def create_joined_data(cartesian_data, summary_mrr):\n",
    "        print(\"Step 6: Creating joined data...\")\n",
    "        \n",
    "        joined = cartesian_data.join(\n",
    "            summary_mrr,\n",
    "            [\"CustomerID\", \"Start_Date\", \"End_Date\", \"Date\"],\n",
    "            \"left\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Date\"),\n",
    "            coalesce(col(\"Sum_MRR\"), lit(0)).alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return joined\n",
    "    \n",
    "    # Step 7: joined_data2 CTE\n",
    "    def create_joined_data2(contract_summary, joined_data):\n",
    "        print(\"Step 7: Creating joined data 2...\")\n",
    "        \n",
    "        joined_data2 = joined_data.join(\n",
    "            contract_summary.select(\"CustomerID\", \"Start_Date\", \"End_Date\", \"Max_Date\"),\n",
    "            [\"CustomerID\", \"Start_Date\", \"End_Date\"],\n",
    "            \"inner\"\n",
    "        )\n",
    "        \n",
    "        return joined_data2\n",
    "    \n",
    "    # Step 8: with_next_date CTE\n",
    "    def create_with_next_date(joined_data2):\n",
    "        print(\"Step 8: Adding next date...\")\n",
    "        \n",
    "        with_next_date = joined_data2.withColumn(\n",
    "            \"Next_Date\",\n",
    "            date_trunc(\"month\", dateadd(\"day\", lit(1), col(\"End_Date\")))\n",
    "        ).withColumn(\n",
    "            \"Sum_MRR_Adjusted\",\n",
    "            when(\n",
    "                dateadd(\"month\", lit(1), col(\"Max_Date\")) < col(\"Next_Date\"),\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"Sum_MRR\"))\n",
    "        )\n",
    "        \n",
    "        return with_next_date\n",
    "    \n",
    "    # Step 9: sorted_data CTE\n",
    "    def create_sorted_data(with_next_date):\n",
    "        print(\"Step 9: Sorting data...\")\n",
    "        \n",
    "        sorted_data = with_next_date.orderBy(\"CustomerID\", \"Start_Date\", \"End_Date\", \"Date\")\n",
    "        \n",
    "        return sorted_data\n",
    "    \n",
    "    # Step 10: rolling_sums CTE\n",
    "    def create_rolling_sums(sorted_data):\n",
    "        print(\"Step 10: Creating rolling sums...\")\n",
    "        \n",
    "        window_spec = Window.partitionBy(\"CustomerID\", \"Start_Date\", \"End_Date\").orderBy(\"Date\")\n",
    "        \n",
    "        rolling_sums = sorted_data.withColumn(\n",
    "            \"UFR_TTM\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-11, 0))\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-2, 0))\n",
    "        )\n",
    "        \n",
    "        return rolling_sums\n",
    "    \n",
    "    # Step 11: final_metrics CTE\n",
    "    def create_final_metrics(rolling_sums):\n",
    "        print(\"Step 11: Creating final metrics...\")\n",
    "        \n",
    "        final_metrics = rolling_sums.withColumn(\n",
    "            \"UFR_ARR\", col(\"Sum_MRR_Adjusted\") * 12\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M_Annualized\", col(\"UFR_TTM\") * 4\n",
    "        ).filter(\n",
    "            (col(\"Sum_MRR_Adjusted\") != 0) |\n",
    "            (col(\"UFR_TTM\") != 0) |\n",
    "            (col(\"UFR_T3M\") != 0)\n",
    "        )\n",
    "        \n",
    "        return final_metrics\n",
    "    \n",
    "    # Step 12: joined_data3 CTE\n",
    "    def create_joined_data3(contract_summary, final_metrics):\n",
    "        print(\"Step 12: Creating joined data 3...\")\n",
    "        \n",
    "        # Filter final_metrics where Date equals Max_Date\n",
    "        filtered_metrics = final_metrics.filter(col(\"Date\") == col(\"Max_Date\"))\n",
    "        \n",
    "        joined_data3 = contract_summary.join(\n",
    "            filtered_metrics,\n",
    "            [\"CustomerID\", \"Start_Date\", \"End_Date\", \"Max_Date\"],\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Max_Date\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"Date\"),\n",
    "            col(\"Sum_MRR_Adjusted\").alias(\"UFR_Amount\"),\n",
    "            col(\"UFR_TTM\"),\n",
    "            col(\"UFR_T3M\"),\n",
    "            col(\"UFR_ARR\"),\n",
    "            col(\"UFR_T3M_Annualized\"),\n",
    "            col(\"Next_Date\")\n",
    "        )\n",
    "        \n",
    "        return joined_data3\n",
    "    \n",
    "    # Step 13: unioneddata2 CTE\n",
    "    def create_unioneddata2(joined_data3):\n",
    "        print(\"Step 13: Creating unioned data...\")\n",
    "        \n",
    "        # Create separate DataFrames for each UNION\n",
    "        mrr_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"MRR\"))\n",
    "        \n",
    "        arr_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_ARR\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"ARR\"))\n",
    "        \n",
    "        t3m_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M\"))\n",
    "        \n",
    "        ttm_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_TTM\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"TTM\"))\n",
    "        \n",
    "        t3m_ann_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M_Annualized\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M (Annualized)\"))\n",
    "        \n",
    "        # Union all data\n",
    "        unioneddata2 = mrr_data.unionAll(arr_data).unionAll(t3m_data).unionAll(ttm_data).unionAll(t3m_ann_data)\n",
    "        \n",
    "        return unioneddata2\n",
    "    \n",
    "    # Step 14: main_data1 CTE\n",
    "    def create_main_data1(unioneddata2):\n",
    "        print(\"Step 14: Creating main data...\")\n",
    "        \n",
    "        main_data1 = unioneddata2.withColumn(\n",
    "            \"Min_Date\", date_trunc(\"month\", col(\"Min_Date\"))\n",
    "        ).withColumn(\n",
    "            \"Next_Date\", date_trunc(\"month\", col(\"Next_Date\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount\") >= 0\n",
    "        )\n",
    "        \n",
    "        return main_data1\n",
    "    \n",
    "    # Step 15: expanded_dates_QOQ CTE\n",
    "    def create_expanded_dates_qoq(session, main_data1):\n",
    "        print(\"Step 15: Creating expanded dates QoQ...\")\n",
    "        \n",
    "        # Create a simple DataFrame with sequence numbers 0, 1, 2\n",
    "        # Using session.sql to create the generator table\n",
    "        generator_df = session.sql(\"\"\"\n",
    "            SELECT value AS seq FROM (\n",
    "                VALUES (0), (1), (2)\n",
    "            ) AS t(value)\n",
    "        \"\"\")\n",
    "        \n",
    "        # Cross join main_data1 with generator to expand dates\n",
    "        expanded_dates_qoq = main_data1.crossJoin(generator_df).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"Next_Date\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Amount\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"UFR_Amount\"),\n",
    "            dateadd(\"month\", col(\"seq\"), col(\"Next_Date\")).alias(\"UFR_Date\")\n",
    "        ).filter(\n",
    "            # Filter condition: DATEADD(MONTH, g.seq, p.\"Next Date\") <= DATEADD(MONTH, 2, p.\"Next Date\")\n",
    "            col(\"UFR_Date\") <= dateadd(\"month\", lit(2), col(\"Next_Date\"))\n",
    "        )\n",
    "        \n",
    "        return expanded_dates_qoq\n",
    "    \n",
    "    # Step 16: filtered_and_adjusted_QOQ CTE\n",
    "    def create_filtered_and_adjusted_qoq(expanded_dates_qoq):\n",
    "        print(\"Step 16: Creating filtered and adjusted QoQ...\")\n",
    "        \n",
    "        filtered_and_adjusted_qoq = expanded_dates_qoq.withColumn(\n",
    "            \"UFR_Amount_Adjusted\",\n",
    "            when(\n",
    "                col(\"Min_Date\") > dateadd(\"month\", lit(-3), col(\"UFR_Date\")),\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"UFR_Amount\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount_Adjusted\") != 0\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"UFR_Date\"),\n",
    "            col(\"Amount\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"UFR_Amount_Adjusted\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        \n",
    "        return filtered_and_adjusted_qoq\n",
    "    \n",
    "    # Step 17: filtered_and_adjusted_QOQ2 CTE\n",
    "    def create_filtered_and_adjusted_qoq2(filtered_and_adjusted_qoq):\n",
    "        print(\"Step 17: Creating filtered and adjusted QoQ2...\")\n",
    "        \n",
    "        filtered_and_adjusted_qoq2 = filtered_and_adjusted_qoq.filter(\n",
    "            col(\"UFR_Date\") < lit(filter_date)  # Use configurable parameter\n",
    "        ).groupBy(\"CustomerID\", \"UFR_Date\", \"Amount\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        \n",
    "        return filtered_and_adjusted_qoq2\n",
    "    \n",
    "    # Step 18: Final SELECT\n",
    "    def create_final_result(filtered_and_adjusted_qoq2):\n",
    "        print(\"Step 18: Creating final result...\")\n",
    "        \n",
    "        final_result = filtered_and_adjusted_qoq2.groupBy(\"CustomerID\", \"UFR_Date\", \"Amount\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        if input_amount == \"ARR\":\n",
    "            final_result = final_result.select(\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                (col(\"UFR_Amount\") / 12).alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "        )\n",
    "        else:  # For MRR\n",
    "            final_result = final_result.select(\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                col(\"UFR_Amount\").alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "            )\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    # Execute all steps\n",
    "    print(f\"\\n🚀 Starting UFR_C_QoQ processing for table: {table_name}\")\n",
    "    # print(f\"📅 Filter date: {filter_date}\")\n",
    "    \n",
    "    cleaned_data = clean_data(df)\n",
    "    contract_summary = create_contract_summary(cleaned_data)\n",
    "    summary_a, summary_b = create_summaries(cleaned_data)\n",
    "    cartesian_data = create_cartesian_data(summary_a, summary_b)\n",
    "    summary_mrr = create_summary_mrr(cleaned_data)\n",
    "    joined_data = create_joined_data(cartesian_data, summary_mrr)\n",
    "    joined_data2 = create_joined_data2(contract_summary, joined_data)\n",
    "    with_next_date = create_with_next_date(joined_data2)\n",
    "    sorted_data = create_sorted_data(with_next_date)\n",
    "    rolling_sums = create_rolling_sums(sorted_data)\n",
    "    final_metrics = create_final_metrics(rolling_sums)\n",
    "    joined_data3 = create_joined_data3(contract_summary, final_metrics)\n",
    "    unioneddata2 = create_unioneddata2(joined_data3)\n",
    "    main_data1 = create_main_data1(unioneddata2)\n",
    "    \n",
    "    # New steps for QoQ logic\n",
    "    expanded_dates_qoq = create_expanded_dates_qoq(session, main_data1)\n",
    "    filtered_and_adjusted_qoq = create_filtered_and_adjusted_qoq(expanded_dates_qoq)\n",
    "    filtered_and_adjusted_qoq2 = create_filtered_and_adjusted_qoq2(filtered_and_adjusted_qoq)\n",
    "    final_result = create_final_result(filtered_and_adjusted_qoq2)\n",
    "    \n",
    "    print(\"✅ UFR_C_QoQ processing completed successfully!\")\n",
    "    return final_result\n",
    "\n",
    "# Example usage:\n",
    "# result_df = UFR_C_QoQ(session, \"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST\", Column_mapping_UFR)\n",
    "# result_df.write.mode(\"overwrite\").saveAsTable(\"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST_C_QoQ1\")\n",
    "\n",
    "\n",
    "\n",
    "def UFR_C_YoY(session: Session, table_name: str, column_mapping: dict = None, filter_date: str = \"2024-04-01\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        session: Snowpark session\n",
    "        table_name: Name of the input table\n",
    "        column_mapping: Dictionary mapping standard column names to actual column names in the input table\n",
    "        filter_date: Date filter for final result (format: 'YYYY-MM-DD')\n",
    "    \n",
    "    Returns:\n",
    "        Snowpark DataFrame with the final result\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use default mapping if none provided\n",
    "    if column_mapping is None:\n",
    "        column_mapping = Column_mapping_UFR\n",
    "    \n",
    "    # Read the input table\n",
    "    df = session.table(table_name)\n",
    "    \n",
    "    # Helper function to get column reference with mapping support\n",
    "    def get_mapped_col(standard_col_name, default_value=None):\n",
    "        \"\"\"\n",
    "        Get column reference using the mapping, with support for missing columns\n",
    "        \n",
    "        Args:\n",
    "            standard_col_name: The standard column key from the mapping\n",
    "            default_value: Default value to use if column is missing (default: None -> lit(\"NA\") for strings, 0 for numbers)\n",
    "        \n",
    "        Returns:\n",
    "            Column expression or literal value\n",
    "        \"\"\"\n",
    "        mapped_name = column_mapping.get(standard_col_name)\n",
    "        \n",
    "        if mapped_name is None:\n",
    "            # Column not present in input, return default value\n",
    "            if default_value is not None:\n",
    "                return lit(default_value)\n",
    "            elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "                return lit(0)  # Default for numeric columns\n",
    "            else:\n",
    "                return lit(\"NA\")  # Default for text columns\n",
    "        \n",
    "        # Column is mapped, try to get it from the DataFrame\n",
    "        columns = df.columns\n",
    "        \n",
    "        # Remove quotes for comparison\n",
    "        clean_mapped_name = mapped_name.strip('\"')\n",
    "        \n",
    "        # Try exact match first\n",
    "        if mapped_name in columns:\n",
    "            return col(mapped_name)\n",
    "        elif clean_mapped_name in columns:\n",
    "            return col(clean_mapped_name)\n",
    "        \n",
    "        # Try case-insensitive match\n",
    "        for c in columns:\n",
    "            if c.upper() == clean_mapped_name.upper():\n",
    "                return col(c)\n",
    "        \n",
    "        # If not found, return default value and print warning\n",
    "        print(f\"Warning: Column {mapped_name} not found in table. Using default value.\")\n",
    "        if default_value is not None:\n",
    "            return lit(default_value)\n",
    "        elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "            return lit(0)\n",
    "        else:\n",
    "            return lit(\"NA\")\n",
    "    \n",
    "    def validate_required_columns():\n",
    "        \"\"\"Validate that all required columns are available\"\"\"\n",
    "        required_columns = [\"DATE\", \"START_DATE\", \"END_DATE\", \"CUSTOMERID\", \"MRR\"]\n",
    "        missing_required = []\n",
    "        \n",
    "        for req_col in required_columns:\n",
    "            if column_mapping.get(req_col) is None:\n",
    "                missing_required.append(req_col)\n",
    "        \n",
    "        if missing_required:\n",
    "            raise ValueError(f\"Required columns are missing from column mapping: {missing_required}\")\n",
    "        \n",
    "        print(\"✓ All required columns are mapped\")\n",
    "        return True\n",
    "    \n",
    "    # Validate required columns\n",
    "    validate_required_columns()\n",
    "    \n",
    "    # Print column mapping summary\n",
    "    print(\"\\n=== Column Mapping Summary ===\")\n",
    "    for std_name, mapped_name in column_mapping.items():\n",
    "        status = \"✓ Mapped\" if mapped_name is not None else \"✗ Missing (using default)\"\n",
    "        mapped_display = mapped_name if mapped_name is not None else \"None\"\n",
    "        print(f\"{std_name:20} -> {mapped_display:20} {status}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: cleaned_data CTE\n",
    "    def clean_data(df):\n",
    "        print(\"Step 1: Cleaning data...\")\n",
    "        \n",
    "        cleaned = df.select(\n",
    "            # Convert date columns using mapping\n",
    "            get_mapped_col(\"DATE\").alias(\"Date\"),\n",
    "            get_mapped_col(\"START_DATE\").alias(\"Start_Date\"),\n",
    "            get_mapped_col(\"END_DATE\").alias(\"End_Date\"),\n",
    "            \n",
    "            # Clean and standardize text columns using mapping\n",
    "            (trim(coalesce(get_mapped_col(\"CUSTOMERID\"), lit(\"NA\")))).alias(\"CustomerID\"),\n",
    "            (trim(coalesce(get_mapped_col(\"UP_SNL_ID\"), lit(\"NA\")))).alias(\"up_snl_id\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT\"), lit(\"NA\")))).alias(\"PRODUCT\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT_GROUP\"), lit(\"NA\")))).alias(\"PRODUCT_GROUP\"),\n",
    "            (trim(coalesce(get_mapped_col(\"OPPORTUNITY_NUMBER\"), lit(\"NA\")))).alias(\"opportunity_number\"),\n",
    "            (trim(coalesce(get_mapped_col(\"REVENUE_TYPE\"), lit(\"NA\")))).alias(\"Revenue_Type\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TAGGING\"), lit(\"NA\")))).alias(\"Tagging\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TIER\"), lit(\"NA\")))).alias(\"Tier\"),\n",
    "            (trim(coalesce(get_mapped_col(\"SEGMENT\"), lit(\"NA\")))).alias(\"Segment\"),\n",
    "            (trim(coalesce(get_mapped_col(\"Region\"), lit(\"NA\")))).alias(\"Region\"),\n",
    "            \n",
    "            # Handle numeric columns using mapping\n",
    "            coalesce(get_mapped_col(\"MRR\"), lit(0)).alias(\"MRR\"),\n",
    "            coalesce(get_mapped_col(\"CARR\"), lit(0)).alias(\"CARR\"),\n",
    "            coalesce(get_mapped_col(\"ADJUSTED_ARR\"), lit(0)).alias(\"Adjusted_ARR\")\n",
    "        ).filter(\n",
    "            # Apply filters\n",
    "            (col(\"End_Date\").isNotNull()) & \n",
    "            (col(\"MRR\") != 0)\n",
    "        )\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    # Step 2: contract_summary CTE\n",
    "    def create_contract_summary(cleaned_data):\n",
    "        print(\"Step 2: Creating contract summary...\")\n",
    "        \n",
    "        contract_summary = cleaned_data.groupBy(\"CustomerID\", \"Start_Date\", \"End_Date\").agg(\n",
    "            spark_min(\"Date\").alias(\"Min_Date\"),\n",
    "            spark_max(\"Date\").alias(\"Max_Date\")\n",
    "        )\n",
    "        \n",
    "        # Calculate contract size\n",
    "        contract_summary = contract_summary.withColumn(\n",
    "            \"Contract_Size\",\n",
    "            when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 1, \"1. Less than 1 year\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 2, \"2. 1-2 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 3, \"3. 2-3 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 4, \"4. 3-4 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 5, \"5. 4-5 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) >= 5, \"6. 5+ years\")\n",
    "            .otherwise(\"NA\")\n",
    "        )\n",
    "        \n",
    "        return contract_summary\n",
    "    \n",
    "    # Step 3: summary_a and summary_b CTEs\n",
    "    def create_summaries(cleaned_data):\n",
    "        print(\"Step 3: Creating summaries...\")\n",
    "        \n",
    "        summary_a = cleaned_data.select(\"CustomerID\", \"Start_Date\", \"End_Date\").distinct()\n",
    "        summary_b = cleaned_data.select(\"Date\").distinct()\n",
    "        \n",
    "        return summary_a, summary_b\n",
    "    \n",
    "    # Step 4: cartesian_data CTE\n",
    "    def create_cartesian_data(summary_a, summary_b):\n",
    "        print(\"Step 4: Creating cartesian data...\")\n",
    "        \n",
    "        # Cross join to create cartesian product\n",
    "        cartesian = summary_a.crossJoin(summary_b)\n",
    "        \n",
    "        return cartesian\n",
    "    \n",
    "    # Step 5: summary_mrr CTE\n",
    "    def create_summary_mrr(cleaned_data):\n",
    "        print(\"Step 5: Creating summary MRR...\")\n",
    "        \n",
    "        summary_mrr = cleaned_data.groupBy(\"CustomerID\", \"Start_Date\", \"End_Date\", \"Date\").agg(\n",
    "            spark_sum(\"MRR\").alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return summary_mrr\n",
    "    \n",
    "    # Step 6: joined_data CTE\n",
    "    def create_joined_data(cartesian_data, summary_mrr):\n",
    "        print(\"Step 6: Creating joined data...\")\n",
    "        \n",
    "        joined = cartesian_data.join(\n",
    "            summary_mrr,\n",
    "            [\"CustomerID\", \"Start_Date\", \"End_Date\", \"Date\"],\n",
    "            \"left\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Date\"),\n",
    "            coalesce(col(\"Sum_MRR\"), lit(0)).alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return joined\n",
    "    \n",
    "    # Step 7: joined_data2 CTE\n",
    "    def create_joined_data2(contract_summary, joined_data):\n",
    "        print(\"Step 7: Creating joined data 2...\")\n",
    "        \n",
    "        joined_data2 = joined_data.join(\n",
    "            contract_summary.select(\"CustomerID\", \"Start_Date\", \"End_Date\", \"Max_Date\"),\n",
    "            [\"CustomerID\", \"Start_Date\", \"End_Date\"],\n",
    "            \"inner\"\n",
    "        )\n",
    "        \n",
    "        return joined_data2\n",
    "    \n",
    "    # Step 8: with_next_date CTE\n",
    "    def create_with_next_date(joined_data2):\n",
    "        print(\"Step 8: Adding next date...\")\n",
    "        \n",
    "        with_next_date = joined_data2.withColumn(\n",
    "            \"Next_Date\",\n",
    "            date_trunc(\"month\", dateadd(\"day\", lit(1), col(\"End_Date\")))\n",
    "        ).withColumn(\n",
    "            \"Sum_MRR_Adjusted\",\n",
    "            when(\n",
    "                dateadd(\"month\", lit(1), col(\"Max_Date\")) < col(\"Next_Date\"),\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"Sum_MRR\"))\n",
    "        )\n",
    "        \n",
    "        return with_next_date\n",
    "    \n",
    "    # Step 9: sorted_data CTE\n",
    "    def create_sorted_data(with_next_date):\n",
    "        print(\"Step 9: Sorting data...\")\n",
    "        \n",
    "        sorted_data = with_next_date.orderBy(\"CustomerID\", \"Start_Date\", \"End_Date\", \"Date\")\n",
    "        \n",
    "        return sorted_data\n",
    "    \n",
    "    # Step 10: rolling_sums CTE\n",
    "    def create_rolling_sums(sorted_data):\n",
    "        print(\"Step 10: Creating rolling sums...\")\n",
    "        \n",
    "        window_spec = Window.partitionBy(\"CustomerID\", \"Start_Date\", \"End_Date\").orderBy(\"Date\")\n",
    "        \n",
    "        rolling_sums = sorted_data.withColumn(\n",
    "            \"UFR_TTM\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-11, 0))\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-2, 0))\n",
    "        )\n",
    "        \n",
    "        return rolling_sums\n",
    "    \n",
    "    # Step 11: final_metrics CTE\n",
    "    def create_final_metrics(rolling_sums):\n",
    "        print(\"Step 11: Creating final metrics...\")\n",
    "        \n",
    "        final_metrics = rolling_sums.withColumn(\n",
    "            \"UFR_ARR\", col(\"Sum_MRR_Adjusted\") * 12\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M_Annualized\", col(\"UFR_TTM\") * 4\n",
    "        ).filter(\n",
    "            (col(\"Sum_MRR_Adjusted\") != 0) |\n",
    "            (col(\"UFR_TTM\") != 0) |\n",
    "            (col(\"UFR_T3M\") != 0)\n",
    "        )\n",
    "        \n",
    "        return final_metrics\n",
    "    \n",
    "    # Step 12: joined_data3 CTE\n",
    "    def create_joined_data3(contract_summary, final_metrics):\n",
    "        print(\"Step 12: Creating joined data 3...\")\n",
    "        \n",
    "        # Filter final_metrics where Date equals Max_Date\n",
    "        filtered_metrics = final_metrics.filter(col(\"Date\") == col(\"Max_Date\"))\n",
    "        \n",
    "        joined_data3 = contract_summary.join(\n",
    "            filtered_metrics,\n",
    "            [\"CustomerID\", \"Start_Date\", \"End_Date\", \"Max_Date\"],\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Max_Date\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"Date\"),\n",
    "            col(\"Sum_MRR_Adjusted\").alias(\"UFR_Amount\"),\n",
    "            col(\"UFR_TTM\"),\n",
    "            col(\"UFR_T3M\"),\n",
    "            col(\"UFR_ARR\"),\n",
    "            col(\"UFR_T3M_Annualized\"),\n",
    "            col(\"Next_Date\")\n",
    "        )\n",
    "        \n",
    "        return joined_data3\n",
    "    \n",
    "    # Step 13: unioneddata2 CTE\n",
    "    def create_unioneddata2(joined_data3):\n",
    "        print(\"Step 13: Creating unioned data...\")\n",
    "        \n",
    "        # Create separate DataFrames for each UNION\n",
    "        mrr_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"MRR\"))\n",
    "        \n",
    "        arr_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_ARR\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"ARR\"))\n",
    "        \n",
    "        t3m_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M\"))\n",
    "        \n",
    "        ttm_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_TTM\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"TTM\"))\n",
    "        \n",
    "        t3m_ann_data = joined_data3.groupBy(\"CustomerID\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M_Annualized\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M (Annualized)\"))\n",
    "        \n",
    "        # Union all data\n",
    "        unioneddata2 = mrr_data.unionAll(arr_data).unionAll(t3m_data).unionAll(ttm_data).unionAll(t3m_ann_data)\n",
    "        \n",
    "        return unioneddata2\n",
    "    \n",
    "    # Step 14: main_data1 CTE\n",
    "    def create_main_data1(unioneddata2):\n",
    "        print(\"Step 14: Creating main data...\")\n",
    "        \n",
    "        main_data1 = unioneddata2.withColumn(\n",
    "            \"Min_Date\", date_trunc(\"month\", col(\"Min_Date\"))\n",
    "        ).withColumn(\n",
    "            \"Next_Date\", date_trunc(\"month\", col(\"Next_Date\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount\") >= 0\n",
    "        )\n",
    "        \n",
    "        return main_data1\n",
    "    \n",
    "    # Step 15: expanded_dates_YOY CTE (FIXED for YoY)\n",
    "    def create_expanded_dates_yoy(main_data1):\n",
    "        print(\"Step 15: Creating expanded dates for YoY...\")\n",
    "        \n",
    "        # Create a generator DataFrame for 12 months (0-11)\n",
    "        # Use the table function with proper column specification\n",
    "        generator_df = session.sql(\"\"\"\n",
    "            SELECT SEQ4() as seq\n",
    "            FROM TABLE(GENERATOR(ROWCOUNT => 12))\n",
    "        \"\"\")\n",
    "        \n",
    "        # Cross join with main_data1 to expand dates\n",
    "        expanded_dates_yoy = main_data1.crossJoin(\n",
    "            generator_df\n",
    "        ).withColumn(\n",
    "            \"UFR_Date\",\n",
    "            dateadd(\"month\", col(\"seq\"), col(\"Next_Date\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Date\") <= dateadd(\"month\", lit(11), col(\"Next_Date\"))\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"Next_Date\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Amount\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"UFR_Amount\"),\n",
    "            col(\"UFR_Date\")\n",
    "        )\n",
    "        \n",
    "        return expanded_dates_yoy\n",
    "    \n",
    "    # Step 16: filtered_and_adjusted_YOY CTE (NEW for YoY)\n",
    "    def create_filtered_and_adjusted_yoy(expanded_dates_yoy):\n",
    "        print(\"Step 16: Creating filtered and adjusted YoY...\")\n",
    "        \n",
    "        filtered_and_adjusted_yoy = expanded_dates_yoy.withColumn(\n",
    "            \"UFR_Amount_Adjusted\",\n",
    "            when(\n",
    "                col(\"Min_Date\") > dateadd(\"month\", lit(-12), col(\"UFR_Date\")),\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"UFR_Amount\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount_Adjusted\") != 0\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"UFR_Date\"),\n",
    "            col(\"Amount\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"UFR_Amount_Adjusted\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        \n",
    "        return filtered_and_adjusted_yoy\n",
    "    \n",
    "    # Step 17: filtered_and_adjusted_YOY2 CTE (NEW for YoY)\n",
    "    def create_filtered_and_adjusted_yoy2(filtered_and_adjusted_yoy):\n",
    "        print(\"Step 17: Creating filtered and adjusted YoY2...\")\n",
    "        \n",
    "        filtered_and_adjusted_yoy2 = filtered_and_adjusted_yoy.filter(\n",
    "            col(\"UFR_Date\") < lit(filter_date)  # Use configurable filter_date parameter\n",
    "        ).groupBy(\"CustomerID\", \"UFR_Date\", \"Amount\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        \n",
    "        return filtered_and_adjusted_yoy2\n",
    "    \n",
    "    # Step 18: Final SELECT (NEW for YoY)\n",
    "    def create_final_result_yoy(filtered_and_adjusted_yoy2):\n",
    "        print(\"Step 18: Creating final YoY result...\")\n",
    "        \n",
    "        final_result = filtered_and_adjusted_yoy2.groupBy(\"CustomerID\", \"UFR_Date\", \"Amount\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        if input_amount == \"ARR\":\n",
    "            final_result = final_result.select(\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                (col(\"UFR_Amount\") / 12).alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "        )\n",
    "        else:  # For MRR\n",
    "            final_result = final_result.select(\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                col(\"UFR_Amount\").alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "        )\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    # Execute all steps\n",
    "    print(f\"\\n🚀 Starting UFR_C_YoY processing for table: {table_name}\")\n",
    "    # print(f\"📅 Filter date: {filter_date}\")\n",
    "    \n",
    "    cleaned_data = clean_data(df)\n",
    "    contract_summary = create_contract_summary(cleaned_data)\n",
    "    summary_a, summary_b = create_summaries(cleaned_data)\n",
    "    cartesian_data = create_cartesian_data(summary_a, summary_b)\n",
    "    summary_mrr = create_summary_mrr(cleaned_data)\n",
    "    joined_data = create_joined_data(cartesian_data, summary_mrr)\n",
    "    joined_data2 = create_joined_data2(contract_summary, joined_data)\n",
    "    with_next_date = create_with_next_date(joined_data2)\n",
    "    sorted_data = create_sorted_data(with_next_date)\n",
    "    rolling_sums = create_rolling_sums(sorted_data)\n",
    "    final_metrics = create_final_metrics(rolling_sums)\n",
    "    joined_data3 = create_joined_data3(contract_summary, final_metrics)\n",
    "    unioneddata2 = create_unioneddata2(joined_data3)\n",
    "    main_data1 = create_main_data1(unioneddata2)\n",
    "    \n",
    "    # New YoY-specific steps\n",
    "    expanded_dates_yoy = create_expanded_dates_yoy(main_data1)\n",
    "    filtered_and_adjusted_yoy = create_filtered_and_adjusted_yoy(expanded_dates_yoy)\n",
    "    filtered_and_adjusted_yoy2 = create_filtered_and_adjusted_yoy2(filtered_and_adjusted_yoy)\n",
    "    final_result = create_final_result_yoy(filtered_and_adjusted_yoy2)\n",
    "    \n",
    "    print(\"✅ UFR_C_YoY processing completed successfully!\")\n",
    "    return final_result\n",
    "\n",
    "# Example usage:\n",
    "# result_df = UFR_C_YoY(session, \"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST\", Column_mapping_UFR)\n",
    "# result_df.write.mode(\"overwrite\").saveAsTable(\"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST_C_YoY1\")\n",
    "\n",
    "\n",
    "def UFR_CP_MoM(session: Session, table_name: str, column_mapping: dict = None, filter_date: str = \"2024-04-01\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        session: Snowpark session\n",
    "        table_name: Name of the input table\n",
    "        column_mapping: Dictionary mapping standard column names to actual column names in the input table\n",
    "        filter_date: Date filter for final result (format: 'YYYY-MM-DD')\n",
    "    \n",
    "    Returns:\n",
    "        Snowpark DataFrame with the final result\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use default mapping if none provided\n",
    "    if column_mapping is None:\n",
    "        column_mapping = Column_mapping_UFR\n",
    "    \n",
    "    # Read the input table\n",
    "    df = session.table(table_name)\n",
    "    \n",
    "    # Helper function to get column reference with mapping support\n",
    "    def get_mapped_col(standard_col_name, default_value=None):\n",
    "        \"\"\"\n",
    "        Get column reference using the mapping, with support for missing columns\n",
    "        \n",
    "        Args:\n",
    "            standard_col_name: The standard column key from the mapping\n",
    "            default_value: Default value to use if column is missing (default: None -> lit(\"NA\") for strings, 0 for numbers)\n",
    "        \n",
    "        Returns:\n",
    "            Column expression or literal value\n",
    "        \"\"\"\n",
    "        mapped_name = column_mapping.get(standard_col_name)\n",
    "        \n",
    "        if mapped_name is None:\n",
    "            # Column not present in input, return default value\n",
    "            if default_value is not None:\n",
    "                return lit(default_value)\n",
    "            elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "                return lit(0)  # Default for numeric columns\n",
    "            else:\n",
    "                return lit(\"NA\")  # Default for text columns\n",
    "        \n",
    "        # Column is mapped, try to get it from the DataFrame\n",
    "        columns = df.columns\n",
    "        \n",
    "        # Remove quotes for comparison\n",
    "        clean_mapped_name = mapped_name.strip('\"')\n",
    "        \n",
    "        # Try exact match first\n",
    "        if mapped_name in columns:\n",
    "            return col(mapped_name)\n",
    "        elif clean_mapped_name in columns:\n",
    "            return col(clean_mapped_name)\n",
    "        \n",
    "        # Try case-insensitive match\n",
    "        for c in columns:\n",
    "            if c.upper() == clean_mapped_name.upper():\n",
    "                return col(c)\n",
    "        \n",
    "        # If not found, return default value and print warning\n",
    "        print(f\"Warning: Column {mapped_name} not found in table. Using default value.\")\n",
    "        if default_value is not None:\n",
    "            return lit(default_value)\n",
    "        elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "            return lit(0)\n",
    "        else:\n",
    "            return lit(\"NA\")\n",
    "    \n",
    "    def validate_required_columns():\n",
    "        \"\"\"Validate that all required columns are available\"\"\"\n",
    "        required_columns = [\"DATE\", \"START_DATE\", \"END_DATE\", \"CUSTOMERID\", \"PRODUCT\", \"REVENUE_TYPE\", \"MRR\"]\n",
    "        missing_required = []\n",
    "        \n",
    "        for req_col in required_columns:\n",
    "            if column_mapping.get(req_col) is None:\n",
    "                missing_required.append(req_col)\n",
    "        \n",
    "        if missing_required:\n",
    "            raise ValueError(f\"Required columns are missing from column mapping: {missing_required}\")\n",
    "        \n",
    "        print(\"✓ All required columns are mapped\")\n",
    "        return True\n",
    "    \n",
    "    # Validate required columns\n",
    "    validate_required_columns()\n",
    "    \n",
    "    # Print column mapping summary\n",
    "    print(\"\\n=== Column Mapping Summary ===\")\n",
    "    for std_name, mapped_name in column_mapping.items():\n",
    "        status = \"✓ Mapped\" if mapped_name is not None else \"✗ Missing (using default)\"\n",
    "        mapped_display = mapped_name if mapped_name is not None else \"None\"\n",
    "        print(f\"{std_name:20} -> {mapped_display:20} {status}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: cleaned_data CTE\n",
    "    def clean_data(df):\n",
    "        print(\"Step 1: Cleaning data...\")\n",
    "        \n",
    "        cleaned = df.select(\n",
    "            # Convert date columns using mapping\n",
    "            get_mapped_col(\"DATE\").alias(\"Date\"),\n",
    "            get_mapped_col(\"START_DATE\").alias(\"Start_Date\"),\n",
    "            get_mapped_col(\"END_DATE\").alias(\"End_Date\"),\n",
    "            \n",
    "            # Clean and standardize text columns using mapping\n",
    "            (trim(coalesce(get_mapped_col(\"CUSTOMERID\"), lit(\"NA\")))).alias(\"CustomerID\"),\n",
    "            (trim(coalesce(get_mapped_col(\"UP_SNL_ID\"), lit(\"NA\")))).alias(\"up_snl_id\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT\"), lit(\"NA\")))).alias(\"PRODUCT\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT_GROUP\"), lit(\"NA\")))).alias(\"PRODUCT_GROUP\"),\n",
    "            (trim(coalesce(get_mapped_col(\"OPPORTUNITY_NUMBER\"), lit(\"NA\")))).alias(\"opportunity_number\"),\n",
    "            (trim(coalesce(get_mapped_col(\"REVENUE_TYPE\"), lit(\"NA\")))).alias(\"Revenue_Type\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TAGGING\"), lit(\"NA\")))).alias(\"Tagging\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TIER\"), lit(\"NA\")))).alias(\"Tier\"),\n",
    "            (trim(coalesce(get_mapped_col(\"SEGMENT\"), lit(\"NA\")))).alias(\"Segment\"),\n",
    "            (trim(coalesce(get_mapped_col(\"Region\"), lit(\"NA\")))).alias(\"Region\"),\n",
    "            \n",
    "            # Handle numeric columns using mapping\n",
    "            coalesce(get_mapped_col(\"MRR\"), lit(0)).alias(\"MRR\"),\n",
    "            coalesce(get_mapped_col(\"CARR\"), lit(0)).alias(\"CARR\"),\n",
    "            coalesce(get_mapped_col(\"ADJUSTED_ARR\"), lit(0)).alias(\"Adjusted_ARR\")\n",
    "        ).filter(\n",
    "            # Apply filters\n",
    "            (col(\"End_Date\").isNotNull()) & \n",
    "            (col(\"MRR\") != 0)\n",
    "        )\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    # Step 2: contract_summary CTE - MODIFIED for SQL2\n",
    "    def create_contract_summary(cleaned_data):\n",
    "        print(\"Step 2: Creating contract summary...\")\n",
    "        \n",
    "        # GROUP BY includes PRODUCT and Revenue_Type for SQL2\n",
    "        contract_summary = cleaned_data.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\").agg(\n",
    "            spark_min(\"Date\").alias(\"Min_Date\"),\n",
    "            spark_max(\"Date\").alias(\"Max_Date\")\n",
    "        )\n",
    "        \n",
    "        # Calculate contract size\n",
    "        contract_summary = contract_summary.withColumn(\n",
    "            \"Contract_Size\",\n",
    "            when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 1, \"1. Less than 1 year\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 2, \"2. 1-2 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 3, \"3. 2-3 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 4, \"4. 3-4 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 5, \"5. 4-5 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) >= 5, \"6. 5+ years\")\n",
    "            .otherwise(\"NA\")\n",
    "        )\n",
    "        \n",
    "        return contract_summary\n",
    "    \n",
    "    # Step 3: summary_a and summary_b CTEs - MODIFIED for SQL2\n",
    "    def create_summaries(cleaned_data):\n",
    "        print(\"Step 3: Creating summaries...\")\n",
    "        \n",
    "        # summary_a includes PRODUCT and Revenue_Type for SQL2\n",
    "        summary_a = cleaned_data.select(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\").distinct()\n",
    "        summary_b = cleaned_data.select(\"Date\").distinct()\n",
    "        \n",
    "        return summary_a, summary_b\n",
    "    \n",
    "    # Step 4: cartesian_data CTE - MODIFIED for SQL2\n",
    "    def create_cartesian_data(summary_a, summary_b):\n",
    "        print(\"Step 4: Creating cartesian data...\")\n",
    "        \n",
    "        # Cross join to create cartesian product - includes PRODUCT and Revenue_Type\n",
    "        cartesian = summary_a.crossJoin(summary_b)\n",
    "        \n",
    "        return cartesian\n",
    "    \n",
    "    # Step 5: summary_mrr CTE - MODIFIED for SQL2\n",
    "    def create_summary_mrr(cleaned_data):\n",
    "        print(\"Step 5: Creating summary MRR...\")\n",
    "        \n",
    "        # GROUP BY includes PRODUCT and Revenue_Type for SQL2\n",
    "        summary_mrr = cleaned_data.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Date\").agg(\n",
    "            spark_sum(\"MRR\").alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return summary_mrr\n",
    "    \n",
    "    # Step 6: joined_data CTE - MODIFIED for SQL2\n",
    "    def create_joined_data(cartesian_data, summary_mrr):\n",
    "        print(\"Step 6: Creating joined data...\")\n",
    "        \n",
    "        # JOIN conditions include PRODUCT and Revenue_Type for SQL2\n",
    "        joined = cartesian_data.join(\n",
    "            summary_mrr,\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Date\"],\n",
    "            \"left\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Revenue_Type\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Date\"),\n",
    "            coalesce(col(\"Sum_MRR\"), lit(0)).alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return joined\n",
    "    \n",
    "    # Step 7: joined_data2 CTE - MODIFIED for SQL2\n",
    "    def create_joined_data2(contract_summary, joined_data):\n",
    "        print(\"Step 7: Creating joined data 2...\")\n",
    "        \n",
    "        # JOIN conditions include PRODUCT and Revenue_Type for SQL2\n",
    "        joined_data2 = joined_data.join(\n",
    "            contract_summary.select(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Max_Date\"),\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\"],\n",
    "            \"inner\"\n",
    "        )\n",
    "        \n",
    "        return joined_data2\n",
    "    \n",
    "    # Step 8: with_next_date CTE\n",
    "    def create_with_next_date(joined_data2):\n",
    "        print(\"Step 8: Adding next date...\")\n",
    "        \n",
    "        with_next_date = joined_data2.withColumn(\n",
    "            \"Next_Date\",\n",
    "            date_trunc(\"month\", dateadd(\"day\", lit(1), col(\"End_Date\")))\n",
    "        ).withColumn(\n",
    "            \"Sum_MRR_Adjusted\",\n",
    "            when(\n",
    "                dateadd(\"month\", lit(1), col(\"Max_Date\")) < col(\"Next_Date\"),\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"Sum_MRR\"))\n",
    "        )\n",
    "        \n",
    "        return with_next_date\n",
    "    \n",
    "    # Step 9: sorted_data CTE - MODIFIED for SQL2\n",
    "    def create_sorted_data(with_next_date):\n",
    "        print(\"Step 9: Sorting data...\")\n",
    "        \n",
    "        # ORDER BY includes PRODUCT and Revenue_Type for SQL2\n",
    "        sorted_data = with_next_date.orderBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Date\")\n",
    "        \n",
    "        return sorted_data\n",
    "    \n",
    "    # Step 10: rolling_sums CTE - MODIFIED for SQL2\n",
    "    def create_rolling_sums(sorted_data):\n",
    "        print(\"Step 10: Creating rolling sums...\")\n",
    "        \n",
    "        # PARTITION BY includes PRODUCT and Revenue_Type for SQL2\n",
    "        window_spec = Window.partitionBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\").orderBy(\"Date\")\n",
    "        \n",
    "        rolling_sums = sorted_data.withColumn(\n",
    "            \"UFR_TTM\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-11, 0))\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-2, 0))\n",
    "        )\n",
    "        \n",
    "        return rolling_sums\n",
    "    \n",
    "    # Step 11: final_metrics CTE\n",
    "    def create_final_metrics(rolling_sums):\n",
    "        print(\"Step 11: Creating final metrics...\")\n",
    "        \n",
    "        final_metrics = rolling_sums.withColumn(\n",
    "            \"UFR_ARR\", col(\"Sum_MRR_Adjusted\") * 12\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M_Annualized\", col(\"UFR_TTM\") * 4\n",
    "        ).filter(\n",
    "            (col(\"Sum_MRR_Adjusted\") != 0) |\n",
    "            (col(\"UFR_TTM\") != 0) |\n",
    "            (col(\"UFR_T3M\") != 0)\n",
    "        )\n",
    "        \n",
    "        return final_metrics\n",
    "    \n",
    "    # Step 12: joined_data3 CTE - MODIFIED for SQL2\n",
    "    def create_joined_data3(contract_summary, final_metrics):\n",
    "        print(\"Step 12: Creating joined data 3...\")\n",
    "        \n",
    "        # Filter final_metrics where Date equals Max_Date\n",
    "        filtered_metrics = final_metrics.filter(col(\"Date\") == col(\"Max_Date\"))\n",
    "        \n",
    "        # JOIN conditions include PRODUCT and Revenue_Type for SQL2\n",
    "        joined_data3 = contract_summary.join(\n",
    "            filtered_metrics,\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Max_Date\"],\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Revenue_Type\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Max_Date\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"Date\"),\n",
    "            col(\"Sum_MRR_Adjusted\").alias(\"UFR_Amount\"),\n",
    "            col(\"UFR_TTM\"),\n",
    "            col(\"UFR_T3M\"),\n",
    "            col(\"UFR_ARR\"),\n",
    "            col(\"UFR_T3M_Annualized\"),\n",
    "            col(\"Next_Date\")\n",
    "        )\n",
    "        \n",
    "        return joined_data3\n",
    "    \n",
    "    # Step 13: unioneddata2 CTE - MODIFIED for SQL2\n",
    "    def create_unioneddata2(joined_data3):\n",
    "        print(\"Step 13: Creating unioned data...\")\n",
    "        \n",
    "        # Create separate DataFrames for each UNION - GROUP BY includes PRODUCT and Revenue_Type\n",
    "        mrr_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"MRR\"))\n",
    "        \n",
    "        arr_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_ARR\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"ARR\"))\n",
    "        \n",
    "        t3m_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M\"))\n",
    "        \n",
    "        ttm_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_TTM\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"TTM\"))\n",
    "        \n",
    "        t3m_ann_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M_Annualized\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M (Annualized)\"))\n",
    "        \n",
    "        # Union all data\n",
    "        unioneddata2 = mrr_data.unionAll(arr_data).unionAll(t3m_data).unionAll(ttm_data).unionAll(t3m_ann_data)\n",
    "        \n",
    "        return unioneddata2\n",
    "    \n",
    "    # Step 14: main_data1 CTE - MODIFIED for SQL2\n",
    "    def create_main_data1(unioneddata2):\n",
    "        print(\"Step 14: Creating main data...\")\n",
    "        \n",
    "        main_data1 = unioneddata2.withColumn(\n",
    "            \"Min_Date\", date_trunc(\"month\", col(\"Min_Date\"))\n",
    "        ).withColumn(\n",
    "            \"Next_Date\", date_trunc(\"month\", col(\"Next_Date\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount\") >= 0\n",
    "        )\n",
    "        \n",
    "        return main_data1\n",
    "    \n",
    "    # Step 15: aggregated_data CTE - MODIFIED for SQL2\n",
    "    def create_aggregated_data(main_data1):\n",
    "        print(\"Step 15: Creating aggregated data...\")\n",
    "        \n",
    "        # GROUP BY includes PRODUCT and Revenue_Type for SQL2\n",
    "        aggregated_data = main_data1.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Next_Date\", \"Amount\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        ).withColumnRenamed(\"Next_Date\", \"UFR_Date\")\n",
    "        \n",
    "        return aggregated_data\n",
    "    \n",
    "    # Step 16: Final SELECT - MODIFIED for SQL2\n",
    "    def create_final_result(aggregated_data):\n",
    "        print(\"Step 16: Creating final result...\")\n",
    "        \n",
    "        # Filter UFR Date < filter_date (configurable parameter)\n",
    "        # GROUP BY includes PRODUCT and Revenue_Type for SQL2\n",
    "        final_result = aggregated_data.filter(\n",
    "            col(\"UFR_Date\") < lit(filter_date)\n",
    "        ).groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"UFR_Date\", \"Amount\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        if input_amount == \"ARR\":\n",
    "            final_result = final_result.select(\n",
    "                col(\"PRODUCT\").alias('\"Product\"'),\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                (col(\"UFR_Amount\") / 12).alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "        )\n",
    "        else:  # For MRR\n",
    "            final_result = final_result.select(\n",
    "                col(\"PRODUCT\").alias('\"Product\"'),\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                col(\"UFR_Amount\").alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "        )\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    # Execute all steps\n",
    "    print(f\"\\n🚀 Starting UFR_CP_MoM processing for table: {table_name}\")\n",
    "    # print(f\"📅 Filter date: {filter_date}\")\n",
    "    \n",
    "    cleaned_data = clean_data(df)\n",
    "    contract_summary = create_contract_summary(cleaned_data)\n",
    "    summary_a, summary_b = create_summaries(cleaned_data)\n",
    "    cartesian_data = create_cartesian_data(summary_a, summary_b)\n",
    "    summary_mrr = create_summary_mrr(cleaned_data)\n",
    "    joined_data = create_joined_data(cartesian_data, summary_mrr)\n",
    "    joined_data2 = create_joined_data2(contract_summary, joined_data)\n",
    "    with_next_date = create_with_next_date(joined_data2)\n",
    "    sorted_data = create_sorted_data(with_next_date)\n",
    "    rolling_sums = create_rolling_sums(sorted_data)\n",
    "    final_metrics = create_final_metrics(rolling_sums)\n",
    "    joined_data3 = create_joined_data3(contract_summary, final_metrics)\n",
    "    unioneddata2 = create_unioneddata2(joined_data3)\n",
    "    main_data1 = create_main_data1(unioneddata2)\n",
    "    aggregated_data = create_aggregated_data(main_data1)\n",
    "    final_result = create_final_result(aggregated_data)\n",
    "    \n",
    "    print(\"✅ UFR_CP_MoM processing completed successfully!\")\n",
    "    return final_result\n",
    "\n",
    "# Example usage:\n",
    "# result_df = UFR_CP_MoM(session, \"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST\", Column_mapping_UFR)\n",
    "# result_df.write.mode(\"overwrite\").saveAsTable(\"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST_CP_MoM1\")\n",
    "\n",
    "\n",
    "def UFR_CP_QoQ(session: Session, table_name: str, column_mapping: dict = None, filter_date: str = \"2024-04-01\"):\n",
    "    \"\"\"    \n",
    "    Args:\n",
    "        session: Snowpark session\n",
    "        table_name: Name of the input table\n",
    "        column_mapping: Dictionary mapping standard column names to actual column names in the input table\n",
    "        filter_date: Date filter for final result (format: 'YYYY-MM-DD')\n",
    "    \n",
    "    Returns:\n",
    "        Snowpark DataFrame with the final result\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use default mapping if none provided\n",
    "    if column_mapping is None:\n",
    "        column_mapping = Column_mapping_UFR\n",
    "    \n",
    "    # Read the input table\n",
    "    df = session.table(table_name)\n",
    "    \n",
    "    # Helper function to get column reference with mapping support\n",
    "    def get_mapped_col(standard_col_name, default_value=None):\n",
    "        \"\"\"\n",
    "        Get column reference using the mapping, with support for missing columns\n",
    "        \n",
    "        Args:\n",
    "            standard_col_name: The standard column key from the mapping\n",
    "            default_value: Default value to use if column is missing (default: None -> lit(\"NA\") for strings, 0 for numbers)\n",
    "        \n",
    "        Returns:\n",
    "            Column expression or literal value\n",
    "        \"\"\"\n",
    "        mapped_name = column_mapping.get(standard_col_name)\n",
    "        \n",
    "        if mapped_name is None:\n",
    "            # Column not present in input, return default value\n",
    "            if default_value is not None:\n",
    "                return lit(default_value)\n",
    "            elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "                return lit(0)  # Default for numeric columns\n",
    "            else:\n",
    "                return lit(\"NA\")  # Default for text columns\n",
    "        \n",
    "        # Column is mapped, try to get it from the DataFrame\n",
    "        columns = df.columns\n",
    "        \n",
    "        # Remove quotes for comparison\n",
    "        clean_mapped_name = mapped_name.strip('\"')\n",
    "        \n",
    "        # Try exact match first\n",
    "        if mapped_name in columns:\n",
    "            return col(mapped_name)\n",
    "        elif clean_mapped_name in columns:\n",
    "            return col(clean_mapped_name)\n",
    "        \n",
    "        # Try case-insensitive match\n",
    "        for c in columns:\n",
    "            if c.upper() == clean_mapped_name.upper():\n",
    "                return col(c)\n",
    "        \n",
    "        # If not found, return default value and print warning\n",
    "        print(f\"Warning: Column {mapped_name} not found in table. Using default value.\")\n",
    "        if default_value is not None:\n",
    "            return lit(default_value)\n",
    "        elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "            return lit(0)\n",
    "        else:\n",
    "            return lit(\"NA\")\n",
    "    \n",
    "    def validate_required_columns():\n",
    "        \"\"\"Validate that all required columns are available\"\"\"\n",
    "        required_columns = [\"DATE\", \"START_DATE\", \"END_DATE\", \"CUSTOMERID\", \"PRODUCT\", \"MRR\"]\n",
    "        missing_required = []\n",
    "        \n",
    "        for req_col in required_columns:\n",
    "            if column_mapping.get(req_col) is None:\n",
    "                missing_required.append(req_col)\n",
    "        \n",
    "        if missing_required:\n",
    "            raise ValueError(f\"Required columns are missing from column mapping: {missing_required}\")\n",
    "        \n",
    "        print(\"✓ All required columns are mapped\")\n",
    "        return True\n",
    "    \n",
    "    # Validate required columns\n",
    "    validate_required_columns()\n",
    "    \n",
    "    # Print column mapping summary\n",
    "    print(\"\\n=== Column Mapping Summary ===\")\n",
    "    for std_name, mapped_name in column_mapping.items():\n",
    "        status = \"✓ Mapped\" if mapped_name is not None else \"✗ Missing (using default)\"\n",
    "        mapped_display = mapped_name if mapped_name is not None else \"None\"\n",
    "        print(f\"{std_name:20} -> {mapped_display:20} {status}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: cleaned_data CTE\n",
    "    def clean_data(df):\n",
    "        print(\"Step 1: Cleaning data...\")\n",
    "        \n",
    "        cleaned = df.select(\n",
    "            # Convert date columns using mapping\n",
    "            get_mapped_col(\"DATE\").alias(\"Date\"),\n",
    "            get_mapped_col(\"START_DATE\").alias(\"Start_Date\"),\n",
    "            get_mapped_col(\"END_DATE\").alias(\"End_Date\"),\n",
    "            \n",
    "            # Clean and standardize text columns using mapping\n",
    "            (trim(coalesce(get_mapped_col(\"CUSTOMERID\"), lit(\"NA\")))).alias(\"CustomerID\"),\n",
    "            (trim(coalesce(get_mapped_col(\"UP_SNL_ID\"), lit(\"NA\")))).alias(\"up_snl_id\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT\"), lit(\"NA\")))).alias(\"PRODUCT\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT_GROUP\"), lit(\"NA\")))).alias(\"PRODUCT_GROUP\"),\n",
    "            (trim(coalesce(get_mapped_col(\"OPPORTUNITY_NUMBER\"), lit(\"NA\")))).alias(\"opportunity_number\"),\n",
    "            (trim(coalesce(get_mapped_col(\"REVENUE_TYPE\"), lit(\"NA\")))).alias(\"Revenue_Type\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TAGGING\"), lit(\"NA\")))).alias(\"Tagging\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TIER\"), lit(\"NA\")))).alias(\"Tier\"),\n",
    "            (trim(coalesce(get_mapped_col(\"SEGMENT\"), lit(\"NA\")))).alias(\"Segment\"),\n",
    "            (trim(coalesce(get_mapped_col(\"Region\"), lit(\"NA\")))).alias(\"Region\"),\n",
    "            \n",
    "            # Handle numeric columns using mapping\n",
    "            coalesce(get_mapped_col(\"MRR\"), lit(0)).alias(\"MRR\"),\n",
    "            coalesce(get_mapped_col(\"CARR\"), lit(0)).alias(\"CARR\"),\n",
    "            coalesce(get_mapped_col(\"ADJUSTED_ARR\"), lit(0)).alias(\"Adjusted_ARR\")\n",
    "        ).filter(\n",
    "            # Apply filters\n",
    "            (col(\"End_Date\").isNotNull()) & \n",
    "            (col(\"MRR\") != 0)\n",
    "        )\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    # Step 2: contract_summary CTE - MODIFIED for SQL2 (no Revenue_Type)\n",
    "    def create_contract_summary(cleaned_data):\n",
    "        print(\"Step 2: Creating contract summary...\")\n",
    "        \n",
    "        # GROUP BY only includes CustomerID and PRODUCT (no Revenue_Type)\n",
    "        contract_summary = cleaned_data.groupBy(\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\").agg(\n",
    "            spark_min(\"Date\").alias(\"Min_Date\"),\n",
    "            spark_max(\"Date\").alias(\"Max_Date\")\n",
    "        )\n",
    "        \n",
    "        # Calculate contract size\n",
    "        contract_summary = contract_summary.withColumn(\n",
    "            \"Contract_Size\",\n",
    "            when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 1, \"1. Less than 1 year\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 2, \"2. 1-2 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 3, \"3. 2-3 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 4, \"4. 3-4 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 5, \"5. 4-5 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) >= 5, \"6. 5+ years\")\n",
    "            .otherwise(\"NA\")\n",
    "        )\n",
    "        \n",
    "        return contract_summary\n",
    "    \n",
    "    # Step 3: summary_a and summary_b CTEs - MODIFIED for SQL2 (no Revenue_Type)\n",
    "    def create_summaries(cleaned_data):\n",
    "        print(\"Step 3: Creating summaries...\")\n",
    "        \n",
    "        # summary_a only includes CustomerID and PRODUCT (no Revenue_Type)\n",
    "        summary_a = cleaned_data.select(\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\").distinct()\n",
    "        summary_b = cleaned_data.select(\"Date\").distinct()\n",
    "        \n",
    "        return summary_a, summary_b\n",
    "    \n",
    "    # Step 4: cartesian_data CTE - MODIFIED for SQL2 (no Revenue_Type)\n",
    "    def create_cartesian_data(summary_a, summary_b):\n",
    "        print(\"Step 4: Creating cartesian data...\")\n",
    "        \n",
    "        # Cross join to create cartesian product - only includes CustomerID and PRODUCT\n",
    "        cartesian = summary_a.crossJoin(summary_b)\n",
    "        \n",
    "        return cartesian\n",
    "    \n",
    "    # Step 5: summary_mrr CTE - MODIFIED for SQL2 (no Revenue_Type)\n",
    "    def create_summary_mrr(cleaned_data):\n",
    "        print(\"Step 5: Creating summary MRR...\")\n",
    "        \n",
    "        # GROUP BY only includes CustomerID and PRODUCT (no Revenue_Type)\n",
    "        summary_mrr = cleaned_data.groupBy(\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\", \"Date\").agg(\n",
    "            spark_sum(\"MRR\").alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return summary_mrr\n",
    "    \n",
    "    # Step 6: joined_data CTE - MODIFIED for SQL2 (no Revenue_Type)\n",
    "    def create_joined_data(cartesian_data, summary_mrr):\n",
    "        print(\"Step 6: Creating joined data...\")\n",
    "        \n",
    "        # JOIN conditions only include CustomerID and PRODUCT (no Revenue_Type)\n",
    "        joined = cartesian_data.join(\n",
    "            summary_mrr,\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\", \"Date\"],\n",
    "            \"left\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Date\"),\n",
    "            coalesce(col(\"Sum_MRR\"), lit(0)).alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return joined\n",
    "    \n",
    "    # Step 7: joined_data2 CTE - MODIFIED for SQL2 (no Revenue_Type)\n",
    "    def create_joined_data2(contract_summary, joined_data):\n",
    "        print(\"Step 7: Creating joined data 2...\")\n",
    "        \n",
    "        # JOIN conditions only include CustomerID and PRODUCT (no Revenue_Type)\n",
    "        joined_data2 = joined_data.join(\n",
    "            contract_summary.select(\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\", \"Max_Date\"),\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\"],\n",
    "            \"inner\"\n",
    "        )\n",
    "        \n",
    "        return joined_data2\n",
    "    \n",
    "    # Step 8: with_next_date CTE\n",
    "    def create_with_next_date(joined_data2):\n",
    "        print(\"Step 8: Adding next date...\")\n",
    "        \n",
    "        with_next_date = joined_data2.withColumn(\n",
    "            \"Next_Date\",\n",
    "            date_trunc(\"month\", dateadd(\"day\", lit(1), col(\"End_Date\")))\n",
    "        ).withColumn(\n",
    "            \"Sum_MRR_Adjusted\",\n",
    "            when(\n",
    "                dateadd(\"month\", lit(1), col(\"Max_Date\")) < col(\"Next_Date\"),\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"Sum_MRR\"))\n",
    "        )\n",
    "        \n",
    "        return with_next_date\n",
    "    \n",
    "    # Step 9: sorted_data CTE - MODIFIED for SQL2 (no Revenue_Type)\n",
    "    def create_sorted_data(with_next_date):\n",
    "        print(\"Step 9: Sorting data...\")\n",
    "        \n",
    "        # ORDER BY only includes CustomerID and PRODUCT (no Revenue_Type)\n",
    "        sorted_data = with_next_date.orderBy(\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\", \"Date\")\n",
    "        \n",
    "        return sorted_data\n",
    "    \n",
    "    # Step 10: rolling_sums CTE - MODIFIED for SQL2 (no Revenue_Type)\n",
    "    def create_rolling_sums(sorted_data):\n",
    "        print(\"Step 10: Creating rolling sums...\")\n",
    "        \n",
    "        # PARTITION BY only includes CustomerID and PRODUCT (no Revenue_Type)\n",
    "        window_spec = Window.partitionBy(\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\").orderBy(\"Date\")\n",
    "        \n",
    "        rolling_sums = sorted_data.withColumn(\n",
    "            \"UFR_TTM\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-11, 0))\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-2, 0))\n",
    "        )\n",
    "        \n",
    "        return rolling_sums\n",
    "    \n",
    "    # Step 11: final_metrics CTE\n",
    "    def create_final_metrics(rolling_sums):\n",
    "        print(\"Step 11: Creating final metrics...\")\n",
    "        \n",
    "        final_metrics = rolling_sums.withColumn(\n",
    "            \"UFR_ARR\", col(\"Sum_MRR_Adjusted\") * 12\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M_Annualized\", col(\"UFR_TTM\") * 4\n",
    "        ).filter(\n",
    "            (col(\"Sum_MRR_Adjusted\") != 0) |\n",
    "            (col(\"UFR_TTM\") != 0) |\n",
    "            (col(\"UFR_T3M\") != 0)\n",
    "        )\n",
    "        \n",
    "        return final_metrics\n",
    "    \n",
    "    # Step 12: joined_data3 CTE - MODIFIED for SQL2 (no Revenue_Type)\n",
    "    def create_joined_data3(contract_summary, final_metrics):\n",
    "        print(\"Step 12: Creating joined data 3...\")\n",
    "        \n",
    "        # Filter final_metrics where Date equals Max_Date\n",
    "        filtered_metrics = final_metrics.filter(col(\"Date\") == col(\"Max_Date\"))\n",
    "        \n",
    "        # JOIN conditions only include CustomerID and PRODUCT (no Revenue_Type)\n",
    "        joined_data3 = contract_summary.join(\n",
    "            filtered_metrics,\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\", \"Max_Date\"],\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Max_Date\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"Date\"),\n",
    "            col(\"Sum_MRR_Adjusted\").alias(\"UFR_Amount\"),\n",
    "            col(\"UFR_TTM\"),\n",
    "            col(\"UFR_T3M\"),\n",
    "            col(\"UFR_ARR\"),\n",
    "            col(\"UFR_T3M_Annualized\"),\n",
    "            col(\"Next_Date\")\n",
    "        )\n",
    "        \n",
    "        return joined_data3\n",
    "    \n",
    "    # Step 13: unioneddata2 CTE - MODIFIED for SQL2 (no Revenue_Type)\n",
    "    def create_unioneddata2(joined_data3):\n",
    "        print(\"Step 13: Creating unioned data...\")\n",
    "        \n",
    "        # Create separate DataFrames for each UNION - GROUP BY only includes CustomerID and PRODUCT\n",
    "        mrr_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"MRR\"))\n",
    "        \n",
    "        arr_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_ARR\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"ARR\"))\n",
    "        \n",
    "        t3m_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M\"))\n",
    "        \n",
    "        ttm_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_TTM\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"TTM\"))\n",
    "        \n",
    "        t3m_ann_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M_Annualized\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M (Annualized)\"))\n",
    "        \n",
    "        # Union all data\n",
    "        unioneddata2 = mrr_data.unionAll(arr_data).unionAll(t3m_data).unionAll(ttm_data).unionAll(t3m_ann_data)\n",
    "        \n",
    "        return unioneddata2\n",
    "    \n",
    "    # Step 14: main_data1 CTE - MODIFIED for SQL2 (no Revenue_Type)\n",
    "    def create_main_data1(unioneddata2):\n",
    "        print(\"Step 14: Creating main data...\")\n",
    "        \n",
    "        main_data1 = unioneddata2.withColumn(\n",
    "            \"Min_Date\", date_trunc(\"month\", col(\"Min_Date\"))\n",
    "        ).withColumn(\n",
    "            \"Next_Date\", date_trunc(\"month\", col(\"Next_Date\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount\") >= 0\n",
    "        )\n",
    "        \n",
    "        return main_data1\n",
    "    \n",
    "    # Step 15: expanded_dates_YOY CTE - NEW for YOY logic\n",
    "    def create_expanded_dates_yoy(main_data1):\n",
    "        print(\"Step 15: Creating expanded dates for YOY...\")\n",
    "        \n",
    "        # Create a generator for 3 months (0, 1, 2)\n",
    "        # This simulates the GENERATOR(ROWCOUNT => 3) from SQL\n",
    "        generator_data = [(0,), (1,), (2,)]\n",
    "        generator_df = session.createDataFrame(generator_data, [\"seq\"])\n",
    "        \n",
    "        # Cross join with main_data1 and add the UFR_Date calculation\n",
    "        expanded_dates = main_data1.crossJoin(generator_df).withColumn(\n",
    "            \"UFR_Date\",\n",
    "            dateadd(\"month\", col(\"seq\"), col(\"Next_Date\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Date\") <= dateadd(\"month\", lit(2), col(\"Next_Date\"))\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Next_Date\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Amount\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"UFR_Amount\"),\n",
    "            col(\"UFR_Date\")\n",
    "        )\n",
    "        \n",
    "        return expanded_dates\n",
    "    \n",
    "    # Step 16: filtered_and_adjusted_YOY CTE - NEW for YOY logic\n",
    "    def create_filtered_and_adjusted_yoy(expanded_dates_yoy):\n",
    "        print(\"Step 16: Creating filtered and adjusted YOY...\")\n",
    "        \n",
    "        filtered_adjusted = expanded_dates_yoy.withColumn(\n",
    "            \"UFR_Amount_Adjusted\",\n",
    "            when(\n",
    "                col(\"Min_Date\") > dateadd(\"month\", lit(-3), col(\"UFR_Date\")),\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"UFR_Amount\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount_Adjusted\") != 0\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"UFR_Date\"),\n",
    "            col(\"Amount\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"UFR_Amount_Adjusted\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        \n",
    "        return filtered_adjusted\n",
    "    \n",
    "    # Step 17: filtered_and_adjusted_YOY2 CTE - NEW for YOY logic\n",
    "    def create_filtered_and_adjusted_yoy2(filtered_and_adjusted_yoy, filter_date):\n",
    "        print(\"Step 17: Creating filtered and adjusted YOY2...\")\n",
    "        \n",
    "        # Filter UFR Date < filter_date (configurable parameter) and GROUP BY\n",
    "        filtered_adjusted2 = filtered_and_adjusted_yoy.filter(\n",
    "            col(\"UFR_Date\") < lit(filter_date)\n",
    "        ).groupBy(\"CustomerID\", \"PRODUCT\", \"UFR_Date\", \"Amount\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        \n",
    "        return filtered_adjusted2\n",
    "    \n",
    "    # Step 18: Final SELECT - MODIFIED for SQL2 (no Revenue_Type)\n",
    "    def create_final_result(filtered_and_adjusted_yoy2):\n",
    "        print(\"Step 18: Creating final result...\")\n",
    "        \n",
    "        # Final GROUP BY only includes CustomerID and PRODUCT (no Revenue_Type)\n",
    "        final_result = filtered_and_adjusted_yoy2.groupBy(\"CustomerID\", \"PRODUCT\", \"UFR_Date\", \"Amount\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        if input_amount == \"ARR\":\n",
    "            final_result = final_result.select(\n",
    "                col(\"PRODUCT\").alias('\"Product\"'),\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                (col(\"UFR_Amount\") / 12).alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "        )\n",
    "        else:  # For MRR\n",
    "            final_result = final_result.select(\n",
    "                col(\"PRODUCT\").alias('\"Product\"'),\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                col(\"UFR_Amount\").alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "        )\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    # Execute all steps\n",
    "    print(f\"\\n🚀 Starting UFR_CP_QoQ processing for table: {table_name}\")\n",
    "    # print(f\"📅 Filter date: {filter_date}\")\n",
    "    \n",
    "    cleaned_data = clean_data(df)\n",
    "    contract_summary = create_contract_summary(cleaned_data)\n",
    "    summary_a, summary_b = create_summaries(cleaned_data)\n",
    "    cartesian_data = create_cartesian_data(summary_a, summary_b)\n",
    "    summary_mrr = create_summary_mrr(cleaned_data)\n",
    "    joined_data = create_joined_data(cartesian_data, summary_mrr)\n",
    "    joined_data2 = create_joined_data2(contract_summary, joined_data)\n",
    "    with_next_date = create_with_next_date(joined_data2)\n",
    "    sorted_data = create_sorted_data(with_next_date)\n",
    "    rolling_sums = create_rolling_sums(sorted_data)\n",
    "    final_metrics = create_final_metrics(rolling_sums)\n",
    "    joined_data3 = create_joined_data3(contract_summary, final_metrics)\n",
    "    unioneddata2 = create_unioneddata2(joined_data3)\n",
    "    main_data1 = create_main_data1(unioneddata2)\n",
    "    expanded_dates_yoy = create_expanded_dates_yoy(main_data1)\n",
    "    filtered_and_adjusted_yoy = create_filtered_and_adjusted_yoy(expanded_dates_yoy)\n",
    "    filtered_and_adjusted_yoy2 = create_filtered_and_adjusted_yoy2(filtered_and_adjusted_yoy, filter_date)\n",
    "    final_result = create_final_result(filtered_and_adjusted_yoy2)\n",
    "    \n",
    "    print(\"✅ UFR_CP_QoQ processing completed successfully!\")\n",
    "    return final_result\n",
    "\n",
    "# Example usage:\n",
    "# result_df = UFR_CP_QoQ(session, \"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST\", Column_mapping_UFR)\n",
    "# result_df.write.mode(\"overwrite\").saveAsTable(\"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST_CP_QoQ1\")\n",
    "\n",
    "\n",
    "def UFR_CP_YoY(session: Session, table_name: str, column_mapping: dict = None, filter_date: str = \"2024-04-01\"):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        session: Snowpark session\n",
    "        table_name: Name of the input table\n",
    "        column_mapping: Dictionary mapping standard column names to actual column names in the input table\n",
    "        filter_date: Date filter for final result (format: 'YYYY-MM-DD')\n",
    "    \n",
    "    Returns:\n",
    "        Snowpark DataFrame with the final result\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use default mapping if none provided\n",
    "    if column_mapping is None:\n",
    "        column_mapping = Column_mapping_UFR\n",
    "    \n",
    "    # Read the input table\n",
    "    df = session.table(table_name)\n",
    "    \n",
    "    # Helper function to get column reference with mapping support\n",
    "    def get_mapped_col(standard_col_name, default_value=None):\n",
    "        \"\"\"\n",
    "        Get column reference using the mapping, with support for missing columns\n",
    "        \n",
    "        Args:\n",
    "            standard_col_name: The standard column key from the mapping\n",
    "            default_value: Default value to use if column is missing (default: None -> lit(\"NA\") for strings, 0 for numbers)\n",
    "        \n",
    "        Returns:\n",
    "            Column expression or literal value\n",
    "        \"\"\"\n",
    "        mapped_name = column_mapping.get(standard_col_name)\n",
    "        \n",
    "        if mapped_name is None:\n",
    "            # Column not present in input, return default value\n",
    "            if default_value is not None:\n",
    "                return lit(default_value)\n",
    "            elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "                return lit(0)  # Default for numeric columns\n",
    "            else:\n",
    "                return lit(\"NA\")  # Default for text columns\n",
    "        \n",
    "        # Column is mapped, try to get it from the DataFrame\n",
    "        columns = df.columns\n",
    "        \n",
    "        # Remove quotes for comparison\n",
    "        clean_mapped_name = mapped_name.strip('\"')\n",
    "        \n",
    "        # Try exact match first\n",
    "        if mapped_name in columns:\n",
    "            return col(mapped_name)\n",
    "        elif clean_mapped_name in columns:\n",
    "            return col(clean_mapped_name)\n",
    "        \n",
    "        # Try case-insensitive match\n",
    "        for c in columns:\n",
    "            if c.upper() == clean_mapped_name.upper():\n",
    "                return col(c)\n",
    "        \n",
    "        # If not found, return default value and print warning\n",
    "        print(f\"Warning: Column {mapped_name} not found in table. Using default value.\")\n",
    "        if default_value is not None:\n",
    "            return lit(default_value)\n",
    "        elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "            return lit(0)\n",
    "        else:\n",
    "            return lit(\"NA\")\n",
    "    \n",
    "    def validate_required_columns():\n",
    "        \"\"\"Validate that all required columns are available\"\"\"\n",
    "        required_columns = [\"DATE\", \"START_DATE\", \"END_DATE\", \"CUSTOMERID\", \"PRODUCT\", \"MRR\"]\n",
    "        missing_required = []\n",
    "        \n",
    "        for req_col in required_columns:\n",
    "            if column_mapping.get(req_col) is None:\n",
    "                missing_required.append(req_col)\n",
    "        \n",
    "        if missing_required:\n",
    "            raise ValueError(f\"Required columns are missing from column mapping: {missing_required}\")\n",
    "        \n",
    "        print(\"✓ All required columns are mapped\")\n",
    "        return True\n",
    "    \n",
    "    # Validate required columns\n",
    "    validate_required_columns()\n",
    "    \n",
    "    # Print column mapping summary\n",
    "    print(\"\\n=== Column Mapping Summary ===\")\n",
    "    for std_name, mapped_name in column_mapping.items():\n",
    "        status = \"✓ Mapped\" if mapped_name is not None else \"✗ Missing (using default)\"\n",
    "        mapped_display = mapped_name if mapped_name is not None else \"None\"\n",
    "        print(f\"{std_name:20} -> {mapped_display:20} {status}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: cleaned_data CTE\n",
    "    def clean_data(df):\n",
    "        print(\"Step 1: Cleaning data...\")\n",
    "        \n",
    "        cleaned = df.select(\n",
    "            # Convert date columns using mapping\n",
    "            get_mapped_col(\"DATE\").alias(\"Date\"),\n",
    "            get_mapped_col(\"START_DATE\").alias(\"Start_Date\"),\n",
    "            get_mapped_col(\"END_DATE\").alias(\"End_Date\"),\n",
    "            \n",
    "            # Clean and standardize text columns using mapping\n",
    "            (trim(coalesce(get_mapped_col(\"CUSTOMERID\"), lit(\"NA\")))).alias(\"CustomerID\"),\n",
    "            (trim(coalesce(get_mapped_col(\"UP_SNL_ID\"), lit(\"NA\")))).alias(\"up_snl_id\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT\"), lit(\"NA\")))).alias(\"PRODUCT\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT_GROUP\"), lit(\"NA\")))).alias(\"PRODUCT_GROUP\"),\n",
    "            (trim(coalesce(get_mapped_col(\"OPPORTUNITY_NUMBER\"), lit(\"NA\")))).alias(\"opportunity_number\"),\n",
    "            (trim(coalesce(get_mapped_col(\"REVENUE_TYPE\"), lit(\"NA\")))).alias(\"Revenue_Type\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TAGGING\"), lit(\"NA\")))).alias(\"Tagging\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TIER\"), lit(\"NA\")))).alias(\"Tier\"),\n",
    "            (trim(coalesce(get_mapped_col(\"SEGMENT\"), lit(\"NA\")))).alias(\"Segment\"),\n",
    "            (trim(coalesce(get_mapped_col(\"Region\"), lit(\"NA\")))).alias(\"Region\"),\n",
    "            \n",
    "            # Handle numeric columns using mapping\n",
    "            coalesce(get_mapped_col(\"MRR\"), lit(0)).alias(\"MRR\"),\n",
    "            coalesce(get_mapped_col(\"CARR\"), lit(0)).alias(\"CARR\"),\n",
    "            coalesce(get_mapped_col(\"ADJUSTED_ARR\"), lit(0)).alias(\"Adjusted_ARR\")\n",
    "        ).filter(\n",
    "            # Apply filters\n",
    "            (col(\"End_Date\").isNotNull()) & \n",
    "            (col(\"MRR\") != 0)\n",
    "        )\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    # Step 2: contract_summary CTE - Same as SQL1 (no Revenue_Type)\n",
    "    def create_contract_summary(cleaned_data):\n",
    "        print(\"Step 2: Creating contract summary...\")\n",
    "        \n",
    "        # GROUP BY only includes CustomerID and PRODUCT (no Revenue_Type)\n",
    "        contract_summary = cleaned_data.groupBy(\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\").agg(\n",
    "            spark_min(\"Date\").alias(\"Min_Date\"),\n",
    "            spark_max(\"Date\").alias(\"Max_Date\")\n",
    "        )\n",
    "        \n",
    "        # Calculate contract size\n",
    "        contract_summary = contract_summary.withColumn(\n",
    "            \"Contract_Size\",\n",
    "            when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 1, \"1. Less than 1 year\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 2, \"2. 1-2 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 3, \"3. 2-3 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 4, \"4. 3-4 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 5, \"5. 4-5 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) >= 5, \"6. 5+ years\")\n",
    "            .otherwise(\"NA\")\n",
    "        )\n",
    "        \n",
    "        return contract_summary\n",
    "    \n",
    "    # Step 3: summary_a and summary_b CTEs - Same as SQL1 (no Revenue_Type)\n",
    "    def create_summaries(cleaned_data):\n",
    "        print(\"Step 3: Creating summaries...\")\n",
    "        \n",
    "        # summary_a only includes CustomerID and PRODUCT (no Revenue_Type)\n",
    "        summary_a = cleaned_data.select(\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\").distinct()\n",
    "        summary_b = cleaned_data.select(\"Date\").distinct()\n",
    "        \n",
    "        return summary_a, summary_b\n",
    "    \n",
    "    # Step 4: cartesian_data CTE - Same as SQL1 (no Revenue_Type)\n",
    "    def create_cartesian_data(summary_a, summary_b):\n",
    "        print(\"Step 4: Creating cartesian data...\")\n",
    "        \n",
    "        # Cross join to create cartesian product - only includes CustomerID and PRODUCT\n",
    "        cartesian = summary_a.crossJoin(summary_b)\n",
    "        \n",
    "        return cartesian\n",
    "    \n",
    "    # Step 5: summary_mrr CTE - Same as SQL1 (no Revenue_Type)\n",
    "    def create_summary_mrr(cleaned_data):\n",
    "        print(\"Step 5: Creating summary MRR...\")\n",
    "        \n",
    "        # GROUP BY only includes CustomerID and PRODUCT (no Revenue_Type)\n",
    "        summary_mrr = cleaned_data.groupBy(\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\", \"Date\").agg(\n",
    "            spark_sum(\"MRR\").alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return summary_mrr\n",
    "    \n",
    "    # Step 6: joined_data CTE - Same as SQL1 (no Revenue_Type)\n",
    "    def create_joined_data(cartesian_data, summary_mrr):\n",
    "        print(\"Step 6: Creating joined data...\")\n",
    "        \n",
    "        # JOIN conditions only include CustomerID and PRODUCT (no Revenue_Type)\n",
    "        joined = cartesian_data.join(\n",
    "            summary_mrr,\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\", \"Date\"],\n",
    "            \"left\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Date\"),\n",
    "            coalesce(col(\"Sum_MRR\"), lit(0)).alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return joined\n",
    "    \n",
    "    # Step 7: joined_data2 CTE - Same as SQL1 (no Revenue_Type)\n",
    "    def create_joined_data2(contract_summary, joined_data):\n",
    "        print(\"Step 7: Creating joined data 2...\")\n",
    "        \n",
    "        # JOIN conditions only include CustomerID and PRODUCT (no Revenue_Type)\n",
    "        joined_data2 = joined_data.join(\n",
    "            contract_summary.select(\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\", \"Max_Date\"),\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\"],\n",
    "            \"inner\"\n",
    "        )\n",
    "        \n",
    "        return joined_data2\n",
    "    \n",
    "    # Step 8: with_next_date CTE - Same as SQL1\n",
    "    def create_with_next_date(joined_data2):\n",
    "        print(\"Step 8: Adding next date...\")\n",
    "        \n",
    "        with_next_date = joined_data2.withColumn(\n",
    "            \"Next_Date\",\n",
    "            date_trunc(\"month\", dateadd(\"day\", lit(1), col(\"End_Date\")))\n",
    "        ).withColumn(\n",
    "            \"Sum_MRR_Adjusted\",\n",
    "            when(\n",
    "                dateadd(\"month\", lit(1), col(\"Max_Date\")) < col(\"Next_Date\"),\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"Sum_MRR\"))\n",
    "        )\n",
    "        \n",
    "        return with_next_date\n",
    "    \n",
    "    # Step 9: sorted_data CTE - Same as SQL1 (no Revenue_Type)\n",
    "    def create_sorted_data(with_next_date):\n",
    "        print(\"Step 9: Sorting data...\")\n",
    "        \n",
    "        # ORDER BY only includes CustomerID and PRODUCT (no Revenue_Type)\n",
    "        sorted_data = with_next_date.orderBy(\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\", \"Date\")\n",
    "        \n",
    "        return sorted_data\n",
    "    \n",
    "    # Step 10: rolling_sums CTE - Same as SQL1 (no Revenue_Type)\n",
    "    def create_rolling_sums(sorted_data):\n",
    "        print(\"Step 10: Creating rolling sums...\")\n",
    "        \n",
    "        # PARTITION BY only includes CustomerID and PRODUCT (no Revenue_Type)\n",
    "        window_spec = Window.partitionBy(\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\").orderBy(\"Date\")\n",
    "        \n",
    "        rolling_sums = sorted_data.withColumn(\n",
    "            \"UFR_TTM\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-11, 0))\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-2, 0))\n",
    "        )\n",
    "        \n",
    "        return rolling_sums\n",
    "    \n",
    "    # Step 11: final_metrics CTE - Same as SQL1\n",
    "    def create_final_metrics(rolling_sums):\n",
    "        print(\"Step 11: Creating final metrics...\")\n",
    "        \n",
    "        final_metrics = rolling_sums.withColumn(\n",
    "            \"UFR_ARR\", col(\"Sum_MRR_Adjusted\") * 12\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M_Annualized\", col(\"UFR_TTM\") * 4\n",
    "        ).filter(\n",
    "            (col(\"Sum_MRR_Adjusted\") != 0) |\n",
    "            (col(\"UFR_TTM\") != 0) |\n",
    "            (col(\"UFR_T3M\") != 0)\n",
    "        )\n",
    "        \n",
    "        return final_metrics\n",
    "    \n",
    "    # Step 12: joined_data3 CTE - Same as SQL1 (no Revenue_Type)\n",
    "    def create_joined_data3(contract_summary, final_metrics):\n",
    "        print(\"Step 12: Creating joined data 3...\")\n",
    "        \n",
    "        # Filter final_metrics where Date equals Max_Date\n",
    "        filtered_metrics = final_metrics.filter(col(\"Date\") == col(\"Max_Date\"))\n",
    "        \n",
    "        # JOIN conditions only include CustomerID and PRODUCT (no Revenue_Type)\n",
    "        joined_data3 = contract_summary.join(\n",
    "            filtered_metrics,\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Start_Date\", \"End_Date\", \"Max_Date\"],\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Max_Date\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"Date\"),\n",
    "            col(\"Sum_MRR_Adjusted\").alias(\"UFR_Amount\"),\n",
    "            col(\"UFR_TTM\"),\n",
    "            col(\"UFR_T3M\"),\n",
    "            col(\"UFR_ARR\"),\n",
    "            col(\"UFR_T3M_Annualized\"),\n",
    "            col(\"Next_Date\")\n",
    "        )\n",
    "        \n",
    "        return joined_data3\n",
    "    \n",
    "    # Step 13: unioneddata2 CTE - Same as SQL1 (no Revenue_Type)\n",
    "    def create_unioneddata2(joined_data3):\n",
    "        print(\"Step 13: Creating unioned data...\")\n",
    "        \n",
    "        # Create separate DataFrames for each UNION - GROUP BY only includes CustomerID and PRODUCT\n",
    "        mrr_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"MRR\"))\n",
    "        \n",
    "        arr_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_ARR\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"ARR\"))\n",
    "        \n",
    "        t3m_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M\"))\n",
    "        \n",
    "        ttm_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_TTM\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"TTM\"))\n",
    "        \n",
    "        t3m_ann_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M_Annualized\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M (Annualized)\"))\n",
    "        \n",
    "        # Union all data\n",
    "        unioneddata2 = mrr_data.unionAll(arr_data).unionAll(t3m_data).unionAll(ttm_data).unionAll(t3m_ann_data)\n",
    "        \n",
    "        return unioneddata2\n",
    "    \n",
    "    # Step 14: main_data1 CTE - Same as SQL1 (no Revenue_Type)\n",
    "    def create_main_data1(unioneddata2):\n",
    "        print(\"Step 14: Creating main data...\")\n",
    "        \n",
    "        main_data1 = unioneddata2.withColumn(\n",
    "            \"Min_Date\", date_trunc(\"month\", col(\"Min_Date\"))\n",
    "        ).withColumn(\n",
    "            \"Next_Date\", date_trunc(\"month\", col(\"Next_Date\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount\") >= 0\n",
    "        )\n",
    "        \n",
    "        return main_data1\n",
    "    \n",
    "    # Step 15: expanded_dates_YOY CTE - MODIFIED for SQL2 (12-month YOY instead of 3-month QoQ)\n",
    "    def create_expanded_dates_yoy(main_data1):\n",
    "        print(\"Step 15: Creating expanded dates for YOY (12-month)...\")\n",
    "        \n",
    "        # Create a generator for 12 months (0, 1, 2, ..., 11) - CHANGED from 3 months\n",
    "        # This simulates the GENERATOR(ROWCOUNT => 12) from SQL2\n",
    "        generator_data = [(i,) for i in range(12)]  # CHANGED: range(12) instead of range(3)\n",
    "        generator_df = session.createDataFrame(generator_data, [\"seq\"])\n",
    "        \n",
    "        # Cross join with main_data1 and add the UFR_Date calculation\n",
    "        expanded_dates = main_data1.crossJoin(generator_df).withColumn(\n",
    "            \"UFR_Date\",\n",
    "            dateadd(\"month\", col(\"seq\"), col(\"Next_Date\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Date\") <= dateadd(\"month\", lit(11), col(\"Next_Date\"))  # CHANGED: lit(11) instead of lit(2)\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Next_Date\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Amount\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"UFR_Amount\"),\n",
    "            col(\"UFR_Date\")\n",
    "        )\n",
    "        \n",
    "        return expanded_dates\n",
    "    \n",
    "    # Step 16: filtered_and_adjusted_YOY CTE - MODIFIED for SQL2 (12-month lookback)\n",
    "    def create_filtered_and_adjusted_yoy(expanded_dates_yoy):\n",
    "        print(\"Step 16: Creating filtered and adjusted YOY (12-month lookback)...\")\n",
    "        \n",
    "        filtered_adjusted = expanded_dates_yoy.withColumn(\n",
    "            \"UFR_Amount_Adjusted\",\n",
    "            when(\n",
    "                col(\"Min_Date\") > dateadd(\"month\", lit(-12), col(\"UFR_Date\")),  # CHANGED: lit(-12) instead of lit(-3)\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"UFR_Amount\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount_Adjusted\") != 0\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"UFR_Date\"),\n",
    "            col(\"Amount\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"UFR_Amount_Adjusted\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        \n",
    "        return filtered_adjusted\n",
    "    \n",
    "    # Step 17: filtered_and_adjusted_YOY2 CTE - Uses configurable filter_date\n",
    "    def create_filtered_and_adjusted_yoy2(filtered_and_adjusted_yoy):\n",
    "        print(\"Step 17: Creating filtered and adjusted YOY2...\")\n",
    "        \n",
    "        # Filter UFR Date < filter_date (configurable parameter) and GROUP BY\n",
    "        filtered_adjusted2 = filtered_and_adjusted_yoy.filter(\n",
    "            col(\"UFR_Date\") < lit(filter_date)\n",
    "        ).groupBy(\"CustomerID\", \"PRODUCT\", \"UFR_Date\", \"Amount\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        \n",
    "        return filtered_adjusted2\n",
    "    \n",
    "    # Step 18: Final SELECT - Same as SQL1 (no Revenue_Type)\n",
    "    def create_final_result(filtered_and_adjusted_yoy2):\n",
    "        print(\"Step 18: Creating final result...\")\n",
    "        \n",
    "        # Final GROUP BY only includes CustomerID and PRODUCT (no Revenue_Type)\n",
    "        final_result = filtered_and_adjusted_yoy2.groupBy(\"CustomerID\", \"PRODUCT\", \"UFR_Date\", \"Amount\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        if input_amount == \"ARR\":\n",
    "            final_result = final_result.select(\n",
    "                col(\"PRODUCT\").alias('\"Product\"'),\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                (col(\"UFR_Amount\") / 12).alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "        )\n",
    "        else:  # For MRR\n",
    "            final_result = final_result.select(\n",
    "                col(\"PRODUCT\").alias('\"Product\"'),\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                col(\"UFR_Amount\").alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "        )\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    # Execute all steps\n",
    "    print(f\"\\n🚀 Starting UFR_CP_YoY processing for table: {table_name}\")\n",
    "    # print(f\"📅 Filter date: {filter_date}\")\n",
    "    \n",
    "    cleaned_data = clean_data(df)\n",
    "    contract_summary = create_contract_summary(cleaned_data)\n",
    "    summary_a, summary_b = create_summaries(cleaned_data)\n",
    "    cartesian_data = create_cartesian_data(summary_a, summary_b)\n",
    "    summary_mrr = create_summary_mrr(cleaned_data)\n",
    "    joined_data = create_joined_data(cartesian_data, summary_mrr)\n",
    "    joined_data2 = create_joined_data2(contract_summary, joined_data)\n",
    "    with_next_date = create_with_next_date(joined_data2)\n",
    "    sorted_data = create_sorted_data(with_next_date)\n",
    "    rolling_sums = create_rolling_sums(sorted_data)\n",
    "    final_metrics = create_final_metrics(rolling_sums)\n",
    "    joined_data3 = create_joined_data3(contract_summary, final_metrics)\n",
    "    unioneddata2 = create_unioneddata2(joined_data3)\n",
    "    main_data1 = create_main_data1(unioneddata2)\n",
    "    expanded_dates_yoy = create_expanded_dates_yoy(main_data1)\n",
    "    filtered_and_adjusted_yoy = create_filtered_and_adjusted_yoy(expanded_dates_yoy)\n",
    "    filtered_and_adjusted_yoy2 = create_filtered_and_adjusted_yoy2(filtered_and_adjusted_yoy)\n",
    "    final_result = create_final_result(filtered_and_adjusted_yoy2)\n",
    "    \n",
    "    print(\"✅ UFR_CP_YoY processing completed successfully!\")\n",
    "    return final_result\n",
    "\n",
    "# Example usage:\n",
    "# result_df = UFR_CP_YoY(session, \"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST\", Column_mapping_UFR)\n",
    "# result_df.write.mode(\"overwrite\").saveAsTable(\"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST_CP_YoY1\")\n",
    "\n",
    "\n",
    "def UFR_CPR_MoM(session: Session, table_name: str, column_mapping: dict = None, filter_date: str = \"2024-04-01\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        session: Snowpark session\n",
    "        table_name: Name of the input table\n",
    "        column_mapping: Dictionary mapping standard column names to actual column names in the input table\n",
    "        filter_date: Date filter for final result (format: 'YYYY-MM-DD')\n",
    "    \n",
    "    Returns:\n",
    "        Snowpark DataFrame with the final result\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use default mapping if none provided\n",
    "    if column_mapping is None:\n",
    "        column_mapping = Column_mapping_UFR\n",
    "    \n",
    "    # Read the input table\n",
    "    df = session.table(table_name)\n",
    "    \n",
    "    # Helper function to get column reference with mapping support\n",
    "    def get_mapped_col(standard_col_name, default_value=None):\n",
    "        \"\"\"\n",
    "        Get column reference using the mapping, with support for missing columns\n",
    "        \n",
    "        Args:\n",
    "            standard_col_name: The standard column key from the mapping\n",
    "            default_value: Default value to use if column is missing (default: None -> lit(\"NA\") for strings, 0 for numbers)\n",
    "        \n",
    "        Returns:\n",
    "            Column expression or literal value\n",
    "        \"\"\"\n",
    "        mapped_name = column_mapping.get(standard_col_name)\n",
    "        \n",
    "        if mapped_name is None:\n",
    "            # Column not present in input, return default value\n",
    "            if default_value is not None:\n",
    "                return lit(default_value)\n",
    "            elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "                return lit(0)  # Default for numeric columns\n",
    "            else:\n",
    "                return lit(\"NA\")  # Default for text columns\n",
    "        \n",
    "        # Column is mapped, try to get it from the DataFrame\n",
    "        columns = df.columns\n",
    "        \n",
    "        # Remove quotes for comparison\n",
    "        clean_mapped_name = mapped_name.strip('\"')\n",
    "        \n",
    "        # Try exact match first\n",
    "        if mapped_name in columns:\n",
    "            return col(mapped_name)\n",
    "        elif clean_mapped_name in columns:\n",
    "            return col(clean_mapped_name)\n",
    "        \n",
    "        # Try case-insensitive match\n",
    "        for c in columns:\n",
    "            if c.upper() == clean_mapped_name.upper():\n",
    "                return col(c)\n",
    "        \n",
    "        # If not found, return default value and print warning\n",
    "        print(f\"Warning: Column {mapped_name} not found in table. Using default value.\")\n",
    "        if default_value is not None:\n",
    "            return lit(default_value)\n",
    "        elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "            return lit(0)\n",
    "        else:\n",
    "            return lit(\"NA\")\n",
    "    \n",
    "    def validate_required_columns():\n",
    "        \"\"\"Validate that all required columns are available\"\"\"\n",
    "        required_columns = [\"DATE\", \"START_DATE\", \"END_DATE\", \"CUSTOMERID\", \"PRODUCT\", \"REVENUE_TYPE\", \"MRR\"]\n",
    "        missing_required = []\n",
    "        \n",
    "        for req_col in required_columns:\n",
    "            if column_mapping.get(req_col) is None:\n",
    "                missing_required.append(req_col)\n",
    "        \n",
    "        if missing_required:\n",
    "            raise ValueError(f\"Required columns are missing from column mapping: {missing_required}\")\n",
    "        \n",
    "        print(\"✓ All required columns are mapped\")\n",
    "        return True\n",
    "    \n",
    "    # Validate required columns\n",
    "    validate_required_columns()\n",
    "    \n",
    "    # Print column mapping summary\n",
    "    print(\"\\n=== Column Mapping Summary ===\")\n",
    "    for std_name, mapped_name in column_mapping.items():\n",
    "        status = \"✓ Mapped\" if mapped_name is not None else \"✗ Missing (using default)\"\n",
    "        mapped_display = mapped_name if mapped_name is not None else \"None\"\n",
    "        print(f\"{std_name:20} -> {mapped_display:20} {status}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: cleaned_data CTE\n",
    "    def clean_data(df):\n",
    "        print(\"Step 1: Cleaning data...\")\n",
    "        \n",
    "        cleaned = df.select(\n",
    "            # Convert date columns using mapping\n",
    "            get_mapped_col(\"DATE\").alias(\"Date\"),\n",
    "            get_mapped_col(\"START_DATE\").alias(\"Start_Date\"),\n",
    "            get_mapped_col(\"END_DATE\").alias(\"End_Date\"),\n",
    "            \n",
    "            # Clean and standardize text columns using mapping\n",
    "            (trim(coalesce(get_mapped_col(\"CUSTOMERID\"), lit(\"NA\")))).alias(\"CustomerID\"),\n",
    "            (trim(coalesce(get_mapped_col(\"UP_SNL_ID\"), lit(\"NA\")))).alias(\"up_snl_id\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT\"), lit(\"NA\")))).alias(\"PRODUCT\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT_GROUP\"), lit(\"NA\")))).alias(\"PRODUCT_GROUP\"),\n",
    "            (trim(coalesce(get_mapped_col(\"OPPORTUNITY_NUMBER\"), lit(\"NA\")))).alias(\"opportunity_number\"),\n",
    "            (trim(coalesce(get_mapped_col(\"REVENUE_TYPE\"), lit(\"NA\")))).alias(\"Revenue_Type\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TAGGING\"), lit(\"NA\")))).alias(\"Tagging\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TIER\"), lit(\"NA\")))).alias(\"Tier\"),\n",
    "            (trim(coalesce(get_mapped_col(\"SEGMENT\"), lit(\"NA\")))).alias(\"Segment\"),\n",
    "            (trim(coalesce(get_mapped_col(\"Region\"), lit(\"NA\")))).alias(\"Region\"),\n",
    "            \n",
    "            # Handle numeric columns using mapping\n",
    "            coalesce(get_mapped_col(\"MRR\"), lit(0)).alias(\"MRR\"),\n",
    "            coalesce(get_mapped_col(\"CARR\"), lit(0)).alias(\"CARR\"),\n",
    "            coalesce(get_mapped_col(\"ADJUSTED_ARR\"), lit(0)).alias(\"Adjusted_ARR\")\n",
    "        ).filter(\n",
    "            # Apply filters\n",
    "            (col(\"End_Date\").isNotNull()) & \n",
    "            (col(\"MRR\") != 0)\n",
    "        )\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    # Step 2: contract_summary CTE\n",
    "    def create_contract_summary(cleaned_data):\n",
    "        print(\"Step 2: Creating contract summary...\")\n",
    "        \n",
    "        contract_summary = cleaned_data.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\").agg(\n",
    "            spark_min(\"Date\").alias(\"Min_Date\"),\n",
    "            spark_max(\"Date\").alias(\"Max_Date\")\n",
    "        )\n",
    "        \n",
    "        # Calculate contract size\n",
    "        contract_summary = contract_summary.withColumn(\n",
    "            \"Contract_Size\",\n",
    "            when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 1, \"1. Less than 1 year\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 2, \"2. 1-2 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 3, \"3. 2-3 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 4, \"4. 3-4 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 5, \"5. 4-5 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) >= 5, \"6. 5+ years\")\n",
    "            .otherwise(\"NA\")\n",
    "        )\n",
    "        \n",
    "        return contract_summary\n",
    "    \n",
    "    # Step 3: summary_a and summary_b CTEs\n",
    "    def create_summaries(cleaned_data):\n",
    "        print(\"Step 3: Creating summaries...\")\n",
    "        \n",
    "        summary_a = cleaned_data.select(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\").distinct()\n",
    "        summary_b = cleaned_data.select(\"Date\").distinct()\n",
    "        \n",
    "        return summary_a, summary_b\n",
    "    \n",
    "    # Step 4: cartesian_data CTE\n",
    "    def create_cartesian_data(summary_a, summary_b):\n",
    "        print(\"Step 4: Creating cartesian data...\")\n",
    "        \n",
    "        # Cross join to create cartesian product\n",
    "        cartesian = summary_a.crossJoin(summary_b)\n",
    "        \n",
    "        return cartesian\n",
    "    \n",
    "    # Step 5: summary_mrr CTE\n",
    "    def create_summary_mrr(cleaned_data):\n",
    "        print(\"Step 5: Creating summary MRR...\")\n",
    "        \n",
    "        summary_mrr = cleaned_data.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Date\").agg(\n",
    "            spark_sum(\"MRR\").alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return summary_mrr\n",
    "    \n",
    "    # Step 6: joined_data CTE\n",
    "    def create_joined_data(cartesian_data, summary_mrr):\n",
    "        print(\"Step 6: Creating joined data...\")\n",
    "        \n",
    "        joined = cartesian_data.join(\n",
    "            summary_mrr,\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Date\"],\n",
    "            \"left\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Revenue_Type\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Date\"),\n",
    "            coalesce(col(\"Sum_MRR\"), lit(0)).alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return joined\n",
    "    \n",
    "    # Step 7: joined_data2 CTE\n",
    "    def create_joined_data2(contract_summary, joined_data):\n",
    "        print(\"Step 7: Creating joined data 2...\")\n",
    "        \n",
    "        joined_data2 = joined_data.join(\n",
    "            contract_summary.select(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Max_Date\"),\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\"],\n",
    "            \"inner\"\n",
    "        )\n",
    "        \n",
    "        return joined_data2\n",
    "    \n",
    "    # Step 8: with_next_date CTE\n",
    "    def create_with_next_date(joined_data2):\n",
    "        print(\"Step 8: Adding next date...\")\n",
    "        \n",
    "        with_next_date = joined_data2.withColumn(\n",
    "            \"Next_Date\",\n",
    "            date_trunc(\"month\", dateadd(\"day\", lit(1), col(\"End_Date\")))\n",
    "        ).withColumn(\n",
    "            \"Sum_MRR_Adjusted\",\n",
    "            when(\n",
    "                dateadd(\"month\", lit(1), col(\"Max_Date\")) < col(\"Next_Date\"),\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"Sum_MRR\"))\n",
    "        )\n",
    "        \n",
    "        return with_next_date\n",
    "    \n",
    "    # Step 9: sorted_data CTE\n",
    "    def create_sorted_data(with_next_date):\n",
    "        print(\"Step 9: Sorting data...\")\n",
    "        \n",
    "        sorted_data = with_next_date.orderBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Date\")\n",
    "        \n",
    "        return sorted_data\n",
    "    \n",
    "    # Step 10: rolling_sums CTE\n",
    "    def create_rolling_sums(sorted_data):\n",
    "        print(\"Step 10: Creating rolling sums...\")\n",
    "        \n",
    "        window_spec = Window.partitionBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\").orderBy(\"Date\")\n",
    "        \n",
    "        rolling_sums = sorted_data.withColumn(\n",
    "            \"UFR_TTM\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-11, 0))\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-2, 0))\n",
    "        )\n",
    "        \n",
    "        return rolling_sums\n",
    "    \n",
    "    # Step 11: final_metrics CTE\n",
    "    def create_final_metrics(rolling_sums):\n",
    "        print(\"Step 11: Creating final metrics...\")\n",
    "        \n",
    "        final_metrics = rolling_sums.withColumn(\n",
    "            \"UFR_ARR\", col(\"Sum_MRR_Adjusted\") * 12\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M_Annualized\", col(\"UFR_TTM\") * 4\n",
    "        ).filter(\n",
    "            (col(\"Sum_MRR_Adjusted\") != 0) |\n",
    "            (col(\"UFR_TTM\") != 0) |\n",
    "            (col(\"UFR_T3M\") != 0)\n",
    "        )\n",
    "        \n",
    "        return final_metrics\n",
    "    \n",
    "    # Step 12: joined_data3 CTE\n",
    "    def create_joined_data3(contract_summary, final_metrics):\n",
    "        print(\"Step 12: Creating joined data 3...\")\n",
    "        \n",
    "        # Filter final_metrics where Date equals Max_Date\n",
    "        filtered_metrics = final_metrics.filter(col(\"Date\") == col(\"Max_Date\"))\n",
    "        \n",
    "        joined_data3 = contract_summary.join(\n",
    "            filtered_metrics,\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Max_Date\"],\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Revenue_Type\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Max_Date\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"Date\"),\n",
    "            col(\"Sum_MRR_Adjusted\").alias(\"UFR_Amount\"),\n",
    "            col(\"UFR_TTM\"),\n",
    "            col(\"UFR_T3M\"),\n",
    "            col(\"UFR_ARR\"),\n",
    "            col(\"UFR_T3M_Annualized\"),\n",
    "            col(\"Next_Date\")\n",
    "        )\n",
    "        \n",
    "        return joined_data3\n",
    "    \n",
    "    # Step 13: unioneddata2 CTE\n",
    "    def create_unioneddata2(joined_data3):\n",
    "        print(\"Step 13: Creating unioned data...\")\n",
    "        \n",
    "        # Create separate DataFrames for each UNION\n",
    "        mrr_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"MRR\"))\n",
    "        \n",
    "        arr_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_ARR\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"ARR\"))\n",
    "        \n",
    "        t3m_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M\"))\n",
    "        \n",
    "        ttm_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_TTM\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"TTM\"))\n",
    "        \n",
    "        t3m_ann_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M_Annualized\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M (Annualized)\"))\n",
    "        \n",
    "        # Union all data\n",
    "        unioneddata2 = mrr_data.unionAll(arr_data).unionAll(t3m_data).unionAll(ttm_data).unionAll(t3m_ann_data)\n",
    "        \n",
    "        return unioneddata2\n",
    "    \n",
    "    # Step 14: main_data1 CTE\n",
    "    def create_main_data1(unioneddata2):\n",
    "        print(\"Step 14: Creating main data...\")\n",
    "        \n",
    "        main_data1 = unioneddata2.withColumn(\n",
    "            \"Min_Date\", date_trunc(\"month\", col(\"Min_Date\"))\n",
    "        ).withColumn(\n",
    "            \"Next_Date\", date_trunc(\"month\", col(\"Next_Date\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount\") >= 0\n",
    "        )\n",
    "        \n",
    "        return main_data1\n",
    "    \n",
    "    # Step 15: aggregated_data CTE\n",
    "    def create_aggregated_data(main_data1):\n",
    "        print(\"Step 15: Creating aggregated data...\")\n",
    "        \n",
    "        aggregated_data = main_data1.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Next_Date\", \"Amount\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        ).withColumnRenamed(\"Next_Date\", \"UFR_Date\")\n",
    "        \n",
    "        return aggregated_data\n",
    "    \n",
    "    # Step 16: Final SELECT\n",
    "    def create_final_result(aggregated_data):\n",
    "        print(\"Step 16: Creating final result...\")\n",
    "        \n",
    "        # Filter UFR Date < filter_date (configurable parameter)\n",
    "        final_result = aggregated_data.filter(\n",
    "            col(\"UFR_Date\") < lit(filter_date)\n",
    "        ).groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"UFR_Date\", \"Amount\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        if input_amount == \"ARR\":\n",
    "            final_result = final_result.select(\n",
    "                col(\"PRODUCT\").alias('\"Product\"'),\n",
    "                col(\"Revenue_Type\").alias('\"Revenue Type\"'),\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                (col(\"UFR_Amount\") / 12).alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "        )\n",
    "        else:  # For MRR\n",
    "            final_result = final_result.select(\n",
    "                col(\"PRODUCT\").alias('\"Product\"'),\n",
    "                col(\"Revenue_Type\").alias('\"Revenue Type\"'),\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                col(\"UFR_Amount\").alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "        )\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    # Execute all steps\n",
    "    print(f\"\\n🚀 Starting UFR_CPR_MoM processing for table: {table_name}\")\n",
    "    # print(f\"📅 Filter date: {filter_date}\")\n",
    "    \n",
    "    cleaned_data = clean_data(df)\n",
    "    contract_summary = create_contract_summary(cleaned_data)\n",
    "    summary_a, summary_b = create_summaries(cleaned_data)\n",
    "    cartesian_data = create_cartesian_data(summary_a, summary_b)\n",
    "    summary_mrr = create_summary_mrr(cleaned_data)\n",
    "    joined_data = create_joined_data(cartesian_data, summary_mrr)\n",
    "    joined_data2 = create_joined_data2(contract_summary, joined_data)\n",
    "    with_next_date = create_with_next_date(joined_data2)\n",
    "    sorted_data = create_sorted_data(with_next_date)\n",
    "    rolling_sums = create_rolling_sums(sorted_data)\n",
    "    final_metrics = create_final_metrics(rolling_sums)\n",
    "    joined_data3 = create_joined_data3(contract_summary, final_metrics)\n",
    "    unioneddata2 = create_unioneddata2(joined_data3)\n",
    "    main_data1 = create_main_data1(unioneddata2)\n",
    "    aggregated_data = create_aggregated_data(main_data1)\n",
    "    final_result = create_final_result(aggregated_data)\n",
    "    \n",
    "    print(\"✅ UFR_CPR_MoM processing completed successfully!\")\n",
    "    return final_result\n",
    "\n",
    "# Example usage:\n",
    "# result_df = UFR_CPR_MoM(session, \"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST\", Column_mapping_UFR)\n",
    "# result_df.write.mode(\"overwrite\").saveAsTable(\"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST_CPR_MoM1\")\n",
    "\n",
    "\n",
    "\n",
    "def UFR_CPR_QoQ(session: Session, table_name: str, column_mapping: dict = None, filter_date: str = \"2024-04-01\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        session: Snowpark session\n",
    "        table_name: Name of the input table\n",
    "        column_mapping: Dictionary mapping standard column names to actual column names in the input table\n",
    "        filter_date: Date filter for final result (format: 'YYYY-MM-DD')\n",
    "    \n",
    "    Returns:\n",
    "        Snowpark DataFrame with the final result\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use default mapping if none provided\n",
    "    if column_mapping is None:\n",
    "        column_mapping = Column_mapping_UFR\n",
    "    \n",
    "    # Read the input table\n",
    "    df = session.table(table_name)\n",
    "    \n",
    "    # Helper function to get column reference with mapping support\n",
    "    def get_mapped_col(standard_col_name, default_value=None):\n",
    "        \"\"\"\n",
    "        Get column reference using the mapping, with support for missing columns\n",
    "        \n",
    "        Args:\n",
    "            standard_col_name: The standard column key from the mapping\n",
    "            default_value: Default value to use if column is missing (default: None -> lit(\"NA\") for strings, 0 for numbers)\n",
    "        \n",
    "        Returns:\n",
    "            Column expression or literal value\n",
    "        \"\"\"\n",
    "        mapped_name = column_mapping.get(standard_col_name)\n",
    "        \n",
    "        if mapped_name is None:\n",
    "            # Column not present in input, return default value\n",
    "            if default_value is not None:\n",
    "                return lit(default_value)\n",
    "            elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "                return lit(0)  # Default for numeric columns\n",
    "            else:\n",
    "                return lit(\"NA\")  # Default for text columns\n",
    "        \n",
    "        # Column is mapped, try to get it from the DataFrame\n",
    "        columns = df.columns\n",
    "        \n",
    "        # Remove quotes for comparison\n",
    "        clean_mapped_name = mapped_name.strip('\"')\n",
    "        \n",
    "        # Try exact match first\n",
    "        if mapped_name in columns:\n",
    "            return col(mapped_name)\n",
    "        elif clean_mapped_name in columns:\n",
    "            return col(clean_mapped_name)\n",
    "        \n",
    "        # Try case-insensitive match\n",
    "        for c in columns:\n",
    "            if c.upper() == clean_mapped_name.upper():\n",
    "                return col(c)\n",
    "        \n",
    "        # If not found, return default value and print warning\n",
    "        print(f\"Warning: Column {mapped_name} not found in table. Using default value.\")\n",
    "        if default_value is not None:\n",
    "            return lit(default_value)\n",
    "        elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "            return lit(0)\n",
    "        else:\n",
    "            return lit(\"NA\")\n",
    "    \n",
    "    def validate_required_columns():\n",
    "        \"\"\"Validate that all required columns are available\"\"\"\n",
    "        required_columns = [\"DATE\", \"START_DATE\", \"END_DATE\", \"CUSTOMERID\", \"PRODUCT\", \"REVENUE_TYPE\", \"MRR\"]\n",
    "        missing_required = []\n",
    "        \n",
    "        for req_col in required_columns:\n",
    "            if column_mapping.get(req_col) is None:\n",
    "                missing_required.append(req_col)\n",
    "        \n",
    "        if missing_required:\n",
    "            raise ValueError(f\"Required columns are missing from column mapping: {missing_required}\")\n",
    "        \n",
    "        print(\"✓ All required columns are mapped\")\n",
    "        return True\n",
    "    \n",
    "    # Validate required columns\n",
    "    validate_required_columns()\n",
    "    \n",
    "    # Print column mapping summary\n",
    "    print(\"\\n=== Column Mapping Summary ===\")\n",
    "    for std_name, mapped_name in column_mapping.items():\n",
    "        status = \"✓ Mapped\" if mapped_name is not None else \"✗ Missing (using default)\"\n",
    "        mapped_display = mapped_name if mapped_name is not None else \"None\"\n",
    "        print(f\"{std_name:20} -> {mapped_display:20} {status}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: cleaned_data CTE\n",
    "    def clean_data(df):\n",
    "        print(\"Step 1: Cleaning data...\")\n",
    "        \n",
    "        cleaned = df.select(\n",
    "            # Convert date columns using mapping\n",
    "            get_mapped_col(\"DATE\").alias(\"Date\"),\n",
    "            get_mapped_col(\"START_DATE\").alias(\"Start_Date\"),\n",
    "            get_mapped_col(\"END_DATE\").alias(\"End_Date\"),\n",
    "            \n",
    "            # Clean and standardize text columns using mapping\n",
    "            (trim(coalesce(get_mapped_col(\"CUSTOMERID\"), lit(\"NA\")))).alias(\"CustomerID\"),\n",
    "            (trim(coalesce(get_mapped_col(\"UP_SNL_ID\"), lit(\"NA\")))).alias(\"up_snl_id\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT\"), lit(\"NA\")))).alias(\"PRODUCT\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT_GROUP\"), lit(\"NA\")))).alias(\"PRODUCT_GROUP\"),\n",
    "            (trim(coalesce(get_mapped_col(\"OPPORTUNITY_NUMBER\"), lit(\"NA\")))).alias(\"opportunity_number\"),\n",
    "            (trim(coalesce(get_mapped_col(\"REVENUE_TYPE\"), lit(\"NA\")))).alias(\"Revenue_Type\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TAGGING\"), lit(\"NA\")))).alias(\"Tagging\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TIER\"), lit(\"NA\")))).alias(\"Tier\"),\n",
    "            (trim(coalesce(get_mapped_col(\"SEGMENT\"), lit(\"NA\")))).alias(\"Segment\"),\n",
    "            (trim(coalesce(get_mapped_col(\"Region\"), lit(\"NA\")))).alias(\"Region\"),\n",
    "            \n",
    "            # Handle numeric columns using mapping\n",
    "            coalesce(get_mapped_col(\"MRR\"), lit(0)).alias(\"MRR\"),\n",
    "            coalesce(get_mapped_col(\"CARR\"), lit(0)).alias(\"CARR\"),\n",
    "            coalesce(get_mapped_col(\"ADJUSTED_ARR\"), lit(0)).alias(\"Adjusted_ARR\")\n",
    "        ).filter(\n",
    "            # Apply filters\n",
    "            (col(\"End_Date\").isNotNull()) & \n",
    "            (col(\"MRR\") != 0)\n",
    "        )\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    # Step 2: contract_summary CTE - INCLUDES Revenue_Type\n",
    "    def create_contract_summary(cleaned_data):\n",
    "        print(\"Step 2: Creating contract summary...\")\n",
    "        \n",
    "        # GROUP BY includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        contract_summary = cleaned_data.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\").agg(\n",
    "            spark_min(\"Date\").alias(\"Min_Date\"),\n",
    "            spark_max(\"Date\").alias(\"Max_Date\")\n",
    "        )\n",
    "        \n",
    "        # Calculate contract size\n",
    "        contract_summary = contract_summary.withColumn(\n",
    "            \"Contract_Size\",\n",
    "            when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 1, \"1. Less than 1 year\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 2, \"2. 1-2 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 3, \"3. 2-3 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 4, \"4. 3-4 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 5, \"5. 4-5 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) >= 5, \"6. 5+ years\")\n",
    "            .otherwise(\"NA\")\n",
    "        )\n",
    "        \n",
    "        return contract_summary\n",
    "    \n",
    "    # Step 3: summary_a and summary_b CTEs - INCLUDES Revenue_Type\n",
    "    def create_summaries(cleaned_data):\n",
    "        print(\"Step 3: Creating summaries...\")\n",
    "        \n",
    "        # summary_a includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        summary_a = cleaned_data.select(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\").distinct()\n",
    "        summary_b = cleaned_data.select(\"Date\").distinct()\n",
    "        \n",
    "        return summary_a, summary_b\n",
    "    \n",
    "    # Step 4: cartesian_data CTE - INCLUDES Revenue_Type\n",
    "    def create_cartesian_data(summary_a, summary_b):\n",
    "        print(\"Step 4: Creating cartesian data...\")\n",
    "        \n",
    "        # Cross join to create cartesian product - includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        cartesian = summary_a.crossJoin(summary_b)\n",
    "        \n",
    "        return cartesian\n",
    "    \n",
    "    # Step 5: summary_mrr CTE - INCLUDES Revenue_Type\n",
    "    def create_summary_mrr(cleaned_data):\n",
    "        print(\"Step 5: Creating summary MRR...\")\n",
    "        \n",
    "        # GROUP BY includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        summary_mrr = cleaned_data.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Date\").agg(\n",
    "            spark_sum(\"MRR\").alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return summary_mrr\n",
    "    \n",
    "    # Step 6: joined_data CTE - INCLUDES Revenue_Type\n",
    "    def create_joined_data(cartesian_data, summary_mrr):\n",
    "        print(\"Step 6: Creating joined data...\")\n",
    "        \n",
    "        # JOIN conditions include CustomerID, PRODUCT, and Revenue_Type\n",
    "        joined = cartesian_data.join(\n",
    "            summary_mrr,\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Date\"],\n",
    "            \"left\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Revenue_Type\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Date\"),\n",
    "            coalesce(col(\"Sum_MRR\"), lit(0)).alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return joined\n",
    "    \n",
    "    # Step 7: joined_data2 CTE - INCLUDES Revenue_Type\n",
    "    def create_joined_data2(contract_summary, joined_data):\n",
    "        print(\"Step 7: Creating joined data 2...\")\n",
    "        \n",
    "        # JOIN conditions include CustomerID, PRODUCT, and Revenue_Type\n",
    "        joined_data2 = joined_data.join(\n",
    "            contract_summary.select(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Max_Date\"),\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\"],\n",
    "            \"inner\"\n",
    "        )\n",
    "        \n",
    "        return joined_data2\n",
    "    \n",
    "    # Step 8: with_next_date CTE\n",
    "    def create_with_next_date(joined_data2):\n",
    "        print(\"Step 8: Adding next date...\")\n",
    "        \n",
    "        with_next_date = joined_data2.withColumn(\n",
    "            \"Next_Date\",\n",
    "            date_trunc(\"month\", dateadd(\"day\", lit(1), col(\"End_Date\")))\n",
    "        ).withColumn(\n",
    "            \"Sum_MRR_Adjusted\",\n",
    "            when(\n",
    "                dateadd(\"month\", lit(1), col(\"Max_Date\")) < col(\"Next_Date\"),\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"Sum_MRR\"))\n",
    "        )\n",
    "        \n",
    "        return with_next_date\n",
    "    \n",
    "    # Step 9: sorted_data CTE - INCLUDES Revenue_Type\n",
    "    def create_sorted_data(with_next_date):\n",
    "        print(\"Step 9: Sorting data...\")\n",
    "        \n",
    "        # ORDER BY includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        sorted_data = with_next_date.orderBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Date\")\n",
    "        \n",
    "        return sorted_data\n",
    "    \n",
    "    # Step 10: rolling_sums CTE - INCLUDES Revenue_Type\n",
    "    def create_rolling_sums(sorted_data):\n",
    "        print(\"Step 10: Creating rolling sums...\")\n",
    "        \n",
    "        # PARTITION BY includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        window_spec = Window.partitionBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\").orderBy(\"Date\")\n",
    "        \n",
    "        rolling_sums = sorted_data.withColumn(\n",
    "            \"UFR_TTM\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-11, 0))\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-2, 0))\n",
    "        )\n",
    "        \n",
    "        return rolling_sums\n",
    "    \n",
    "    # Step 11: final_metrics CTE\n",
    "    def create_final_metrics(rolling_sums):\n",
    "        print(\"Step 11: Creating final metrics...\")\n",
    "        \n",
    "        final_metrics = rolling_sums.withColumn(\n",
    "            \"UFR_ARR\", col(\"Sum_MRR_Adjusted\") * 12\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M_Annualized\", col(\"UFR_TTM\") * 4\n",
    "        ).filter(\n",
    "            (col(\"Sum_MRR_Adjusted\") != 0) |\n",
    "            (col(\"UFR_TTM\") != 0) |\n",
    "            (col(\"UFR_T3M\") != 0)\n",
    "        )\n",
    "        \n",
    "        return final_metrics\n",
    "    \n",
    "    # Step 12: joined_data3 CTE - INCLUDES Revenue_Type\n",
    "    def create_joined_data3(contract_summary, final_metrics):\n",
    "        print(\"Step 12: Creating joined data 3...\")\n",
    "        \n",
    "        # Filter final_metrics where Date equals Max_Date\n",
    "        filtered_metrics = final_metrics.filter(col(\"Date\") == col(\"Max_Date\"))\n",
    "        \n",
    "        # JOIN conditions include CustomerID, PRODUCT, and Revenue_Type\n",
    "        joined_data3 = contract_summary.join(\n",
    "            filtered_metrics,\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Max_Date\"],\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Revenue_Type\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Max_Date\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"Date\"),\n",
    "            col(\"Sum_MRR_Adjusted\").alias(\"UFR_Amount\"),\n",
    "            col(\"UFR_TTM\"),\n",
    "            col(\"UFR_T3M\"),\n",
    "            col(\"UFR_ARR\"),\n",
    "            col(\"UFR_T3M_Annualized\"),\n",
    "            col(\"Next_Date\")\n",
    "        )\n",
    "        \n",
    "        return joined_data3\n",
    "    \n",
    "    # Step 13: unioneddata2 CTE - INCLUDES Revenue_Type\n",
    "    def create_unioneddata2(joined_data3):\n",
    "        print(\"Step 13: Creating unioned data...\")\n",
    "        \n",
    "        # Create separate DataFrames for each UNION - GROUP BY includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        mrr_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"MRR\"))\n",
    "        \n",
    "        arr_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_ARR\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"ARR\"))\n",
    "        \n",
    "        t3m_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M\"))\n",
    "        \n",
    "        ttm_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_TTM\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"TTM\"))\n",
    "        \n",
    "        t3m_ann_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M_Annualized\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M (Annualized)\"))\n",
    "        \n",
    "        # Union all data\n",
    "        unioneddata2 = mrr_data.unionAll(arr_data).unionAll(t3m_data).unionAll(ttm_data).unionAll(t3m_ann_data)\n",
    "        \n",
    "        return unioneddata2\n",
    "    \n",
    "    # Step 14: main_data1 CTE - INCLUDES Revenue_Type\n",
    "    def create_main_data1(unioneddata2):\n",
    "        print(\"Step 14: Creating main data...\")\n",
    "        \n",
    "        main_data1 = unioneddata2.withColumn(\n",
    "            \"Min_Date\", date_trunc(\"month\", col(\"Min_Date\"))\n",
    "        ).withColumn(\n",
    "            \"Next_Date\", date_trunc(\"month\", col(\"Next_Date\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount\") >= 0\n",
    "        )\n",
    "        \n",
    "        return main_data1\n",
    "    \n",
    "    # Step 15: expanded_dates_YOY CTE - INCLUDES Revenue_Type\n",
    "    def create_expanded_dates_yoy(main_data1):\n",
    "        print(\"Step 15: Creating expanded dates for YOY...\")\n",
    "        \n",
    "        # Create a generator for 3 months (0, 1, 2)\n",
    "        # This simulates the GENERATOR(ROWCOUNT => 3) from SQL\n",
    "        generator_data = [(0,), (1,), (2,)]\n",
    "        generator_df = session.createDataFrame(generator_data, [\"seq\"])\n",
    "        \n",
    "        # Cross join with main_data1 and add the UFR_Date calculation\n",
    "        expanded_dates = main_data1.crossJoin(generator_df).withColumn(\n",
    "            \"UFR_Date\",\n",
    "            dateadd(\"month\", col(\"seq\"), col(\"Next_Date\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Date\") <= dateadd(\"month\", lit(2), col(\"Next_Date\"))\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Revenue_Type\"),\n",
    "            col(\"Next_Date\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Amount\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"UFR_Amount\"),\n",
    "            col(\"UFR_Date\")\n",
    "        )\n",
    "        \n",
    "        return expanded_dates\n",
    "    \n",
    "    # Step 16: filtered_and_adjusted_YOY CTE - INCLUDES Revenue_Type\n",
    "    def create_filtered_and_adjusted_yoy(expanded_dates_yoy):\n",
    "        print(\"Step 16: Creating filtered and adjusted YOY...\")\n",
    "        \n",
    "        filtered_adjusted = expanded_dates_yoy.withColumn(\n",
    "            \"UFR_Amount_Adjusted\",\n",
    "            when(\n",
    "                col(\"Min_Date\") > dateadd(\"month\", lit(-3), col(\"UFR_Date\")),\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"UFR_Amount\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount_Adjusted\") != 0\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Revenue_Type\"),\n",
    "            col(\"UFR_Date\"),\n",
    "            col(\"Amount\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"UFR_Amount_Adjusted\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        \n",
    "        return filtered_adjusted\n",
    "    \n",
    "    # Step 17: filtered_and_adjusted_YOY2 CTE - INCLUDES Revenue_Type\n",
    "    def create_filtered_and_adjusted_yoy2(filtered_and_adjusted_yoy):\n",
    "        print(\"Step 17: Creating filtered and adjusted YOY2...\")\n",
    "        \n",
    "        # Filter UFR Date < filter_date and GROUP BY includes Revenue_Type\n",
    "        filtered_adjusted2 = filtered_and_adjusted_yoy.filter(\n",
    "            col(\"UFR_Date\") < lit(filter_date)\n",
    "        ).groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"UFR_Date\", \"Amount\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        ).select(\n",
    "            # Rename columns to match the requested output format\n",
    "            col(\"PRODUCT\").alias('\"Product\"'),\n",
    "            col(\"CustomerID\").alias('\"CustName\"'),\n",
    "            col(\"Revenue_Type\").alias('\"Revenue Type\"'),\n",
    "            col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "            col(\"UFR_Amount\").alias('\"UFR Amount\"'),\n",
    "            col(\"AMOUNT\").alias('\"Amount\"')  # Amount remains the same\n",
    "        )\n",
    "        \n",
    "        return filtered_adjusted2\n",
    "    \n",
    "    # Step 18: Final SELECT - INCLUDES Revenue_Type\n",
    "    # def create_final_result(filtered_and_adjusted_yoy2):\n",
    "    #     print(\"Step 18: Creating final result...\")\n",
    "        \n",
    "    #     # Final GROUP BY includes CustomerID, PRODUCT, and Revenue_Type\n",
    "    #     final_result = filtered_and_adjusted_yoy2.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"UFR_Date\", \"Amount\").agg(\n",
    "    #         spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "    #     )\n",
    "        \n",
    "    #     return final_result\n",
    "\n",
    "    # Step 18: Fixed Final SELECT - INCLUDES Revenue_Type\n",
    "    def create_final_result(filtered_and_adjusted_yoy2):\n",
    "        print(\"Step 18: Creating final result...\")\n",
    "        \n",
    "        # Final GROUP BY using the quoted column names that exist in filtered_and_adjusted_yoy2\n",
    "        final_result = filtered_and_adjusted_yoy2.groupBy('\"CustName\"', '\"Product\"', '\"Revenue Type\"', '\"UFR Date\"', '\"Amount\"').agg(\n",
    "            spark_sum('\"UFR Amount\"').alias('\"UFR Amount\"')\n",
    "        )\n",
    "        if input_amount == \"ARR\":\n",
    "            final_result = final_result.select(\n",
    "                col('\"Product\"'),\n",
    "                col('\"Revenue Type\"'),\n",
    "                col('\"CustName\"'),\n",
    "                col('\"UFR Date\"'),\n",
    "                (col('\"UFR Amount\"') / 12).alias('\"UFR Amount\"'),\n",
    "                col('\"Amount\"')\n",
    "        )\n",
    "        else:  # For MRR\n",
    "            final_result = final_result.select(\n",
    "                col('\"Product\"'),\n",
    "                col('\"CustName\"'),\n",
    "                col('\"Revenue Type\"'),\n",
    "                col('\"UFR Date\"'),\n",
    "                col('\"UFR Amount\"'),\n",
    "                col('\"Amount\"')\n",
    "        )\n",
    "        return final_result\n",
    "    \n",
    "    # Execute all steps\n",
    "    print(f\"\\n🚀 Starting UFR_CPR_QoQ processing for table: {table_name}\")\n",
    "    # print(f\"📅 Filter date: {filter_date}\")\n",
    "    \n",
    "    cleaned_data = clean_data(df)\n",
    "    contract_summary = create_contract_summary(cleaned_data)\n",
    "    summary_a, summary_b = create_summaries(cleaned_data)\n",
    "    cartesian_data = create_cartesian_data(summary_a, summary_b)\n",
    "    summary_mrr = create_summary_mrr(cleaned_data)\n",
    "    joined_data = create_joined_data(cartesian_data, summary_mrr)\n",
    "    joined_data2 = create_joined_data2(contract_summary, joined_data)\n",
    "    with_next_date = create_with_next_date(joined_data2)\n",
    "    sorted_data = create_sorted_data(with_next_date)\n",
    "    rolling_sums = create_rolling_sums(sorted_data)\n",
    "    final_metrics = create_final_metrics(rolling_sums)\n",
    "    joined_data3 = create_joined_data3(contract_summary, final_metrics)\n",
    "    unioneddata2 = create_unioneddata2(joined_data3)\n",
    "    main_data1 = create_main_data1(unioneddata2)\n",
    "    expanded_dates_yoy = create_expanded_dates_yoy(main_data1)\n",
    "    filtered_and_adjusted_yoy = create_filtered_and_adjusted_yoy(expanded_dates_yoy)\n",
    "    filtered_and_adjusted_yoy2 = create_filtered_and_adjusted_yoy2(filtered_and_adjusted_yoy)\n",
    "    final_result = create_final_result(filtered_and_adjusted_yoy2)\n",
    "    \n",
    "    print(\"✅ UFR_CPR_QoQ processing completed successfully!\")\n",
    "    return final_result\n",
    "\n",
    "# Example usage:\n",
    "# result_df = UFR_CPR_QoQ(session, \"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST\", Column_mapping_UFR)\n",
    "# result_df.write.mode(\"overwrite\").saveAsTable(\"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST_CPR_QoQ1\")\n",
    "\n",
    "\n",
    "def UFR_CPR_YoY(session: Session, table_name: str, column_mapping: dict = None, filter_date: str = \"2024-04-01\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        session: Snowpark session\n",
    "        table_name: Name of the input table\n",
    "        column_mapping: Dictionary mapping standard column names to actual column names in the input table\n",
    "        filter_date: Date filter for final result (format: 'YYYY-MM-DD')\n",
    "    \n",
    "    Returns:\n",
    "        Snowpark DataFrame with the final result\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use default mapping if none provided\n",
    "    if column_mapping is None:\n",
    "        column_mapping = Column_mapping_UFR\n",
    "    \n",
    "    # Read the input table\n",
    "    df = session.table(table_name)\n",
    "    \n",
    "    # Helper function to get column reference with mapping support\n",
    "    def get_mapped_col(standard_col_name, default_value=None):\n",
    "        \"\"\"\n",
    "        Get column reference using the mapping, with support for missing columns\n",
    "        \n",
    "        Args:\n",
    "            standard_col_name: The standard column key from the mapping\n",
    "            default_value: Default value to use if column is missing (default: None -> lit(\"NA\") for strings, 0 for numbers)\n",
    "        \n",
    "        Returns:\n",
    "            Column expression or literal value\n",
    "        \"\"\"\n",
    "        mapped_name = column_mapping.get(standard_col_name)\n",
    "        \n",
    "        if mapped_name is None:\n",
    "            # Column not present in input, return default value\n",
    "            if default_value is not None:\n",
    "                return lit(default_value)\n",
    "            elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "                return lit(0)  # Default for numeric columns\n",
    "            else:\n",
    "                return lit(\"NA\")  # Default for text columns\n",
    "        \n",
    "        # Column is mapped, try to get it from the DataFrame\n",
    "        columns = df.columns\n",
    "        \n",
    "        # Remove quotes for comparison\n",
    "        clean_mapped_name = mapped_name.strip('\"')\n",
    "        \n",
    "        # Try exact match first\n",
    "        if mapped_name in columns:\n",
    "            return col(mapped_name)\n",
    "        elif clean_mapped_name in columns:\n",
    "            return col(clean_mapped_name)\n",
    "        \n",
    "        # Try case-insensitive match\n",
    "        for c in columns:\n",
    "            if c.upper() == clean_mapped_name.upper():\n",
    "                return col(c)\n",
    "        \n",
    "        # If not found, return default value and print warning\n",
    "        print(f\"Warning: Column {mapped_name} not found in table. Using default value.\")\n",
    "        if default_value is not None:\n",
    "            return lit(default_value)\n",
    "        elif standard_col_name in [\"MRR\", \"CARR\", \"ADJUSTED_ARR\"]:\n",
    "            return lit(0)\n",
    "        else:\n",
    "            return lit(\"NA\")\n",
    "    \n",
    "    def validate_required_columns():\n",
    "        \"\"\"Validate that all required columns are available\"\"\"\n",
    "        required_columns = [\"DATE\", \"START_DATE\", \"END_DATE\", \"CUSTOMERID\", \"PRODUCT\", \"REVENUE_TYPE\", \"MRR\"]\n",
    "        missing_required = []\n",
    "        \n",
    "        for req_col in required_columns:\n",
    "            if column_mapping.get(req_col) is None:\n",
    "                missing_required.append(req_col)\n",
    "        \n",
    "        if missing_required:\n",
    "            raise ValueError(f\"Required columns are missing from column mapping: {missing_required}\")\n",
    "        \n",
    "        print(\"✓ All required columns are mapped\")\n",
    "        return True\n",
    "    \n",
    "    # Validate required columns\n",
    "    validate_required_columns()\n",
    "    \n",
    "    # Print column mapping summary\n",
    "    print(\"\\n=== Column Mapping Summary ===\")\n",
    "    for std_name, mapped_name in column_mapping.items():\n",
    "        status = \"✓ Mapped\" if mapped_name is not None else \"✗ Missing (using default)\"\n",
    "        mapped_display = mapped_name if mapped_name is not None else \"None\"\n",
    "        print(f\"{std_name:20} -> {mapped_display:20} {status}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: cleaned_data CTE\n",
    "    def clean_data(df):\n",
    "        print(\"Step 1: Cleaning data...\")\n",
    "        \n",
    "        cleaned = df.select(\n",
    "            # Convert date columns using mapping\n",
    "            get_mapped_col(\"DATE\").alias(\"Date\"),\n",
    "            get_mapped_col(\"START_DATE\").alias(\"Start_Date\"),\n",
    "            get_mapped_col(\"END_DATE\").alias(\"End_Date\"),\n",
    "            \n",
    "            # Clean and standardize text columns using mapping\n",
    "            (trim(coalesce(get_mapped_col(\"CUSTOMERID\"), lit(\"NA\")))).alias(\"CustomerID\"),\n",
    "            (trim(coalesce(get_mapped_col(\"UP_SNL_ID\"), lit(\"NA\")))).alias(\"up_snl_id\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT\"), lit(\"NA\")))).alias(\"PRODUCT\"),\n",
    "            (trim(coalesce(get_mapped_col(\"PRODUCT_GROUP\"), lit(\"NA\")))).alias(\"PRODUCT_GROUP\"),\n",
    "            (trim(coalesce(get_mapped_col(\"OPPORTUNITY_NUMBER\"), lit(\"NA\")))).alias(\"opportunity_number\"),\n",
    "            (trim(coalesce(get_mapped_col(\"REVENUE_TYPE\"), lit(\"NA\")))).alias(\"Revenue_Type\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TAGGING\"), lit(\"NA\")))).alias(\"Tagging\"),\n",
    "            (trim(coalesce(get_mapped_col(\"TIER\"), lit(\"NA\")))).alias(\"Tier\"),\n",
    "            (trim(coalesce(get_mapped_col(\"SEGMENT\"), lit(\"NA\")))).alias(\"Segment\"),\n",
    "            (trim(coalesce(get_mapped_col(\"Region\"), lit(\"NA\")))).alias(\"Region\"),\n",
    "            \n",
    "            # Handle numeric columns using mapping\n",
    "            coalesce(get_mapped_col(\"MRR\"), lit(0)).alias(\"MRR\"),\n",
    "            coalesce(get_mapped_col(\"CARR\"), lit(0)).alias(\"CARR\"),\n",
    "            coalesce(get_mapped_col(\"ADJUSTED_ARR\"), lit(0)).alias(\"Adjusted_ARR\")\n",
    "        ).filter(\n",
    "            # Apply filters\n",
    "            (col(\"End_Date\").isNotNull()) & \n",
    "            (col(\"MRR\") != 0)\n",
    "        )\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    # Step 2: contract_summary CTE - INCLUDES Revenue_Type\n",
    "    def create_contract_summary(cleaned_data):\n",
    "        print(\"Step 2: Creating contract summary...\")\n",
    "        \n",
    "        # GROUP BY includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        contract_summary = cleaned_data.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\").agg(\n",
    "            spark_min(\"Date\").alias(\"Min_Date\"),\n",
    "            spark_max(\"Date\").alias(\"Max_Date\")\n",
    "        )\n",
    "        \n",
    "        # Calculate contract size\n",
    "        contract_summary = contract_summary.withColumn(\n",
    "            \"Contract_Size\",\n",
    "            when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 1, \"1. Less than 1 year\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 2, \"2. 1-2 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 3, \"3. 2-3 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 4, \"4. 3-4 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) < 5, \"5. 4-5 years\")\n",
    "            .when(datediff(\"year\", col(\"Start_Date\"), col(\"End_Date\")) >= 5, \"6. 5+ years\")\n",
    "            .otherwise(\"NA\")\n",
    "        )\n",
    "        \n",
    "        return contract_summary\n",
    "    \n",
    "    # Step 3: summary_a and summary_b CTEs - INCLUDES Revenue_Type\n",
    "    def create_summaries(cleaned_data):\n",
    "        print(\"Step 3: Creating summaries...\")\n",
    "        \n",
    "        # summary_a includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        summary_a = cleaned_data.select(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\").distinct()\n",
    "        summary_b = cleaned_data.select(\"Date\").distinct()\n",
    "        \n",
    "        return summary_a, summary_b\n",
    "    \n",
    "    # Step 4: cartesian_data CTE - INCLUDES Revenue_Type\n",
    "    def create_cartesian_data(summary_a, summary_b):\n",
    "        print(\"Step 4: Creating cartesian data...\")\n",
    "        \n",
    "        # Cross join to create cartesian product - includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        cartesian = summary_a.crossJoin(summary_b)\n",
    "        \n",
    "        return cartesian\n",
    "    \n",
    "    # Step 5: summary_mrr CTE - INCLUDES Revenue_Type\n",
    "    def create_summary_mrr(cleaned_data):\n",
    "        print(\"Step 5: Creating summary MRR...\")\n",
    "        \n",
    "        # GROUP BY includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        summary_mrr = cleaned_data.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Date\").agg(\n",
    "            spark_sum(\"MRR\").alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return summary_mrr\n",
    "    \n",
    "    # Step 6: joined_data CTE - INCLUDES Revenue_Type\n",
    "    def create_joined_data(cartesian_data, summary_mrr):\n",
    "        print(\"Step 6: Creating joined data...\")\n",
    "        \n",
    "        # JOIN conditions include CustomerID, PRODUCT, and Revenue_Type\n",
    "        joined = cartesian_data.join(\n",
    "            summary_mrr,\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Date\"],\n",
    "            \"left\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Revenue_Type\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Date\"),\n",
    "            coalesce(col(\"Sum_MRR\"), lit(0)).alias(\"Sum_MRR\")\n",
    "        )\n",
    "        \n",
    "        return joined\n",
    "    \n",
    "    # Step 7: joined_data2 CTE - INCLUDES Revenue_Type\n",
    "    def create_joined_data2(contract_summary, joined_data):\n",
    "        print(\"Step 7: Creating joined data 2...\")\n",
    "        \n",
    "        # JOIN conditions include CustomerID, PRODUCT, and Revenue_Type\n",
    "        joined_data2 = joined_data.join(\n",
    "            contract_summary.select(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Max_Date\"),\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\"],\n",
    "            \"inner\"\n",
    "        )\n",
    "        \n",
    "        return joined_data2\n",
    "    \n",
    "    # Step 8: with_next_date CTE\n",
    "    def create_with_next_date(joined_data2):\n",
    "        print(\"Step 8: Adding next date...\")\n",
    "        \n",
    "        with_next_date = joined_data2.withColumn(\n",
    "            \"Next_Date\",\n",
    "            date_trunc(\"month\", dateadd(\"day\", lit(1), col(\"End_Date\")))\n",
    "        ).withColumn(\n",
    "            \"Sum_MRR_Adjusted\",\n",
    "            when(\n",
    "                dateadd(\"month\", lit(1), col(\"Max_Date\")) < col(\"Next_Date\"),\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"Sum_MRR\"))\n",
    "        )\n",
    "        \n",
    "        return with_next_date\n",
    "    \n",
    "    # Step 9: sorted_data CTE - INCLUDES Revenue_Type\n",
    "    def create_sorted_data(with_next_date):\n",
    "        print(\"Step 9: Sorting data...\")\n",
    "        \n",
    "        # ORDER BY includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        sorted_data = with_next_date.orderBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Date\")\n",
    "        \n",
    "        return sorted_data\n",
    "    \n",
    "    # Step 10: rolling_sums CTE - INCLUDES Revenue_Type\n",
    "    def create_rolling_sums(sorted_data):\n",
    "        print(\"Step 10: Creating rolling sums...\")\n",
    "        \n",
    "        # PARTITION BY includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        window_spec = Window.partitionBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\").orderBy(\"Date\")\n",
    "        \n",
    "        rolling_sums = sorted_data.withColumn(\n",
    "            \"UFR_TTM\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-11, 0))\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M\",\n",
    "            spark_sum(\"Sum_MRR_Adjusted\").over(window_spec.rowsBetween(-2, 0))\n",
    "        )\n",
    "        \n",
    "        return rolling_sums\n",
    "    \n",
    "    # Step 11: final_metrics CTE\n",
    "    def create_final_metrics(rolling_sums):\n",
    "        print(\"Step 11: Creating final metrics...\")\n",
    "        \n",
    "        final_metrics = rolling_sums.withColumn(\n",
    "            \"UFR_ARR\", col(\"Sum_MRR_Adjusted\") * 12\n",
    "        ).withColumn(\n",
    "            \"UFR_T3M_Annualized\", col(\"UFR_TTM\") * 4\n",
    "        ).filter(\n",
    "            (col(\"Sum_MRR_Adjusted\") != 0) |\n",
    "            (col(\"UFR_TTM\") != 0) |\n",
    "            (col(\"UFR_T3M\") != 0)\n",
    "        )\n",
    "        \n",
    "        return final_metrics\n",
    "    \n",
    "    # Step 12: joined_data3 CTE - INCLUDES Revenue_Type\n",
    "    def create_joined_data3(contract_summary, final_metrics):\n",
    "        print(\"Step 12: Creating joined data 3...\")\n",
    "        \n",
    "        # Filter final_metrics where Date equals Max_Date\n",
    "        filtered_metrics = final_metrics.filter(col(\"Date\") == col(\"Max_Date\"))\n",
    "        \n",
    "        # JOIN conditions include CustomerID, PRODUCT, and Revenue_Type\n",
    "        joined_data3 = contract_summary.join(\n",
    "            filtered_metrics,\n",
    "            [\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Start_Date\", \"End_Date\", \"Max_Date\"],\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Revenue_Type\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Max_Date\"),\n",
    "            col(\"Start_Date\"),\n",
    "            col(\"End_Date\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"Date\"),\n",
    "            col(\"Sum_MRR_Adjusted\").alias(\"UFR_Amount\"),\n",
    "            col(\"UFR_TTM\"),\n",
    "            col(\"UFR_T3M\"),\n",
    "            col(\"UFR_ARR\"),\n",
    "            col(\"UFR_T3M_Annualized\"),\n",
    "            col(\"Next_Date\")\n",
    "        )\n",
    "        \n",
    "        return joined_data3\n",
    "    \n",
    "    # Step 13: unioneddata2 CTE - INCLUDES Revenue_Type\n",
    "    def create_unioneddata2(joined_data3):\n",
    "        print(\"Step 13: Creating unioned data...\")\n",
    "        \n",
    "        # Create separate DataFrames for each UNION - GROUP BY includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        mrr_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"MRR\"))\n",
    "        \n",
    "        arr_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_ARR\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"ARR\"))\n",
    "        \n",
    "        t3m_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M\"))\n",
    "        \n",
    "        ttm_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_TTM\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"TTM\"))\n",
    "        \n",
    "        t3m_ann_data = joined_data3.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"Min_Date\", \"Next_Date\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_T3M_Annualized\").alias(\"UFR_Amount\")\n",
    "        ).withColumn(\"Amount\", lit(\"T3M (Annualized)\"))\n",
    "        \n",
    "        # Union all data\n",
    "        unioneddata2 = mrr_data.unionAll(arr_data).unionAll(t3m_data).unionAll(ttm_data).unionAll(t3m_ann_data)\n",
    "        \n",
    "        return unioneddata2\n",
    "    \n",
    "    # Step 14: main_data1 CTE - INCLUDES Revenue_Type\n",
    "    def create_main_data1(unioneddata2):\n",
    "        print(\"Step 14: Creating main data...\")\n",
    "        \n",
    "        main_data1 = unioneddata2.withColumn(\n",
    "            \"Min_Date\", date_trunc(\"month\", col(\"Min_Date\"))\n",
    "        ).withColumn(\n",
    "            \"Next_Date\", date_trunc(\"month\", col(\"Next_Date\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount\") >= 0\n",
    "        )\n",
    "        \n",
    "        return main_data1\n",
    "    \n",
    "    # Step 15: expanded_dates_YOY CTE - INCLUDES Revenue_Type and uses 12 months instead of 3\n",
    "    def create_expanded_dates_yoy(main_data1):\n",
    "        print(\"Step 15: Creating expanded dates for YOY...\")\n",
    "        \n",
    "        # Create a generator for 12 months (0 through 11) to match SQL2's GENERATOR(ROWCOUNT => 12)\n",
    "        generator_data = [(i,) for i in range(12)]\n",
    "        generator_df = session.createDataFrame(generator_data, [\"seq\"])\n",
    "        \n",
    "        # Cross join with main_data1 and add the UFR_Date calculation\n",
    "        expanded_dates = main_data1.crossJoin(generator_df).withColumn(\n",
    "            \"UFR_Date\",\n",
    "            dateadd(\"month\", col(\"seq\"), col(\"Next_Date\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Date\") <= dateadd(\"month\", lit(11), col(\"Next_Date\"))\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Revenue_Type\"),\n",
    "            col(\"Next_Date\"),\n",
    "            col(\"Min_Date\"),\n",
    "            col(\"Amount\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"UFR_Amount\"),\n",
    "            col(\"UFR_Date\")\n",
    "        )\n",
    "        \n",
    "        return expanded_dates\n",
    "    \n",
    "    # Step 16: filtered_and_adjusted_YOY CTE - INCLUDES Revenue_Type and uses 12 months lookback\n",
    "    def create_filtered_and_adjusted_yoy(expanded_dates_yoy):\n",
    "        print(\"Step 16: Creating filtered and adjusted YOY...\")\n",
    "        \n",
    "        filtered_adjusted = expanded_dates_yoy.withColumn(\n",
    "            \"UFR_Amount_Adjusted\",\n",
    "            when(\n",
    "                col(\"Min_Date\") > dateadd(\"month\", lit(-12), col(\"UFR_Date\")),\n",
    "                lit(0)\n",
    "            ).otherwise(col(\"UFR_Amount\"))\n",
    "        ).filter(\n",
    "            col(\"UFR_Amount_Adjusted\") != 0\n",
    "        ).select(\n",
    "            col(\"CustomerID\"),\n",
    "            col(\"PRODUCT\"),\n",
    "            col(\"Revenue_Type\"),\n",
    "            col(\"UFR_Date\"),\n",
    "            col(\"Amount\"),\n",
    "            col(\"Contract_Size\"),\n",
    "            col(\"UFR_Amount_Adjusted\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        \n",
    "        return filtered_adjusted\n",
    "    \n",
    "    # Step 17: filtered_and_adjusted_YOY2 CTE - INCLUDES Revenue_Type\n",
    "    def create_filtered_and_adjusted_yoy2(filtered_and_adjusted_yoy):\n",
    "        print(\"Step 17: Creating filtered and adjusted YOY2...\")\n",
    "        \n",
    "        # Filter UFR Date < filter_date (configurable parameter) and GROUP BY includes Revenue_Type\n",
    "        filtered_adjusted2 = filtered_and_adjusted_yoy.filter(\n",
    "            col(\"UFR_Date\") < lit(filter_date)\n",
    "        ).groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"UFR_Date\", \"Amount\", \"Contract_Size\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        \n",
    "        return filtered_adjusted2\n",
    "    \n",
    "    # Step 18: Final SELECT - INCLUDES Revenue_Type\n",
    "    def create_final_result(filtered_and_adjusted_yoy2):\n",
    "        print(\"Step 18: Creating final result...\")\n",
    "        \n",
    "        # Final GROUP BY includes CustomerID, PRODUCT, and Revenue_Type\n",
    "        final_result = filtered_and_adjusted_yoy2.groupBy(\"CustomerID\", \"PRODUCT\", \"Revenue_Type\", \"UFR_Date\", \"Amount\").agg(\n",
    "            spark_sum(\"UFR_Amount\").alias(\"UFR_Amount\")\n",
    "        )\n",
    "        if input_amount == \"ARR\":\n",
    "            final_result = final_result.select(\n",
    "                col(\"PRODUCT\").alias('\"Product\"'),\n",
    "                col(\"Revenue_Type\").alias('\"Revenue Type\"'),\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                (col(\"UFR_Amount\") / 12).alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "        )\n",
    "        else:  # For MRR\n",
    "            final_result = final_result.select(\n",
    "                col(\"PRODUCT\").alias('\"Product\"'),\n",
    "                col(\"Revenue_Type\").alias('\"Revenue Type\"'),\n",
    "                col(\"CustomerID\").alias('\"CustName\"'),\n",
    "                col(\"UFR_Date\").alias('\"UFR Date\"'),\n",
    "                col(\"UFR_Amount\").alias('\"UFR Amount\"'),\n",
    "                col(\"AMOUNT\").alias('\"Amount\"')\n",
    "        )\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    # Execute all steps\n",
    "    print(f\"\\n🚀 Starting UFR_CPR_YoY processing for table: {table_name}\")\n",
    "    # print(f\"📅 Filter date: {filter_date}\")\n",
    "    \n",
    "    cleaned_data = clean_data(df)\n",
    "    contract_summary = create_contract_summary(cleaned_data)\n",
    "    summary_a, summary_b = create_summaries(cleaned_data)\n",
    "    cartesian_data = create_cartesian_data(summary_a, summary_b)\n",
    "    summary_mrr = create_summary_mrr(cleaned_data)\n",
    "    joined_data = create_joined_data(cartesian_data, summary_mrr)\n",
    "    joined_data2 = create_joined_data2(contract_summary, joined_data)\n",
    "    with_next_date = create_with_next_date(joined_data2)\n",
    "    sorted_data = create_sorted_data(with_next_date)\n",
    "    rolling_sums = create_rolling_sums(sorted_data)\n",
    "    final_metrics = create_final_metrics(rolling_sums)\n",
    "    joined_data3 = create_joined_data3(contract_summary, final_metrics)\n",
    "    unioneddata2 = create_unioneddata2(joined_data3)\n",
    "    main_data1 = create_main_data1(unioneddata2)\n",
    "    expanded_dates_yoy = create_expanded_dates_yoy(main_data1)\n",
    "    filtered_and_adjusted_yoy = create_filtered_and_adjusted_yoy(expanded_dates_yoy)\n",
    "    filtered_and_adjusted_yoy2 = create_filtered_and_adjusted_yoy2(filtered_and_adjusted_yoy)\n",
    "    final_result = create_final_result(filtered_and_adjusted_yoy2)\n",
    "    \n",
    "    print(\"✅ UFR_CPR_YoY processing completed successfully!\")\n",
    "    return final_result\n",
    "\n",
    "# Example usage:\n",
    "# result_df = UFR_CPR_YoY(session, \"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST\", Column_mapping_UFR)\n",
    "# result_df.write.mode(\"overwrite\").saveAsTable(\"SUBSCRIPTION_ACCEL.SOURCE.UFR_TEST_CPR_YoY1\")\n",
    "\n",
    "\n",
    "\n",
    "#---UFR JOIN codes\n",
    "\n",
    "\n",
    "# Configuration\n",
    "# lookback_list_UFR = [1, 3, 12]  # Multiple periods: MOM, QOQ, YOY\n",
    "# pbi_retention_output_path = \"NEWFOLD_TEST_DB.SOURCE.MRR_BLOWOUT_10K_21JUNE_PBI_MASTER\"\n",
    "# input_file_path = \"NEWFOLD_TEST_DB.SOURCE.MRR_BLOWOUT_10K\"\n",
    "\n",
    "# Define table names\n",
    "main_table_name_PBI_C = f\"{pbi_retention_output_path}_C_NOTEBOOK\"\n",
    "secondary_tables_PBI_C = {\n",
    "    1: f\"{input_file_path}_C_MOM\",   # Month\n",
    "    3: f\"{input_file_path}_C_QOQ\",   # Quarter \n",
    "    12: f\"{input_file_path}_C_YOY\"   # Year\n",
    "}\n",
    "\n",
    "# Common columns for join and update\n",
    "join_columns = ['Amount', 'CustName', 'Current Period']\n",
    "update_columns = ['UFR Amount', 'UFR Date']\n",
    "\n",
    "# Period mapping based on lookback values\n",
    "period_mapping = {\n",
    "    1: \"Month\",\n",
    "    3: \"Quarter\", \n",
    "    12: \"Year\"\n",
    "}\n",
    "\n",
    "def UFR_PBI_C_final(session, lookback_list_UFR):\n",
    "    def clean_numeric_column_PBI_C(df, col_name):\n",
    "        \"\"\"Clean numeric column by converting empty strings and whitespace to NULL\"\"\"\n",
    "        return when(\n",
    "            (col(col_name).is_null()) | \n",
    "            (trim(col(col_name)) == \"\") | \n",
    "            (length(trim(col(col_name))) == 0),\n",
    "            lit(None)\n",
    "        ).otherwise(col(col_name))\n",
    "    \n",
    "    def clean_date_column_PBI_C(df, col_name):\n",
    "        \"\"\"Clean date column by converting empty strings and whitespace to NULL\"\"\"\n",
    "        return when(\n",
    "            (col(col_name).is_null()) | \n",
    "            (trim(col(col_name)) == \"\") | \n",
    "            (length(trim(col(col_name))) == 0),\n",
    "            lit(None)\n",
    "        ).otherwise(col(col_name))\n",
    "    \n",
    "    def find_column_name(df, target_col):\n",
    "        \"\"\"Find the actual column name that matches the target (case insensitive, handles quotes)\"\"\"\n",
    "        target_clean = target_col.upper().replace(' ', '').replace('_', '').replace('\"', '')\n",
    "        for col_name in df.columns:\n",
    "            col_clean = col_name.upper().replace(' ', '').replace('_', '').replace('\"', '')\n",
    "            if col_clean == target_clean:\n",
    "                return col_name\n",
    "        return None\n",
    "    \n",
    "    def prepare_secondary_data_for_period(session, lookback_period):\n",
    "        \"\"\"Prepare secondary table data for a specific period\"\"\"\n",
    "        try:\n",
    "            if lookback_period not in secondary_tables_PBI_C:\n",
    "                print(f\"Invalid lookback period: {lookback_period}\")\n",
    "                return None\n",
    "                \n",
    "            secondary_table_name = secondary_tables_PBI_C[lookback_period]\n",
    "            period_filter_value = period_mapping.get(lookback_period)\n",
    "            \n",
    "            print(f\"Processing secondary table: {secondary_table_name} for period: {period_filter_value}\")\n",
    "            \n",
    "            # Load secondary table\n",
    "            try:\n",
    "                secondary_df = session.table(secondary_table_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Secondary table {secondary_table_name} not available: {str(e)}\")\n",
    "                return None\n",
    "            \n",
    "            # Find column names in secondary table\n",
    "            sec_amount_col = find_column_name(secondary_df, 'Amount')\n",
    "            sec_custname_col = find_column_name(secondary_df, 'CustName')\n",
    "            sec_ufr_amount_col = find_column_name(secondary_df, 'UFR Amount')\n",
    "            sec_ufr_date_col = find_column_name(secondary_df, 'UFR Date')\n",
    "            \n",
    "            if not all([sec_amount_col, sec_custname_col, sec_ufr_date_col]):\n",
    "                print(f\"Required columns not found in secondary table {secondary_table_name}\")\n",
    "                return None\n",
    "            \n",
    "            # Clean and prepare secondary data with period identifier\n",
    "            secondary_clean = secondary_df.select(\n",
    "                clean_numeric_column_PBI_C(secondary_df, sec_amount_col).alias(\"Amount\"),\n",
    "                col(sec_custname_col).alias(\"CustName\"),\n",
    "                clean_date_column_PBI_C(secondary_df, sec_ufr_date_col).alias(\"UFR_Date\"),\n",
    "                clean_numeric_column_PBI_C(secondary_df, sec_ufr_amount_col).alias(\"UFR_Amount\") if sec_ufr_amount_col else lit(None).alias(\"UFR_Amount\"),\n",
    "                lit(period_filter_value).alias(\"Period_Source\"),  # Track which period this data came from\n",
    "                lit(lookback_period).alias(\"Lookback_Period\")     # Track lookback period\n",
    "            ).filter(\n",
    "                col(\"Amount\").is_not_null() & \n",
    "                col(\"CustName\").is_not_null() &\n",
    "                col(\"UFR_Date\").is_not_null() &\n",
    "                (trim(col(\"CustName\")) != \"\")\n",
    "            )\n",
    "            \n",
    "            return secondary_clean\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing secondary data for period {lookback_period}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def perform_multi_period_join(session, lookback_list_UFR):\n",
    "        \"\"\"Perform joins for multiple periods and combine results\"\"\"\n",
    "        try:\n",
    "            # Load and clean main table\n",
    "            main_df_original = session.table(main_table_name_PBI_C)\n",
    "            print(f\"Main table loaded successfully with {main_df_original.count()} rows\")\n",
    "            \n",
    "            # Find column names in main table\n",
    "            main_amount_col = find_column_name(main_df_original, 'Amount')\n",
    "            main_custname_col = find_column_name(main_df_original, 'CustName')\n",
    "            main_current_period_col = find_column_name(main_df_original, 'Current Period')\n",
    "            main_period_col = find_column_name(main_df_original, 'Period')\n",
    "            main_ufr_amount_col = find_column_name(main_df_original, 'UFR Amount')\n",
    "            main_ufr_date_col = find_column_name(main_df_original, 'UFR Date')\n",
    "            main_ufr_tag_col = find_column_name(main_df_original, 'UFR Tag')\n",
    "            \n",
    "            if not all([main_amount_col, main_custname_col, main_current_period_col, main_period_col]):\n",
    "                print(\"Required columns not found in main table\")\n",
    "                return None\n",
    "            \n",
    "            # Clean main table\n",
    "            main_select_cols = []\n",
    "            for c in main_df_original.columns:\n",
    "                if c == main_amount_col:\n",
    "                    main_select_cols.append(clean_numeric_column_PBI_C(main_df_original, c).alias(c))\n",
    "                elif c == main_ufr_amount_col:\n",
    "                    main_select_cols.append(clean_numeric_column_PBI_C(main_df_original, c).alias(c))\n",
    "                elif c == main_ufr_date_col:\n",
    "                    main_select_cols.append(clean_date_column_PBI_C(main_df_original, c).alias(c))\n",
    "                elif c == main_current_period_col:\n",
    "                    main_select_cols.append(clean_date_column_PBI_C(main_df_original, c).alias(c))\n",
    "                else:\n",
    "                    main_select_cols.append(col(c))\n",
    "            \n",
    "            main_clean = main_df_original.select(*main_select_cols)\n",
    "            \n",
    "            # Prepare all secondary data for all periods\n",
    "            all_secondary_data = []\n",
    "            valid_periods = []\n",
    "            \n",
    "            for period in lookback_list_UFR:\n",
    "                secondary_data = prepare_secondary_data_for_period(session, period)\n",
    "                if secondary_data is not None:\n",
    "                    all_secondary_data.append(secondary_data)\n",
    "                    valid_periods.append(period)\n",
    "                    print(f\"Successfully prepared data for period {period}\")\n",
    "                else:\n",
    "                    print(f\"Skipping period {period} due to data preparation failure\")\n",
    "            \n",
    "            if not all_secondary_data:\n",
    "                print(\"No valid secondary data found for any period\")\n",
    "                return main_clean\n",
    "            \n",
    "            # Union all secondary data\n",
    "            print(f\"Combining secondary data from {len(all_secondary_data)} periods\")\n",
    "            combined_secondary = all_secondary_data[0]\n",
    "            for i in range(1, len(all_secondary_data)):\n",
    "                combined_secondary = combined_secondary.union(all_secondary_data[i])\n",
    "            \n",
    "            print(f\"Combined secondary data has {combined_secondary.count()} rows\")\n",
    "            \n",
    "            # Create mapping of periods to filter main table\n",
    "            period_filter_values = [period_mapping.get(p) for p in valid_periods]\n",
    "            print(f\"Will process main table rows with Period in: {period_filter_values}\")\n",
    "            \n",
    "            # Filter main table for relevant periods\n",
    "            main_filtered = main_clean.filter(\n",
    "                col(main_period_col).isin(period_filter_values) &\n",
    "                col(main_amount_col).is_not_null() & \n",
    "                col(main_custname_col).is_not_null() &\n",
    "                col(main_current_period_col).is_not_null() &\n",
    "                (trim(col(main_custname_col)) != \"\")\n",
    "            )\n",
    "            \n",
    "            print(f\"Main table rows after filtering for relevant periods: {main_filtered.count()}\")\n",
    "            \n",
    "            # Perform join with combined secondary data\n",
    "            join_condition = (\n",
    "                (main_filtered[main_amount_col] == combined_secondary[\"Amount\"]) & \n",
    "                (main_filtered[main_custname_col] == combined_secondary[\"CustName\"]) &\n",
    "                (main_filtered[main_current_period_col] == combined_secondary[\"UFR_Date\"]) &\n",
    "                (main_filtered[main_period_col] == combined_secondary[\"Period_Source\"])  # Match period\n",
    "            )\n",
    "            \n",
    "            print(\"Performing left outer join with multi-period data...\")\n",
    "            joined_df = main_filtered.join(combined_secondary, join_condition, \"left\")\n",
    "            \n",
    "            # Build result with updated UFR values\n",
    "            select_columns = []\n",
    "            \n",
    "            # Add all main table columns except UFR columns\n",
    "            for c in main_filtered.columns:\n",
    "                if c not in [main_ufr_amount_col, main_ufr_date_col, main_ufr_tag_col]:\n",
    "                    select_columns.append(col(c))\n",
    "            \n",
    "            # Handle UFR Amount (secondary takes precedence)\n",
    "            if main_ufr_amount_col:\n",
    "                ufr_amount_col = coalesce(\n",
    "                    col(\"UFR_Amount\"),  # From secondary\n",
    "                    col(main_ufr_amount_col)  # From main\n",
    "                ).alias(main_ufr_amount_col)\n",
    "            else:\n",
    "                ufr_amount_col = col(\"UFR_Amount\").alias('\"UFR Amount\"')\n",
    "            select_columns.append(ufr_amount_col)\n",
    "            \n",
    "            # Handle UFR Date (secondary takes precedence)\n",
    "            if main_ufr_date_col:\n",
    "                ufr_date_col = coalesce(\n",
    "                    col(\"UFR_Date\"),  # From secondary\n",
    "                    col(main_ufr_date_col)  # From main\n",
    "                ).alias(main_ufr_date_col)\n",
    "            else:\n",
    "                ufr_date_col = col(\"UFR_Date\").alias('\"UFR Date\"')\n",
    "            select_columns.append(ufr_date_col)\n",
    "            \n",
    "            # Handle UFR Tag (set to \"UFR\" if UFR Date exists, otherwise \"Not UFR\")\n",
    "            if main_ufr_tag_col:\n",
    "                ufr_tag_col = when(\n",
    "                    coalesce(col(\"UFR_Date\"), col(main_ufr_date_col) if main_ufr_date_col else lit(None)).is_not_null(),\n",
    "                    lit(\"UFR\")\n",
    "                ).otherwise(lit(\"Not UFR\")).alias(main_ufr_tag_col)\n",
    "            else:\n",
    "                ufr_tag_col = when(\n",
    "                    col(\"UFR_Date\").is_not_null(),\n",
    "                    lit(\"UFR\")\n",
    "                ).otherwise(lit(\"Not UFR\")).alias('\"UFR Tag\"')\n",
    "            select_columns.append(ufr_tag_col)\n",
    "            \n",
    "            joined_result = joined_df.select(*select_columns)\n",
    "            \n",
    "            # Now merge back with original main table to maintain all rows\n",
    "            print(\"Merging results back with original main table...\")\n",
    "            \n",
    "            # Create unique keys for joining back\n",
    "            main_with_key = main_clean.with_column(\n",
    "                \"temp_join_key\",\n",
    "                concat(\n",
    "                    coalesce(col(main_amount_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_custname_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_current_period_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_period_col), lit(\"NULL\"))\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            joined_with_key = joined_result.with_column(\n",
    "                \"temp_join_key\",\n",
    "                concat(\n",
    "                    coalesce(col(main_amount_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_custname_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_current_period_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_period_col), lit(\"NULL\"))\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Final merge\n",
    "            final_result = main_with_key.join(\n",
    "                joined_with_key.select(\n",
    "                    col(\"temp_join_key\").alias(\"joined_key\"),\n",
    "                    col(main_ufr_amount_col if main_ufr_amount_col else '\"UFR Amount\"').alias(\"new_UFR_Amount\"),\n",
    "                    col(main_ufr_date_col if main_ufr_date_col else '\"UFR Date\"').alias(\"new_UFR_Date\"),\n",
    "                    col(main_ufr_tag_col if main_ufr_tag_col else '\"UFR Tag\"').alias(\"new_UFR_Tag\")\n",
    "                ),\n",
    "                main_with_key[\"temp_join_key\"] == col(\"joined_key\"),\n",
    "                \"left\"\n",
    "            )\n",
    "            \n",
    "            # Build final select with all original columns\n",
    "            final_select_cols = []\n",
    "            for c in main_clean.columns:\n",
    "                if c == main_ufr_amount_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Amount\"), col(c)).alias(c))\n",
    "                elif c == main_ufr_date_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Date\"), col(c)).alias(c))\n",
    "                elif c == main_ufr_tag_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Tag\"), col(c), lit(\"Not UFR\")).alias(c))\n",
    "                else:\n",
    "                    final_select_cols.append(col(c))\n",
    "            \n",
    "            # Add UFR columns if they didn't exist in main table\n",
    "            if not main_ufr_amount_col:\n",
    "                final_select_cols.append(coalesce(col(\"new_UFR_Amount\"), lit(None)).alias('\"UFR Amount\"'))\n",
    "            if not main_ufr_date_col:\n",
    "                final_select_cols.append(coalesce(col(\"new_UFR_Date\"), lit(None)).alias('\"UFR Date\"'))\n",
    "            if not main_ufr_tag_col:\n",
    "                final_select_cols.append(coalesce(col(\"new_UFR_Tag\"), lit(\"Not UFR\")).alias('\"UFR Tag\"'))\n",
    "            \n",
    "            result_df = final_result.select(*final_select_cols)\n",
    "            \n",
    "            # Show summary statistics\n",
    "            total_original_rows = main_df_original.count()\n",
    "            total_result_rows = result_df.count()\n",
    "            \n",
    "            print(f\"Original main table rows: {total_original_rows}\")\n",
    "            print(f\"Final result rows: {total_result_rows}\")\n",
    "            \n",
    "            if total_result_rows == total_original_rows:\n",
    "                print(\"✓ Successfully maintained original row count\")\n",
    "            else:\n",
    "                print(\"⚠ Warning: Row count changed!\")\n",
    "            \n",
    "            # Show UFR statistics by period\n",
    "            print(f\"\\nProcessed periods: {valid_periods}\")\n",
    "            print(f\"Period mappings: {[period_mapping.get(p) for p in valid_periods]}\")\n",
    "            \n",
    "            # Show UFR Tag summary\n",
    "            ufr_tag_col_name = main_ufr_tag_col if main_ufr_tag_col else '\"UFR Tag\"'\n",
    "            try:\n",
    "                result_df.group_by(ufr_tag_col_name).count().order_by(ufr_tag_col_name).show()\n",
    "            except:\n",
    "                print(\"Could not display UFR Tag summary\")\n",
    "            \n",
    "            return result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in multi-period join operation: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def UFR_PBI_C_main(session):\n",
    "        \"\"\"Main function to execute the multi-period join process\"\"\"\n",
    "        print(\"=*=\"*25)\n",
    "        print(\"Starting multi-period UFR join process...\")\n",
    "        print(f\"Lookback periods: {lookback_list_UFR}\")\n",
    "        print(f\"Period mappings: {[period_mapping.get(p, 'Unknown') for p in lookback_list_UFR]}\")\n",
    "        print(\"Join conditions: Amount = Amount AND CustName = CustName AND Current Period = UFR Date AND Period = Period_Source\")\n",
    "        \n",
    "        result = perform_multi_period_join(session, lookback_list_UFR)\n",
    "        \n",
    "        if result is not None:\n",
    "            print(\"Multi-period join completed successfully!\")\n",
    "            \n",
    "            # Save result\n",
    "            pbi_c_ufr = main_table_name_PBI_C + \"_UFR\"\n",
    "            result.write.mode(\"overwrite\").save_as_table(pbi_c_ufr)\n",
    "            print(\"=\"*100)\n",
    "            print(f\"___Saving on path: {pbi_c_ufr}\")\n",
    "            print(\"=\"*100)\n",
    "        else:\n",
    "            print(\"Multi-period join process failed\")\n",
    "            print(\"=\"*100)\n",
    "    \n",
    "    UFR_PBI_C_main(session)\n",
    "\n",
    "# Example usage:\n",
    "# UFR_PBI_C_final(session, [1, 3])      # For MOM and QOQ\n",
    "# UFR_PBI_C_final(session, [1, 3, 12])  # For MOM, QOQ, and YOY\n",
    "# UFR_PBI_C_final(session, [12])        # For YOY only\n",
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "# lookback_list_UFR = [1, 3, 12]  # Multiple periods: MOM, QOQ, YOY\n",
    "# pbi_retention_output_path = \"NEWFOLD_TEST_DB.SOURCE.MRR_BLOWOUT_10K_21JUNE_PBI_MASTER\"\n",
    "# input_file_path = \"NEWFOLD_TEST_DB.SOURCE.MRR_BLOWOUT_10K\"\n",
    "\n",
    "# Define table names\n",
    "main_table_name_PBI_CP = f\"{pbi_retention_output_path}_CP\"\n",
    "secondary_tables_PBI_CP = {\n",
    "    1: f\"{input_file_path}_CP_MOM\",   # Month\n",
    "    3: f\"{input_file_path}_CP_QOQ\",   # Quarter  \n",
    "    12: f\"{input_file_path}_CP_YOY\"   # Year\n",
    "}\n",
    "\n",
    "# Common columns for join and update (with proper column names) - Added Product\n",
    "join_columns = ['Amount', 'CustName', 'Current Period', 'Product']  # Added Product column\n",
    "update_columns = ['UFR Amount', 'UFR Date']\n",
    "\n",
    "# Period mapping based on lookback values\n",
    "period_mapping = {\n",
    "    1: \"Month\",\n",
    "    3: \"Quarter\", \n",
    "    12: \"Year\"\n",
    "}\n",
    "\n",
    "def UFR_PBI_CP_final(session, lookback_list_UFR):\n",
    "    def clean_numeric_column_PBI_CP(df, col_name):\n",
    "        \"\"\"Clean numeric column by converting empty strings and whitespace to NULL\"\"\"\n",
    "        return when(\n",
    "            (col(col_name).is_null()) | \n",
    "            (trim(col(col_name)) == \"\") | \n",
    "            (length(trim(col(col_name))) == 0),\n",
    "            lit(None)\n",
    "        ).otherwise(col(col_name))\n",
    "    \n",
    "    def clean_date_column_PBI_CP(df, col_name):\n",
    "        \"\"\"Clean date column by converting empty strings and whitespace to NULL\"\"\"\n",
    "        return when(\n",
    "            (col(col_name).is_null()) | \n",
    "            (trim(col(col_name)) == \"\") | \n",
    "            (length(trim(col(col_name))) == 0),\n",
    "            lit(None)\n",
    "        ).otherwise(col(col_name))\n",
    "    \n",
    "    def find_column_name(df, target_col):\n",
    "        \"\"\"Find the actual column name that matches the target (case insensitive, handles quotes)\"\"\"\n",
    "        target_clean = target_col.upper().replace(' ', '').replace('_', '').replace('\"', '')\n",
    "        for col_name in df.columns:\n",
    "            col_clean = col_name.upper().replace(' ', '').replace('_', '').replace('\"', '')\n",
    "            if col_clean == target_clean:\n",
    "                return col_name\n",
    "        return None\n",
    "    \n",
    "    def prepare_secondary_data_for_period(session, lookback_period):\n",
    "        \"\"\"Prepare secondary table data for a specific period\"\"\"\n",
    "        try:\n",
    "            if lookback_period not in secondary_tables_PBI_CP:\n",
    "                print(f\"Invalid lookback period: {lookback_period}\")\n",
    "                return None\n",
    "                \n",
    "            secondary_table_name = secondary_tables_PBI_CP[lookback_period]\n",
    "            period_filter_value = period_mapping.get(lookback_period)\n",
    "            \n",
    "            print(f\"Processing secondary table: {secondary_table_name} for period: {period_filter_value}\")\n",
    "            \n",
    "            # Load secondary table\n",
    "            try:\n",
    "                secondary_df = session.table(secondary_table_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Secondary table {secondary_table_name} not available: {str(e)}\")\n",
    "                return None\n",
    "            \n",
    "            # Find column names in secondary table\n",
    "            sec_amount_col = find_column_name(secondary_df, 'Amount')\n",
    "            sec_custname_col = find_column_name(secondary_df, 'CustName')\n",
    "            sec_product_col = find_column_name(secondary_df, 'Product')  # Added Product column\n",
    "            sec_ufr_amount_col = find_column_name(secondary_df, 'UFR Amount')\n",
    "            sec_ufr_date_col = find_column_name(secondary_df, 'UFR Date')\n",
    "            \n",
    "            if not all([sec_amount_col, sec_custname_col, sec_product_col, sec_ufr_date_col]):\n",
    "                print(f\"Required columns not found in secondary table {secondary_table_name}\")\n",
    "                return None\n",
    "            \n",
    "            # Clean and prepare secondary data with period identifier\n",
    "            secondary_clean = secondary_df.select(\n",
    "                clean_numeric_column_PBI_CP(secondary_df, sec_amount_col).alias(\"Amount\"),\n",
    "                col(sec_custname_col).alias(\"CustName\"),\n",
    "                col(sec_product_col).alias(\"Product\"),  # Added Product column\n",
    "                clean_date_column_PBI_CP(secondary_df, sec_ufr_date_col).alias(\"UFR_Date\"),\n",
    "                clean_numeric_column_PBI_CP(secondary_df, sec_ufr_amount_col).alias(\"UFR_Amount\") if sec_ufr_amount_col else lit(None).alias(\"UFR_Amount\"),\n",
    "                lit(period_filter_value).alias(\"Period_Source\"),  # Track which period this data came from\n",
    "                lit(lookback_period).alias(\"Lookback_Period\")     # Track lookback period\n",
    "            ).filter(\n",
    "                col(\"Amount\").is_not_null() & \n",
    "                col(\"CustName\").is_not_null() &\n",
    "                col(\"Product\").is_not_null() &  # Added Product filter\n",
    "                col(\"UFR_Date\").is_not_null() &\n",
    "                (trim(col(\"CustName\")) != \"\") &\n",
    "                (trim(col(\"Product\")) != \"\")  # Added Product trim filter\n",
    "            )\n",
    "            \n",
    "            return secondary_clean\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing secondary data for period {lookback_period}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def perform_multi_period_join(session, lookback_list_UFR):\n",
    "        \"\"\"Perform joins for multiple periods and combine results\"\"\"\n",
    "        try:\n",
    "            # Load and clean main table\n",
    "            main_df_original = session.table(main_table_name_PBI_CP)\n",
    "            print(f\"Main table loaded successfully with {main_df_original.count()} rows\")\n",
    "            \n",
    "            # Find column names in main table\n",
    "            main_amount_col = find_column_name(main_df_original, 'Amount')\n",
    "            main_custname_col = find_column_name(main_df_original, 'CustName')\n",
    "            main_current_period_col = find_column_name(main_df_original, 'Current Period')\n",
    "            main_product_col = find_column_name(main_df_original, 'Product')  # Added Product column\n",
    "            main_period_col = find_column_name(main_df_original, 'Period')\n",
    "            main_ufr_amount_col = find_column_name(main_df_original, 'UFR Amount')\n",
    "            main_ufr_date_col = find_column_name(main_df_original, 'UFR Date')\n",
    "            main_ufr_tag_col = find_column_name(main_df_original, 'UFR Tag')\n",
    "            \n",
    "            if not all([main_amount_col, main_custname_col, main_current_period_col, main_product_col, main_period_col]):\n",
    "                print(\"Required columns not found in main table\")\n",
    "                return None\n",
    "            \n",
    "            # Clean main table\n",
    "            main_select_cols = []\n",
    "            for c in main_df_original.columns:\n",
    "                if c == main_amount_col:\n",
    "                    main_select_cols.append(clean_numeric_column_PBI_CP(main_df_original, c).alias(c))\n",
    "                elif c == main_ufr_amount_col:\n",
    "                    main_select_cols.append(clean_numeric_column_PBI_CP(main_df_original, c).alias(c))\n",
    "                elif c == main_ufr_date_col:\n",
    "                    main_select_cols.append(clean_date_column_PBI_CP(main_df_original, c).alias(c))\n",
    "                elif c == main_current_period_col:\n",
    "                    main_select_cols.append(clean_date_column_PBI_CP(main_df_original, c).alias(c))\n",
    "                else:\n",
    "                    main_select_cols.append(col(c))\n",
    "            \n",
    "            main_clean = main_df_original.select(*main_select_cols)\n",
    "            \n",
    "            # Prepare all secondary data for all periods\n",
    "            all_secondary_data = []\n",
    "            valid_periods = []\n",
    "            \n",
    "            for period in lookback_list_UFR:\n",
    "                secondary_data = prepare_secondary_data_for_period(session, period)\n",
    "                if secondary_data is not None:\n",
    "                    all_secondary_data.append(secondary_data)\n",
    "                    valid_periods.append(period)\n",
    "                    print(f\"Successfully prepared data for period {period}\")\n",
    "                else:\n",
    "                    print(f\"Skipping period {period} due to data preparation failure\")\n",
    "            \n",
    "            if not all_secondary_data:\n",
    "                print(\"No valid secondary data found for any period\")\n",
    "                return main_clean\n",
    "            \n",
    "            # Union all secondary data\n",
    "            print(f\"Combining secondary data from {len(all_secondary_data)} periods\")\n",
    "            combined_secondary = all_secondary_data[0]\n",
    "            for i in range(1, len(all_secondary_data)):\n",
    "                combined_secondary = combined_secondary.union(all_secondary_data[i])\n",
    "            \n",
    "            print(f\"Combined secondary data has {combined_secondary.count()} rows\")\n",
    "            \n",
    "            # Create mapping of periods to filter main table\n",
    "            period_filter_values = [period_mapping.get(p) for p in valid_periods]\n",
    "            print(f\"Will process main table rows with Period in: {period_filter_values}\")\n",
    "            \n",
    "            # Filter main table for relevant periods\n",
    "            main_filtered = main_clean.filter(\n",
    "                col(main_period_col).isin(period_filter_values) &\n",
    "                col(main_amount_col).is_not_null() & \n",
    "                col(main_custname_col).is_not_null() &\n",
    "                col(main_current_period_col).is_not_null() &\n",
    "                col(main_product_col).is_not_null() &  # Added Product filter\n",
    "                (trim(col(main_custname_col)) != \"\") &\n",
    "                (trim(col(main_product_col)) != \"\")  # Added Product trim filter\n",
    "            )\n",
    "            \n",
    "            print(f\"Main table rows after filtering for relevant periods: {main_filtered.count()}\")\n",
    "            \n",
    "            # Perform join with combined secondary data - Updated join condition to include Product\n",
    "            join_condition = (\n",
    "                (main_filtered[main_amount_col] == combined_secondary[\"Amount\"]) & \n",
    "                (main_filtered[main_custname_col] == combined_secondary[\"CustName\"]) &\n",
    "                (main_filtered[main_current_period_col] == combined_secondary[\"UFR_Date\"]) &\n",
    "                (main_filtered[main_product_col] == combined_secondary[\"Product\"]) &  # Added Product join condition\n",
    "                (main_filtered[main_period_col] == combined_secondary[\"Period_Source\"])  # Match period\n",
    "            )\n",
    "            \n",
    "            print(\"Performing left outer join with multi-period data...\")\n",
    "            joined_df = main_filtered.join(combined_secondary, join_condition, \"left\")\n",
    "            \n",
    "            # Build result with updated UFR values\n",
    "            select_columns = []\n",
    "            \n",
    "            # Add all main table columns except UFR columns\n",
    "            for c in main_filtered.columns:\n",
    "                if c not in [main_ufr_amount_col, main_ufr_date_col, main_ufr_tag_col]:\n",
    "                    select_columns.append(col(c))\n",
    "            \n",
    "            # Handle UFR Amount (secondary takes precedence)\n",
    "            if main_ufr_amount_col:\n",
    "                ufr_amount_col = coalesce(\n",
    "                    col(\"UFR_Amount\"),  # From secondary\n",
    "                    col(main_ufr_amount_col)  # From main\n",
    "                ).alias(main_ufr_amount_col)\n",
    "            else:\n",
    "                ufr_amount_col = col(\"UFR_Amount\").alias('\"UFR Amount\"')\n",
    "            select_columns.append(ufr_amount_col)\n",
    "            \n",
    "            # Handle UFR Date (secondary takes precedence)\n",
    "            if main_ufr_date_col:\n",
    "                ufr_date_col = coalesce(\n",
    "                    col(\"UFR_Date\"),  # From secondary\n",
    "                    col(main_ufr_date_col)  # From main\n",
    "                ).alias(main_ufr_date_col)\n",
    "            else:\n",
    "                ufr_date_col = col(\"UFR_Date\").alias('\"UFR Date\"')\n",
    "            select_columns.append(ufr_date_col)\n",
    "            \n",
    "            # Handle UFR Tag (set to \"UFR\" if UFR Date exists, otherwise \"Not UFR\")\n",
    "            if main_ufr_tag_col:\n",
    "                ufr_tag_col = when(\n",
    "                    coalesce(col(\"UFR_Date\"), col(main_ufr_date_col) if main_ufr_date_col else lit(None)).is_not_null(),\n",
    "                    lit(\"UFR\")\n",
    "                ).otherwise(lit(\"Not UFR\")).alias(main_ufr_tag_col)\n",
    "            else:\n",
    "                ufr_tag_col = when(\n",
    "                    col(\"UFR_Date\").is_not_null(),\n",
    "                    lit(\"UFR\")\n",
    "                ).otherwise(lit(\"Not UFR\")).alias('\"UFR Tag\"')\n",
    "            select_columns.append(ufr_tag_col)\n",
    "            \n",
    "            joined_result = joined_df.select(*select_columns)\n",
    "            \n",
    "            # Now merge back with original main table to maintain all rows\n",
    "            print(\"Merging results back with original main table...\")\n",
    "            \n",
    "            # Create unique keys for joining back\n",
    "            main_with_key = main_clean.with_column(\n",
    "                \"temp_join_key\",\n",
    "                concat(\n",
    "                    coalesce(col(main_amount_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_custname_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_current_period_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_product_col), lit(\"NULL\")),  # Added Product to key\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_period_col), lit(\"NULL\"))\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            joined_with_key = joined_result.with_column(\n",
    "                \"temp_join_key\",\n",
    "                concat(\n",
    "                    coalesce(col(main_amount_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_custname_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_current_period_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_product_col), lit(\"NULL\")),  # Added Product to key\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_period_col), lit(\"NULL\"))\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Final merge\n",
    "            final_result = main_with_key.join(\n",
    "                joined_with_key.select(\n",
    "                    col(\"temp_join_key\").alias(\"joined_key\"),\n",
    "                    col(main_ufr_amount_col if main_ufr_amount_col else '\"UFR Amount\"').alias(\"new_UFR_Amount\"),\n",
    "                    col(main_ufr_date_col if main_ufr_date_col else '\"UFR Date\"').alias(\"new_UFR_Date\"),\n",
    "                    col(main_ufr_tag_col if main_ufr_tag_col else '\"UFR Tag\"').alias(\"new_UFR_Tag\")\n",
    "                ),\n",
    "                main_with_key[\"temp_join_key\"] == col(\"joined_key\"),\n",
    "                \"left\"\n",
    "            )\n",
    "            \n",
    "            # Build final select with all original columns\n",
    "            final_select_cols = []\n",
    "            for c in main_clean.columns:\n",
    "                if c == main_ufr_amount_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Amount\"), col(c)).alias(c))\n",
    "                elif c == main_ufr_date_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Date\"), col(c)).alias(c))\n",
    "                elif c == main_ufr_tag_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Tag\"), col(c), lit(\"Not UFR\")).alias(c))\n",
    "                else:\n",
    "                    final_select_cols.append(col(c))\n",
    "            \n",
    "            # Add UFR columns if they didn't exist in main table\n",
    "            if not main_ufr_amount_col:\n",
    "                final_select_cols.append(coalesce(col(\"new_UFR_Amount\"), lit(None)).alias('\"UFR Amount\"'))\n",
    "            if not main_ufr_date_col:\n",
    "                final_select_cols.append(coalesce(col(\"new_UFR_Date\"), lit(None)).alias('\"UFR Date\"'))\n",
    "            if not main_ufr_tag_col:\n",
    "                final_select_cols.append(coalesce(col(\"new_UFR_Tag\"), lit(\"Not UFR\")).alias('\"UFR Tag\"'))\n",
    "            \n",
    "            result_df = final_result.select(*final_select_cols)\n",
    "            \n",
    "            # Show summary statistics\n",
    "            total_original_rows = main_df_original.count()\n",
    "            total_result_rows = result_df.count()\n",
    "            \n",
    "            print(f\"Original main table rows: {total_original_rows}\")\n",
    "            print(f\"Final result rows: {total_result_rows}\")\n",
    "            \n",
    "            if total_result_rows == total_original_rows:\n",
    "                print(\"✓ Successfully maintained original row count\")\n",
    "            else:\n",
    "                print(\"⚠ Warning: Row count changed!\")\n",
    "            \n",
    "            # Show UFR statistics by period\n",
    "            print(f\"\\nProcessed periods: {valid_periods}\")\n",
    "            print(f\"Period mappings: {[period_mapping.get(p) for p in valid_periods]}\")\n",
    "            \n",
    "            # Show UFR Tag summary\n",
    "            ufr_tag_col_name = main_ufr_tag_col if main_ufr_tag_col else '\"UFR Tag\"'\n",
    "            try:\n",
    "                result_df.group_by(ufr_tag_col_name).count().order_by(ufr_tag_col_name).show()\n",
    "            except:\n",
    "                print(\"Could not display UFR Tag summary\")\n",
    "            \n",
    "            return result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in multi-period join operation: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def UFR_PBI_CP_main(session):\n",
    "        \"\"\"Main function to execute the multi-period join process\"\"\"\n",
    "        print(\"=*=\"*25)\n",
    "        print(\"Starting multi-period UFR join process...\")\n",
    "        print(f\"Lookback periods: {lookback_list_UFR}\")\n",
    "        print(f\"Period mappings: {[period_mapping.get(p, 'Unknown') for p in lookback_list_UFR]}\")\n",
    "        print(\"Join conditions: Amount = Amount AND CustName = CustName AND Current Period = UFR Date AND Product = Product AND Period = Period_Source\")\n",
    "        \n",
    "        result = perform_multi_period_join(session, lookback_list_UFR)\n",
    "        \n",
    "        if result is not None:\n",
    "            print(\"Multi-period join completed successfully!\")\n",
    "            \n",
    "            # Save result\n",
    "            pbi_cp_ufr = main_table_name_PBI_CP + \"_UFR\"\n",
    "            result.write.mode(\"overwrite\").save_as_table(pbi_cp_ufr)\n",
    "            print(\"=\"*100)\n",
    "            print(f\"___Saving on path: {pbi_cp_ufr}\")\n",
    "            print(\"=\"*100)\n",
    "        else:\n",
    "            print(\"Multi-period join process failed\")\n",
    "            print(\"=\"*100)\n",
    "    \n",
    "    UFR_PBI_CP_main(session)\n",
    "\n",
    "# Example usage:\n",
    "# UFR_PBI_CP_final(session, [1, 3])      # For MOM and QOQ\n",
    "# UFR_PBI_CP_final(session, [1, 3, 12])  # For MOM, QOQ, and YOY\n",
    "# UFR_PBI_CP_final(session, [12])        # For YOY only\n",
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "# lookback_list_UFR = [1, 3, 12]  # Multiple periods: MOM, QOQ, YOY\n",
    "# pbi_retention_output_path = \"NEWFOLD_TEST_DB.SOURCE.MRR_BLOWOUT_10K_21JUNE_PBI_MASTER\"\n",
    "# input_file_path = \"NEWFOLD_TEST_DB.SOURCE.MRR_BLOWOUT_10K\"\n",
    "\n",
    "# Define table names\n",
    "main_table_name_PBI_CPR = f\"{pbi_retention_output_path}_CPR\"\n",
    "secondary_tables_PBI_CPR = {\n",
    "    1: f\"{input_file_path}_CPR_MOM\",   # Month\n",
    "    3: f\"{input_file_path}_CPR_QOQ\",   # Quarter  \n",
    "    12: f\"{input_file_path}_CPR_YOY\"   # Year\n",
    "}\n",
    "\n",
    "# Common columns for join and update (updated with Product and Revenue Type)\n",
    "join_columns = ['Amount', 'CustName', 'Current Period', 'Product', 'Revenue Type']\n",
    "update_columns = ['UFR Amount', 'UFR Date']\n",
    "\n",
    "# Period mapping based on lookback values\n",
    "period_mapping = {\n",
    "    1: \"Month\",\n",
    "    3: \"Quarter\", \n",
    "    12: \"Year\"\n",
    "}\n",
    "\n",
    "def UFR_PBI_CPR_final(session, lookback_list_UFR):\n",
    "    def clean_numeric_column_PBI_CPR(df, col_name):\n",
    "        \"\"\"Clean numeric column by converting empty strings and whitespace to NULL\"\"\"\n",
    "        return when(\n",
    "            (col(col_name).is_null()) | \n",
    "            (trim(col(col_name)) == \"\") | \n",
    "            (length(trim(col(col_name))) == 0),\n",
    "            lit(None)\n",
    "        ).otherwise(col(col_name))\n",
    "    \n",
    "    def clean_date_column_PBI_CPR(df, col_name):\n",
    "        \"\"\"Clean date column by converting empty strings and whitespace to NULL\"\"\"\n",
    "        return when(\n",
    "            (col(col_name).is_null()) | \n",
    "            (trim(col(col_name)) == \"\") | \n",
    "            (length(trim(col(col_name))) == 0),\n",
    "            lit(None)\n",
    "        ).otherwise(col(col_name))\n",
    "    \n",
    "    def clean_string_column_PBI_CPR(df, col_name):\n",
    "        \"\"\"Clean string column by converting empty strings and whitespace to NULL\"\"\"\n",
    "        return when(\n",
    "            (col(col_name).is_null()) | \n",
    "            (trim(col(col_name)) == \"\") | \n",
    "            (length(trim(col(col_name))) == 0),\n",
    "            lit(None)\n",
    "        ).otherwise(trim(col(col_name)))\n",
    "    \n",
    "    def find_column_name(df, target_col):\n",
    "        \"\"\"Find the actual column name that matches the target (case insensitive, handles quotes)\"\"\"\n",
    "        target_clean = target_col.upper().replace(' ', '').replace('_', '').replace('\"', '')\n",
    "        for col_name in df.columns:\n",
    "            col_clean = col_name.upper().replace(' ', '').replace('_', '').replace('\"', '')\n",
    "            if col_clean == target_clean:\n",
    "                return col_name\n",
    "        return None\n",
    "    \n",
    "    def prepare_secondary_data_for_period(session, lookback_period):\n",
    "        \"\"\"Prepare secondary table data for a specific period\"\"\"\n",
    "        try:\n",
    "            if lookback_period not in secondary_tables_PBI_CPR:\n",
    "                print(f\"Invalid lookback period: {lookback_period}\")\n",
    "                return None\n",
    "                \n",
    "            secondary_table_name = secondary_tables_PBI_CPR[lookback_period]\n",
    "            period_filter_value = period_mapping.get(lookback_period)\n",
    "            \n",
    "            print(f\"Processing secondary table: {secondary_table_name} for period: {period_filter_value}\")\n",
    "            \n",
    "            # Load secondary table\n",
    "            try:\n",
    "                secondary_df = session.table(secondary_table_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Secondary table {secondary_table_name} not available: {str(e)}\")\n",
    "                return None\n",
    "            \n",
    "            # Find column names in secondary table\n",
    "            sec_amount_col = find_column_name(secondary_df, 'Amount')\n",
    "            sec_custname_col = find_column_name(secondary_df, 'CustName')\n",
    "            sec_product_col = find_column_name(secondary_df, 'Product')\n",
    "            sec_revenue_type_col = find_column_name(secondary_df, 'Revenue Type')\n",
    "            sec_ufr_amount_col = find_column_name(secondary_df, 'UFR Amount')\n",
    "            sec_ufr_date_col = find_column_name(secondary_df, 'UFR Date')\n",
    "            \n",
    "            if not all([sec_amount_col, sec_custname_col, sec_product_col, sec_revenue_type_col, sec_ufr_date_col]):\n",
    "                print(f\"Required columns not found in secondary table {secondary_table_name}\")\n",
    "                return None\n",
    "            \n",
    "            # Clean and prepare secondary data with period identifier\n",
    "            secondary_clean = secondary_df.select(\n",
    "                clean_numeric_column_PBI_CPR(secondary_df, sec_amount_col).alias(\"Amount\"),\n",
    "                clean_string_column_PBI_CPR(secondary_df, sec_custname_col).alias(\"CustName\"),\n",
    "                clean_string_column_PBI_CPR(secondary_df, sec_product_col).alias(\"Product\"),\n",
    "                clean_string_column_PBI_CPR(secondary_df, sec_revenue_type_col).alias(\"Revenue_Type\"),\n",
    "                clean_date_column_PBI_CPR(secondary_df, sec_ufr_date_col).alias(\"UFR_Date\"),\n",
    "                clean_numeric_column_PBI_CPR(secondary_df, sec_ufr_amount_col).alias(\"UFR_Amount\") if sec_ufr_amount_col else lit(None).alias(\"UFR_Amount\"),\n",
    "                lit(period_filter_value).alias(\"Period_Source\"),  # Track which period this data came from\n",
    "                lit(lookback_period).alias(\"Lookback_Period\")     # Track lookback period\n",
    "            ).filter(\n",
    "                col(\"Amount\").is_not_null() & \n",
    "                col(\"CustName\").is_not_null() &\n",
    "                col(\"Product\").is_not_null() &\n",
    "                col(\"Revenue_Type\").is_not_null() &\n",
    "                col(\"UFR_Date\").is_not_null() &\n",
    "                (trim(col(\"CustName\")) != \"\") &\n",
    "                (trim(col(\"Product\")) != \"\") &\n",
    "                (trim(col(\"Revenue_Type\")) != \"\")\n",
    "            )\n",
    "            \n",
    "            return secondary_clean\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing secondary data for period {lookback_period}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def perform_multi_period_join(session, lookback_list_UFR):\n",
    "        \"\"\"Perform joins for multiple periods and combine results\"\"\"\n",
    "        try:\n",
    "            # Load and clean main table\n",
    "            main_df_original = session.table(main_table_name_PBI_CPR)\n",
    "            print(f\"Main table loaded successfully with {main_df_original.count()} rows\")\n",
    "            \n",
    "            # Find column names in main table\n",
    "            main_amount_col = find_column_name(main_df_original, 'Amount')\n",
    "            main_custname_col = find_column_name(main_df_original, 'CustName')\n",
    "            main_current_period_col = find_column_name(main_df_original, 'Current Period')\n",
    "            main_product_col = find_column_name(main_df_original, 'Product')\n",
    "            main_revenue_type_col = find_column_name(main_df_original, 'Revenue Type')\n",
    "            main_period_col = find_column_name(main_df_original, 'Period')\n",
    "            main_ufr_amount_col = find_column_name(main_df_original, 'UFR Amount')\n",
    "            main_ufr_date_col = find_column_name(main_df_original, 'UFR Date')\n",
    "            main_ufr_tag_col = find_column_name(main_df_original, 'UFR Tag')\n",
    "            \n",
    "            if not all([main_amount_col, main_custname_col, main_current_period_col, main_product_col, main_revenue_type_col, main_period_col]):\n",
    "                print(\"Required columns not found in main table\")\n",
    "                return None\n",
    "            \n",
    "            # Clean main table\n",
    "            main_select_cols = []\n",
    "            for c in main_df_original.columns:\n",
    "                if c == main_amount_col:\n",
    "                    main_select_cols.append(clean_numeric_column_PBI_CPR(main_df_original, c).alias(c))\n",
    "                elif c == main_ufr_amount_col:\n",
    "                    main_select_cols.append(clean_numeric_column_PBI_CPR(main_df_original, c).alias(c))\n",
    "                elif c == main_ufr_date_col:\n",
    "                    main_select_cols.append(clean_date_column_PBI_CPR(main_df_original, c).alias(c))\n",
    "                elif c == main_current_period_col:\n",
    "                    main_select_cols.append(clean_date_column_PBI_CPR(main_df_original, c).alias(c))\n",
    "                elif c in [main_custname_col, main_product_col, main_revenue_type_col]:\n",
    "                    main_select_cols.append(clean_string_column_PBI_CPR(main_df_original, c).alias(c))\n",
    "                else:\n",
    "                    main_select_cols.append(col(c))\n",
    "            \n",
    "            main_clean = main_df_original.select(*main_select_cols)\n",
    "            \n",
    "            # Prepare all secondary data for all periods\n",
    "            all_secondary_data = []\n",
    "            valid_periods = []\n",
    "            \n",
    "            for period in lookback_list_UFR:\n",
    "                secondary_data = prepare_secondary_data_for_period(session, period)\n",
    "                if secondary_data is not None:\n",
    "                    all_secondary_data.append(secondary_data)\n",
    "                    valid_periods.append(period)\n",
    "                    print(f\"Successfully prepared data for period {period}\")\n",
    "                else:\n",
    "                    print(f\"Skipping period {period} due to data preparation failure\")\n",
    "            \n",
    "            if not all_secondary_data:\n",
    "                print(\"No valid secondary data found for any period\")\n",
    "                return main_clean\n",
    "            \n",
    "            # Union all secondary data\n",
    "            print(f\"Combining secondary data from {len(all_secondary_data)} periods\")\n",
    "            combined_secondary = all_secondary_data[0]\n",
    "            for i in range(1, len(all_secondary_data)):\n",
    "                combined_secondary = combined_secondary.union(all_secondary_data[i])\n",
    "            \n",
    "            print(f\"Combined secondary data has {combined_secondary.count()} rows\")\n",
    "            \n",
    "            # Create mapping of periods to filter main table\n",
    "            period_filter_values = [period_mapping.get(p) for p in valid_periods]\n",
    "            print(f\"Will process main table rows with Period in: {period_filter_values}\")\n",
    "            \n",
    "            # Filter main table for relevant periods\n",
    "            main_filtered = main_clean.filter(\n",
    "                col(main_period_col).isin(period_filter_values) &\n",
    "                col(main_amount_col).is_not_null() & \n",
    "                col(main_custname_col).is_not_null() &\n",
    "                col(main_current_period_col).is_not_null() &\n",
    "                col(main_product_col).is_not_null() &\n",
    "                col(main_revenue_type_col).is_not_null() &\n",
    "                (trim(col(main_custname_col)) != \"\") &\n",
    "                (trim(col(main_product_col)) != \"\") &\n",
    "                (trim(col(main_revenue_type_col)) != \"\")\n",
    "            )\n",
    "            \n",
    "            print(f\"Main table rows after filtering for relevant periods: {main_filtered.count()}\")\n",
    "            \n",
    "            # Perform join with combined secondary data (5 join conditions)\n",
    "            join_condition = (\n",
    "                (main_filtered[main_amount_col] == combined_secondary[\"Amount\"]) & \n",
    "                (main_filtered[main_custname_col] == combined_secondary[\"CustName\"]) &\n",
    "                (main_filtered[main_current_period_col] == combined_secondary[\"UFR_Date\"]) &\n",
    "                (main_filtered[main_product_col] == combined_secondary[\"Product\"]) &\n",
    "                (main_filtered[main_revenue_type_col] == combined_secondary[\"Revenue_Type\"]) &\n",
    "                (main_filtered[main_period_col] == combined_secondary[\"Period_Source\"])  # Match period\n",
    "            )\n",
    "            \n",
    "            print(\"Performing left outer join with multi-period data (5 join conditions + period matching)...\")\n",
    "            joined_df = main_filtered.join(combined_secondary, join_condition, \"left\")\n",
    "            \n",
    "            # Build result with updated UFR values\n",
    "            select_columns = []\n",
    "            \n",
    "            # Add all main table columns except UFR columns\n",
    "            for c in main_filtered.columns:\n",
    "                if c not in [main_ufr_amount_col, main_ufr_date_col, main_ufr_tag_col]:\n",
    "                    select_columns.append(col(c))\n",
    "            \n",
    "            # Handle UFR Amount (secondary takes precedence)\n",
    "            if main_ufr_amount_col:\n",
    "                ufr_amount_col = coalesce(\n",
    "                    col(\"UFR_Amount\"),  # From secondary\n",
    "                    col(main_ufr_amount_col)  # From main\n",
    "                ).alias(main_ufr_amount_col)\n",
    "            else:\n",
    "                ufr_amount_col = col(\"UFR_Amount\").alias('\"UFR Amount\"')\n",
    "            select_columns.append(ufr_amount_col)\n",
    "            \n",
    "            # Handle UFR Date (secondary takes precedence)\n",
    "            if main_ufr_date_col:\n",
    "                ufr_date_col = coalesce(\n",
    "                    col(\"UFR_Date\"),  # From secondary\n",
    "                    col(main_ufr_date_col)  # From main\n",
    "                ).alias(main_ufr_date_col)\n",
    "            else:\n",
    "                ufr_date_col = col(\"UFR_Date\").alias('\"UFR Date\"')\n",
    "            select_columns.append(ufr_date_col)\n",
    "            \n",
    "            # Handle UFR Tag (set to \"UFR\" if UFR Date exists, otherwise \"Not UFR\")\n",
    "            if main_ufr_tag_col:\n",
    "                ufr_tag_col = when(\n",
    "                    coalesce(col(\"UFR_Date\"), col(main_ufr_date_col) if main_ufr_date_col else lit(None)).is_not_null(),\n",
    "                    lit(\"UFR\")\n",
    "                ).otherwise(lit(\"Not UFR\")).alias(main_ufr_tag_col)\n",
    "            else:\n",
    "                ufr_tag_col = when(\n",
    "                    col(\"UFR_Date\").is_not_null(),\n",
    "                    lit(\"UFR\")\n",
    "                ).otherwise(lit(\"Not UFR\")).alias('\"UFR Tag\"')\n",
    "            select_columns.append(ufr_tag_col)\n",
    "            \n",
    "            joined_result = joined_df.select(*select_columns)\n",
    "            \n",
    "            # Now merge back with original main table to maintain all rows\n",
    "            print(\"Merging results back with original main table...\")\n",
    "            \n",
    "            # Create unique keys for joining back\n",
    "            main_with_key = main_clean.with_column(\n",
    "                \"temp_join_key\",\n",
    "                concat(\n",
    "                    coalesce(col(main_amount_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_custname_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_current_period_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_product_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_revenue_type_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_period_col), lit(\"NULL\"))\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            joined_with_key = joined_result.with_column(\n",
    "                \"temp_join_key\",\n",
    "                concat(\n",
    "                    coalesce(col(main_amount_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_custname_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_current_period_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_product_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_revenue_type_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_period_col), lit(\"NULL\"))\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Final merge\n",
    "            final_result = main_with_key.join(\n",
    "                joined_with_key.select(\n",
    "                    col(\"temp_join_key\").alias(\"joined_key\"),\n",
    "                    col(main_ufr_amount_col if main_ufr_amount_col else '\"UFR Amount\"').alias(\"new_UFR_Amount\"),\n",
    "                    col(main_ufr_date_col if main_ufr_date_col else '\"UFR Date\"').alias(\"new_UFR_Date\"),\n",
    "                    col(main_ufr_tag_col if main_ufr_tag_col else '\"UFR Tag\"').alias(\"new_UFR_Tag\")\n",
    "                ),\n",
    "                main_with_key[\"temp_join_key\"] == col(\"joined_key\"),\n",
    "                \"left\"\n",
    "            )\n",
    "            \n",
    "            # Build final select with all original columns\n",
    "            final_select_cols = []\n",
    "            for c in main_clean.columns:\n",
    "                if c == main_ufr_amount_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Amount\"), col(c)).alias(c))\n",
    "                elif c == main_ufr_date_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Date\"), col(c)).alias(c))\n",
    "                elif c == main_ufr_tag_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Tag\"), col(c), lit(\"Not UFR\")).alias(c))\n",
    "                else:\n",
    "                    final_select_cols.append(col(c))\n",
    "            \n",
    "            # Add UFR columns if they didn't exist in main table\n",
    "            if not main_ufr_amount_col:\n",
    "                final_select_cols.append(coalesce(col(\"new_UFR_Amount\"), lit(None)).alias('\"UFR Amount\"'))\n",
    "            if not main_ufr_date_col:\n",
    "                final_select_cols.append(coalesce(col(\"new_UFR_Date\"), lit(None)).alias('\"UFR Date\"'))\n",
    "            if not main_ufr_tag_col:\n",
    "                final_select_cols.append(coalesce(col(\"new_UFR_Tag\"), lit(\"Not UFR\")).alias('\"UFR Tag\"'))\n",
    "            \n",
    "            result_df = final_result.select(*final_select_cols)\n",
    "            \n",
    "            # Show summary statistics\n",
    "            total_original_rows = main_df_original.count()\n",
    "            total_result_rows = result_df.count()\n",
    "            \n",
    "            print(f\"Original main table rows: {total_original_rows}\")\n",
    "            print(f\"Final result rows: {total_result_rows}\")\n",
    "            \n",
    "            if total_result_rows == total_original_rows:\n",
    "                print(\"✓ Successfully maintained original row count\")\n",
    "            else:\n",
    "                print(\"⚠ Warning: Row count changed!\")\n",
    "            \n",
    "            # Show UFR statistics by period\n",
    "            print(f\"\\nProcessed periods: {valid_periods}\")\n",
    "            print(f\"Period mappings: {[period_mapping.get(p) for p in valid_periods]}\")\n",
    "            \n",
    "            # Show UFR Tag summary\n",
    "            ufr_tag_col_name = main_ufr_tag_col if main_ufr_tag_col else '\"UFR Tag\"'\n",
    "            try:\n",
    "                result_df.group_by(ufr_tag_col_name).count().order_by(ufr_tag_col_name).show()\n",
    "            except:\n",
    "                print(\"Could not display UFR Tag summary\")\n",
    "            \n",
    "            return result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in multi-period join operation: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def UFR_PBI_CPR_main(session):\n",
    "        \"\"\"Main function to execute the multi-period join process\"\"\"\n",
    "        print(\"=*=\"*25)\n",
    "        print(\"Starting multi-period UFR join process for CPR...\")\n",
    "        print(f\"Lookback periods: {lookback_list_UFR}\")\n",
    "        print(f\"Period mappings: {[period_mapping.get(p, 'Unknown') for p in lookback_list_UFR]}\")\n",
    "        print(\"Join conditions: Amount = Amount AND CustName = CustName AND Current Period = UFR Date AND Product = Product AND Revenue Type = Revenue Type AND Period = Period_Source\")\n",
    "        \n",
    "        result = perform_multi_period_join(session, lookback_list_UFR)\n",
    "        \n",
    "        if result is not None:\n",
    "            print(\"Multi-period join completed successfully!\")\n",
    "            \n",
    "            # Save result\n",
    "            pbi_cpr_ufr = main_table_name_PBI_CPR + \"_UFR\"\n",
    "            result.write.mode(\"overwrite\").save_as_table(pbi_cpr_ufr)\n",
    "            print(\"=\"*100)\n",
    "            print(f\"___Saving on path: {pbi_cpr_ufr}\")\n",
    "            print(\"=\"*100)\n",
    "        else:\n",
    "            print(\"Multi-period join process failed\")\n",
    "            print(\"=\"*100)\n",
    "    \n",
    "    UFR_PBI_CPR_main(session)\n",
    "\n",
    "# Example usage:\n",
    "# UFR_PBI_CPR_final(session, [1, 3])      # For MOM and QOQ\n",
    "# UFR_PBI_CPR_final(session, [1, 3, 12])  # For MOM, QOQ, and YOY\n",
    "# UFR_PBI_CPR_final(session, [12])        # For YOY only\n",
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "# lookback_list_UFR = [1, 3, 12]  # Multiple periods: MOM, QOQ, YOY\n",
    "# excel_retention_output_path = \"NEWFOLD_TEST_DB.SOURCE.MRR_BLOWOUT_10K_SM22JUNE_EXCEL_MASTER\"\n",
    "# input_file_path = \"NEWFOLD_TEST_DB.SOURCE.MRR_BLOWOUT_10K\"\n",
    "\n",
    "# Common columns for join and update\n",
    "join_columns = ['Amount', 'CustName', 'Current Period']\n",
    "update_columns = ['UFR Amount', 'UFR Date']\n",
    "\n",
    "# Period mapping based on lookback values\n",
    "period_mapping = {\n",
    "    1: \"Month\",\n",
    "    3: \"Quarter\", \n",
    "    12: \"Year\"\n",
    "}\n",
    "\n",
    "def UFR_Excel_C_final(session, lookback_list_UFR):\n",
    "    \n",
    "    # Define table names\n",
    "    main_table_name_Excel_C = f\"{excel_retention_output_path}_C\"\n",
    "    output_Excel_C = main_table_name_Excel_C + \"_UFR\"\n",
    "    secondary_tables_Excel_C = {\n",
    "        1: f\"{input_file_path}_C_MOM\",   # Month\n",
    "        3: f\"{input_file_path}_C_QOQ\",   # Quarter\n",
    "        12: f\"{input_file_path}_C_YOY\"   # Year\n",
    "    }\n",
    "    \n",
    "    def clean_numeric_column_Excel_C(df, col_name):\n",
    "        \"\"\"Clean numeric column by converting empty strings and whitespace to NULL\"\"\"\n",
    "        return when(\n",
    "            (col(col_name).is_null()) | \n",
    "            (trim(col(col_name)) == \"\") | \n",
    "            (length(trim(col(col_name))) == 0),\n",
    "            lit(None)\n",
    "        ).otherwise(col(col_name))\n",
    "    \n",
    "    def clean_date_column_Excel_C(df, col_name):\n",
    "        \"\"\"Clean date column by converting empty strings and whitespace to NULL\"\"\"\n",
    "        return when(\n",
    "            (col(col_name).is_null()) | \n",
    "            (trim(col(col_name)) == \"\") | \n",
    "            (length(trim(col(col_name))) == 0),\n",
    "            lit(None)\n",
    "        ).otherwise(col(col_name))\n",
    "    \n",
    "    def find_column_name_Excel_C(df, target_col):\n",
    "        \"\"\"Find the actual column name that matches the target (case insensitive, handles quotes)\"\"\"\n",
    "        target_clean = target_col.upper().replace(' ', '').replace('_', '').replace('\"', '')\n",
    "        for col_name in df.columns:\n",
    "            col_clean = col_name.upper().replace(' ', '').replace('_', '').replace('\"', '')\n",
    "            if col_clean == target_clean:\n",
    "                return col_name\n",
    "        return None\n",
    "    \n",
    "    def prepare_secondary_data_for_period_Excel_C(session, lookback_period):\n",
    "        \"\"\"Prepare secondary table data for a specific period\"\"\"\n",
    "        try:\n",
    "            if lookback_period not in secondary_tables_Excel_C:\n",
    "                print(f\"Invalid lookback period: {lookback_period}\")\n",
    "                return None\n",
    "                \n",
    "            secondary_table_name = secondary_tables_Excel_C[lookback_period]\n",
    "            period_filter_value = period_mapping.get(lookback_period)\n",
    "            \n",
    "            print(f\"Processing secondary table: {secondary_table_name} for period: {period_filter_value}\")\n",
    "            \n",
    "            # Load secondary table\n",
    "            try:\n",
    "                secondary_df = session.table(secondary_table_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Secondary table {secondary_table_name} not available: {str(e)}\")\n",
    "                return None\n",
    "            \n",
    "            # Find column names in secondary table\n",
    "            sec_amount_col = find_column_name_Excel_C(secondary_df, 'Amount')\n",
    "            sec_custname_col = find_column_name_Excel_C(secondary_df, 'CustName')\n",
    "            sec_ufr_amount_col = find_column_name_Excel_C(secondary_df, 'UFR Amount')\n",
    "            sec_ufr_date_col = find_column_name_Excel_C(secondary_df, 'UFR Date')\n",
    "            \n",
    "            if not all([sec_amount_col, sec_custname_col, sec_ufr_date_col]):\n",
    "                print(f\"Required columns not found in secondary table {secondary_table_name}\")\n",
    "                return None\n",
    "            \n",
    "            # Clean and prepare secondary data with period identifier\n",
    "            secondary_clean = secondary_df.select(\n",
    "                clean_numeric_column_Excel_C(secondary_df, sec_amount_col).alias(\"Amount\"),\n",
    "                col(sec_custname_col).alias(\"CustName\"),\n",
    "                clean_date_column_Excel_C(secondary_df, sec_ufr_date_col).alias(\"UFR_Date\"),\n",
    "                clean_numeric_column_Excel_C(secondary_df, sec_ufr_amount_col).alias(\"UFR_Amount\") if sec_ufr_amount_col else lit(None).alias(\"UFR_Amount\"),\n",
    "                lit(period_filter_value).alias(\"Period_Source\"),  # Track which period this data came from\n",
    "                lit(lookback_period).alias(\"Lookback_Period\")     # Track lookback period\n",
    "            ).filter(\n",
    "                col(\"Amount\").is_not_null() & \n",
    "                col(\"CustName\").is_not_null() &\n",
    "                col(\"UFR_Date\").is_not_null() &\n",
    "                (trim(col(\"CustName\")) != \"\")\n",
    "            )\n",
    "            \n",
    "            return secondary_clean\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing secondary data for period {lookback_period}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def perform_multi_period_join_Excel_C(session, lookback_list_UFR):\n",
    "        \"\"\"Perform joins for multiple periods and combine results\"\"\"\n",
    "        try:\n",
    "            # Load main table\n",
    "            main_df_original = session.table(main_table_name_Excel_C)\n",
    "            print(f\"Main table loaded successfully with {main_df_original.count()} rows\")\n",
    "            \n",
    "            # Find column names in main table\n",
    "            main_amount_col = find_column_name_Excel_C(main_df_original, 'Amount')\n",
    "            main_custname_col = find_column_name_Excel_C(main_df_original, 'CustName')\n",
    "            main_current_period_col = find_column_name_Excel_C(main_df_original, 'Current Period')\n",
    "            main_period_col = find_column_name_Excel_C(main_df_original, 'Period')\n",
    "            main_ufr_amount_col = find_column_name_Excel_C(main_df_original, 'UFR Amount')\n",
    "            main_ufr_date_col = find_column_name_Excel_C(main_df_original, 'UFR Date')\n",
    "            main_ufr_tag_col = find_column_name_Excel_C(main_df_original, 'UFR Tag')\n",
    "            \n",
    "            if not all([main_amount_col, main_custname_col, main_current_period_col]):\n",
    "                print(\"Required columns not found in main table\")\n",
    "                return None\n",
    "            \n",
    "            # Clean main table\n",
    "            main_select_cols = []\n",
    "            for c in main_df_original.columns:\n",
    "                if c == main_amount_col:\n",
    "                    main_select_cols.append(clean_numeric_column_Excel_C(main_df_original, c).alias(c))\n",
    "                elif c == main_ufr_amount_col:\n",
    "                    main_select_cols.append(clean_numeric_column_Excel_C(main_df_original, c).alias(c))\n",
    "                elif c == main_ufr_date_col:\n",
    "                    main_select_cols.append(clean_date_column_Excel_C(main_df_original, c).alias(c))\n",
    "                elif c == main_current_period_col:\n",
    "                    main_select_cols.append(clean_date_column_Excel_C(main_df_original, c).alias(c))\n",
    "                else:\n",
    "                    main_select_cols.append(col(c))\n",
    "            \n",
    "            # Add missing UFR columns if they don't exist\n",
    "            if not main_ufr_amount_col:\n",
    "                main_select_cols.append(lit(None).cast(DecimalType()).alias('\"UFR Amount\"'))\n",
    "                main_ufr_amount_col = '\"UFR Amount\"'\n",
    "                print(\"Added UFR Amount column\")\n",
    "                \n",
    "            if not main_ufr_date_col:\n",
    "                main_select_cols.append(lit(None).cast(DateType()).alias('\"UFR Date\"'))\n",
    "                main_ufr_date_col = '\"UFR Date\"'\n",
    "                print(\"Added UFR Date column\")\n",
    "                \n",
    "            if not main_ufr_tag_col:\n",
    "                main_select_cols.append(lit(\"Not UFR\").cast(StringType()).alias('\"UFR Tag\"'))\n",
    "                main_ufr_tag_col = '\"UFR Tag\"'\n",
    "                print(\"Added UFR Tag column\")\n",
    "            \n",
    "            main_clean = main_df_original.select(*main_select_cols)\n",
    "            \n",
    "            # Prepare all secondary data for all periods\n",
    "            all_secondary_data = []\n",
    "            valid_periods = []\n",
    "            \n",
    "            for period in lookback_list_UFR:\n",
    "                secondary_data = prepare_secondary_data_for_period_Excel_C(session, period)\n",
    "                if secondary_data is not None:\n",
    "                    all_secondary_data.append(secondary_data)\n",
    "                    valid_periods.append(period)\n",
    "                    print(f\"Successfully prepared data for period {period}\")\n",
    "                else:\n",
    "                    print(f\"Skipping period {period} due to data preparation failure\")\n",
    "            \n",
    "            if not all_secondary_data:\n",
    "                print(\"No valid secondary data found for any period\")\n",
    "                return main_clean\n",
    "            \n",
    "            # Union all secondary data\n",
    "            print(f\"Combining secondary data from {len(all_secondary_data)} periods\")\n",
    "            combined_secondary = all_secondary_data[0]\n",
    "            for i in range(1, len(all_secondary_data)):\n",
    "                combined_secondary = combined_secondary.union(all_secondary_data[i])\n",
    "            \n",
    "            print(f\"Combined secondary data has {combined_secondary.count()} rows\")\n",
    "            \n",
    "            # Create mapping of periods to filter main table (if Period column exists)\n",
    "            period_filter_values = [period_mapping.get(p) for p in valid_periods]\n",
    "            print(f\"Period mappings for filtering: {period_filter_values}\")\n",
    "            \n",
    "            # Filter main table based on Period column if it exists\n",
    "            if main_period_col:\n",
    "                print(f\"Filtering main table rows with Period in: {period_filter_values}\")\n",
    "                main_filtered = main_clean.filter(\n",
    "                    col(main_period_col).isin(period_filter_values) &\n",
    "                    col(main_amount_col).is_not_null() & \n",
    "                    col(main_custname_col).is_not_null() &\n",
    "                    col(main_current_period_col).is_not_null() &\n",
    "                    (trim(col(main_custname_col)) != \"\")\n",
    "                )\n",
    "            else:\n",
    "                print(\"No Period column found in main table, processing all rows\")\n",
    "                main_filtered = main_clean.filter(\n",
    "                    col(main_amount_col).is_not_null() & \n",
    "                    col(main_custname_col).is_not_null() &\n",
    "                    col(main_current_period_col).is_not_null() &\n",
    "                    (trim(col(main_custname_col)) != \"\")\n",
    "                )\n",
    "            \n",
    "            print(f\"Main table rows after filtering: {main_filtered.count()}\")\n",
    "            \n",
    "            # Perform join with combined secondary data\n",
    "            if main_period_col:\n",
    "                # Join with period matching\n",
    "                join_condition = (\n",
    "                    (main_filtered[main_amount_col] == combined_secondary[\"Amount\"]) & \n",
    "                    (main_filtered[main_custname_col] == combined_secondary[\"CustName\"]) &\n",
    "                    (main_filtered[main_current_period_col] == combined_secondary[\"UFR_Date\"]) &\n",
    "                    (main_filtered[main_period_col] == combined_secondary[\"Period_Source\"])\n",
    "                )\n",
    "            else:\n",
    "                # Join without period matching\n",
    "                join_condition = (\n",
    "                    (main_filtered[main_amount_col] == combined_secondary[\"Amount\"]) & \n",
    "                    (main_filtered[main_custname_col] == combined_secondary[\"CustName\"]) &\n",
    "                    (main_filtered[main_current_period_col] == combined_secondary[\"UFR_Date\"])\n",
    "                )\n",
    "            \n",
    "            print(\"Performing left outer join with multi-period data...\")\n",
    "            joined_df = main_filtered.join(combined_secondary, join_condition, \"left\")\n",
    "            \n",
    "            # Build result with updated UFR values\n",
    "            select_columns = []\n",
    "            \n",
    "            # Add all main table columns except UFR columns\n",
    "            for c in main_filtered.columns:\n",
    "                if c not in [main_ufr_amount_col, main_ufr_date_col, main_ufr_tag_col]:\n",
    "                    select_columns.append(col(c))\n",
    "            \n",
    "            # Handle UFR Amount (secondary takes precedence)\n",
    "            ufr_amount_col = coalesce(\n",
    "                col(\"UFR_Amount\"),  # From secondary\n",
    "                col(main_ufr_amount_col)  # From main\n",
    "            ).alias(main_ufr_amount_col)\n",
    "            select_columns.append(ufr_amount_col)\n",
    "            \n",
    "            # Handle UFR Date (secondary takes precedence)\n",
    "            ufr_date_col = coalesce(\n",
    "                col(\"UFR_Date\"),  # From secondary\n",
    "                col(main_ufr_date_col)  # From main\n",
    "            ).alias(main_ufr_date_col)\n",
    "            select_columns.append(ufr_date_col)\n",
    "            \n",
    "            # Handle UFR Tag (set to \"UFR\" if UFR Date exists, otherwise \"Not UFR\")\n",
    "            ufr_tag_col = when(\n",
    "                coalesce(col(\"UFR_Date\"), col(main_ufr_date_col)).is_not_null(),\n",
    "                lit(\"UFR\")\n",
    "            ).otherwise(lit(\"Not UFR\")).alias(main_ufr_tag_col)\n",
    "            select_columns.append(ufr_tag_col)\n",
    "            \n",
    "            joined_result = joined_df.select(*select_columns)\n",
    "            \n",
    "            # Merge back with original main table to maintain all rows\n",
    "            print(\"Merging results back with original main table...\")\n",
    "            \n",
    "            # Create unique keys for joining back\n",
    "            main_with_key = main_clean.with_column(\n",
    "                \"temp_join_key\",\n",
    "                concat(\n",
    "                    coalesce(col(main_amount_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_custname_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_current_period_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_period_col), lit(\"NULL\")) if main_period_col else lit(\"NULL\")\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            joined_with_key = joined_result.with_column(\n",
    "                \"temp_join_key\",\n",
    "                concat(\n",
    "                    coalesce(col(main_amount_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_custname_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_current_period_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_period_col), lit(\"NULL\")) if main_period_col else lit(\"NULL\")\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Final merge\n",
    "            final_result = main_with_key.join(\n",
    "                joined_with_key.select(\n",
    "                    col(\"temp_join_key\").alias(\"joined_key\"),\n",
    "                    col(main_ufr_amount_col).alias(\"new_UFR_Amount\"),\n",
    "                    col(main_ufr_date_col).alias(\"new_UFR_Date\"),\n",
    "                    col(main_ufr_tag_col).alias(\"new_UFR_Tag\")\n",
    "                ),\n",
    "                main_with_key[\"temp_join_key\"] == col(\"joined_key\"),\n",
    "                \"left\"\n",
    "            )\n",
    "            \n",
    "            # Build final select with all original columns\n",
    "            final_select_cols = []\n",
    "            for c in main_clean.columns:\n",
    "                if c == main_ufr_amount_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Amount\"), col(c)).alias(c))\n",
    "                elif c == main_ufr_date_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Date\"), col(c)).alias(c))\n",
    "                elif c == main_ufr_tag_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Tag\"), col(c), lit(\"Not UFR\")).alias(c))\n",
    "                else:\n",
    "                    final_select_cols.append(col(c))\n",
    "            \n",
    "            result_df = final_result.select(*final_select_cols)\n",
    "            \n",
    "            # Show summary statistics\n",
    "            total_original_rows = main_df_original.count()\n",
    "            total_result_rows = result_df.count()\n",
    "            \n",
    "            print(f\"Original main table rows: {total_original_rows}\")\n",
    "            print(f\"Final result rows: {total_result_rows}\")\n",
    "            \n",
    "            if total_result_rows == total_original_rows:\n",
    "                print(\"✓ Successfully maintained original row count\")\n",
    "            else:\n",
    "                print(\"⚠ Warning: Row count changed!\")\n",
    "            \n",
    "            # Show UFR statistics by period\n",
    "            print(f\"\\nProcessed periods: {valid_periods}\")\n",
    "            print(f\"Period mappings: {[period_mapping.get(p) for p in valid_periods]}\")\n",
    "            \n",
    "            # Show UFR Tag summary\n",
    "            try:\n",
    "                result_df.group_by(main_ufr_tag_col).count().order_by(main_ufr_tag_col).show()\n",
    "            except:\n",
    "                print(\"Could not display UFR Tag summary\")\n",
    "            \n",
    "            return result_df, main_period_col  # Return main_period_col for use in main function\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in multi-period join operation: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None, None\n",
    "    \n",
    "    def UFR_Excel_C_main(session):\n",
    "        \"\"\"Main function to execute the multi-period join process\"\"\"\n",
    "        print(\"=*=\"*25)\n",
    "        print(\"Starting multi-period UFR join process...\")\n",
    "        print(f\"Lookback periods: {lookback_list_UFR}\")\n",
    "        print(f\"Period mappings: {[period_mapping.get(p, 'Unknown') for p in lookback_list_UFR]}\")\n",
    "        print(\"Join conditions: Amount = Amount AND CustName = CustName AND Current Period = UFR Date\")\n",
    "        \n",
    "        result, main_period_col = perform_multi_period_join_Excel_C(session, lookback_list_UFR)\n",
    "        \n",
    "        if main_period_col:\n",
    "            print(\"Additional condition: Period = Period_Source (filtered by period mappings)\")\n",
    "        \n",
    "        if result is not None:\n",
    "            print(\"Multi-period join completed successfully!\")\n",
    "            \n",
    "            # Save result\n",
    "            result.write.mode(\"overwrite\").save_as_table(output_Excel_C)\n",
    "            print(\"=\"*100)\n",
    "            print(f\"___Saving on path: {output_Excel_C}\")\n",
    "            print(\"=\"*100)\n",
    "            \n",
    "            # Show final statistics\n",
    "            row_count = result.count()\n",
    "            print(f\"Total rows in output table: {row_count}\")\n",
    "            \n",
    "            # Show UFR Tag summary\n",
    "            print(\"Final UFR Tag summary:\")\n",
    "            try:\n",
    "                # Find the UFR Tag column name\n",
    "                main_df_original = session.table(main_table_name_Excel_C)\n",
    "                main_ufr_tag_col = find_column_name_Excel_C(main_df_original, 'UFR Tag')\n",
    "                if not main_ufr_tag_col:\n",
    "                    main_ufr_tag_col = '\"UFR Tag\"'\n",
    "                result.group_by(main_ufr_tag_col).count().order_by(main_ufr_tag_col).show()\n",
    "            except:\n",
    "                print(\"Could not display final UFR Tag summary\")\n",
    "        else:\n",
    "            print(\"Multi-period join process failed\")\n",
    "            print(\"=\"*100)\n",
    "    \n",
    "    UFR_Excel_C_main(session)\n",
    "\n",
    "# Example usage:\n",
    "# UFR_Excel_C_final(session, [1, 3])      # For MOM and QOQ\n",
    "# UFR_Excel_C_final(session, [1, 3, 12])  # For MOM, QOQ, and YOY\n",
    "# UFR_Excel_C_final(session, [12])        # For YOY only\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "# lookback_list_UFR = [1, 3, 12]  # Multiple periods: MOM, QOQ, YOY\n",
    "# excel_retention_output_path = \"NEWFOLD_TEST_DB.SOURCE.MRR_BLOWOUT_10K_SM22JUNE_EXCEL_MASTER\"\n",
    "# input_file_path = \"NEWFOLD_TEST_DB.SOURCE.MRR_BLOWOUT_10K\"\n",
    "\n",
    "# Common columns for join and update (with proper column names) - UPDATED to include Product\n",
    "join_columns = ['Amount', 'CustName', 'Current Period', 'Product']\n",
    "update_columns = ['UFR Amount', 'UFR Date']\n",
    "\n",
    "# Period mapping based on lookback values\n",
    "period_mapping = {\n",
    "    1: \"Month\",\n",
    "    3: \"Quarter\", \n",
    "    12: \"Year\"\n",
    "}\n",
    "\n",
    "def UFR_Excel_CP_final(session, lookback_list_UFR):\n",
    "    \n",
    "    # Define table names\n",
    "    main_table_name_Excel_CP = f\"{excel_retention_output_path}_CP\"\n",
    "    output_Excel_CP = main_table_name_Excel_CP + \"_UFR\"\n",
    "    secondary_tables_Excel_CP = {\n",
    "        1: f\"{input_file_path}_CP_MOM\",   # Month\n",
    "        3: f\"{input_file_path}_CP_QOQ\",   # Quarter\n",
    "        12: f\"{input_file_path}_CP_YOY\"   # Year\n",
    "    }\n",
    "    \n",
    "    def clean_numeric_column_Excel_CP(df, col_name):\n",
    "        \"\"\"Clean numeric column by converting empty strings and whitespace to NULL\"\"\"\n",
    "        return when(\n",
    "            (col(col_name).is_null()) | \n",
    "            (trim(col(col_name)) == \"\") | \n",
    "            (length(trim(col(col_name))) == 0),\n",
    "            lit(None)\n",
    "        ).otherwise(col(col_name))\n",
    "    \n",
    "    def clean_date_column_Excel_CP(df, col_name):\n",
    "        \"\"\"Clean date column by converting empty strings and whitespace to NULL\"\"\"\n",
    "        return when(\n",
    "            (col(col_name).is_null()) | \n",
    "            (trim(col(col_name)) == \"\") | \n",
    "            (length(trim(col(col_name))) == 0),\n",
    "            lit(None)\n",
    "        ).otherwise(col(col_name))\n",
    "    \n",
    "    def find_column_name_Excel_CP(df, target_col):\n",
    "        \"\"\"Find the actual column name that matches the target (case insensitive, handles quotes)\"\"\"\n",
    "        target_clean = target_col.upper().replace(' ', '').replace('_', '').replace('\"', '')\n",
    "        for col_name in df.columns:\n",
    "            col_clean = col_name.upper().replace(' ', '').replace('_', '').replace('\"', '')\n",
    "            if col_clean == target_clean:\n",
    "                return col_name\n",
    "        return None\n",
    "    \n",
    "    def prepare_secondary_data_for_period_Excel_CP(session, lookback_period):\n",
    "        \"\"\"Prepare secondary table data for a specific period\"\"\"\n",
    "        try:\n",
    "            if lookback_period not in secondary_tables_Excel_CP:\n",
    "                print(f\"Invalid lookback period: {lookback_period}\")\n",
    "                return None\n",
    "                \n",
    "            secondary_table_name = secondary_tables_Excel_CP[lookback_period]\n",
    "            period_filter_value = period_mapping.get(lookback_period)\n",
    "            \n",
    "            print(f\"Processing secondary table: {secondary_table_name} for period: {period_filter_value}\")\n",
    "            \n",
    "            # Load secondary table\n",
    "            try:\n",
    "                secondary_df = session.table(secondary_table_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Secondary table {secondary_table_name} not available: {str(e)}\")\n",
    "                return None\n",
    "            \n",
    "            # Find column names in secondary table - UPDATED to include Product\n",
    "            sec_amount_col = find_column_name_Excel_CP(secondary_df, 'Amount')\n",
    "            sec_custname_col = find_column_name_Excel_CP(secondary_df, 'CustName')\n",
    "            sec_product_col = find_column_name_Excel_CP(secondary_df, 'Product')\n",
    "            sec_ufr_amount_col = find_column_name_Excel_CP(secondary_df, 'UFR Amount')\n",
    "            sec_ufr_date_col = find_column_name_Excel_CP(secondary_df, 'UFR Date')\n",
    "            \n",
    "            if not all([sec_amount_col, sec_custname_col, sec_product_col, sec_ufr_date_col]):\n",
    "                print(f\"Required columns not found in secondary table {secondary_table_name}\")\n",
    "                return None\n",
    "            \n",
    "            # Clean and prepare secondary data with period identifier\n",
    "            secondary_clean = secondary_df.select(\n",
    "                clean_numeric_column_Excel_CP(secondary_df, sec_amount_col).alias(\"Amount\"),\n",
    "                col(sec_custname_col).alias(\"CustName\"),\n",
    "                col(sec_product_col).alias(\"Product\"),\n",
    "                clean_date_column_Excel_CP(secondary_df, sec_ufr_date_col).alias(\"UFR_Date\"),\n",
    "                clean_numeric_column_Excel_CP(secondary_df, sec_ufr_amount_col).alias(\"UFR_Amount\") if sec_ufr_amount_col else lit(None).alias(\"UFR_Amount\"),\n",
    "                lit(period_filter_value).alias(\"Period_Source\"),  # Track which period this data came from\n",
    "                lit(lookback_period).alias(\"Lookback_Period\")     # Track lookback period\n",
    "            ).filter(\n",
    "                col(\"Amount\").is_not_null() & \n",
    "                col(\"CustName\").is_not_null() &\n",
    "                col(\"Product\").is_not_null() &\n",
    "                col(\"UFR_Date\").is_not_null() &\n",
    "                (trim(col(\"CustName\")) != \"\") &\n",
    "                (trim(col(\"Product\")) != \"\")\n",
    "            )\n",
    "            \n",
    "            return secondary_clean\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing secondary data for period {lookback_period}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def perform_multi_period_join_Excel_CP(session, lookback_list_UFR):\n",
    "        \"\"\"Perform joins for multiple periods and combine results\"\"\"\n",
    "        try:\n",
    "            # Load main table\n",
    "            main_df_original = session.table(main_table_name_Excel_CP)\n",
    "            print(f\"Main table loaded successfully with {main_df_original.count()} rows\")\n",
    "            \n",
    "            # Find column names in main table - UPDATED to include Product\n",
    "            main_amount_col = find_column_name_Excel_CP(main_df_original, 'Amount')\n",
    "            main_custname_col = find_column_name_Excel_CP(main_df_original, 'CustName')\n",
    "            main_product_col = find_column_name_Excel_CP(main_df_original, 'Product')\n",
    "            main_current_period_col = find_column_name_Excel_CP(main_df_original, 'Current Period')\n",
    "            main_period_col = find_column_name_Excel_CP(main_df_original, 'Period')\n",
    "            main_ufr_amount_col = find_column_name_Excel_CP(main_df_original, 'UFR Amount')\n",
    "            main_ufr_date_col = find_column_name_Excel_CP(main_df_original, 'UFR Date')\n",
    "            main_ufr_tag_col = find_column_name_Excel_CP(main_df_original, 'UFR Tag')\n",
    "            \n",
    "            if not all([main_amount_col, main_custname_col, main_product_col, main_current_period_col]):\n",
    "                print(\"Required columns not found in main table\")\n",
    "                return None\n",
    "            \n",
    "            # Clean main table\n",
    "            main_select_cols = []\n",
    "            for c in main_df_original.columns:\n",
    "                if c == main_amount_col:\n",
    "                    main_select_cols.append(clean_numeric_column_Excel_CP(main_df_original, c).alias(c))\n",
    "                elif c == main_ufr_amount_col:\n",
    "                    main_select_cols.append(clean_numeric_column_Excel_CP(main_df_original, c).alias(c))\n",
    "                elif c == main_ufr_date_col:\n",
    "                    main_select_cols.append(clean_date_column_Excel_CP(main_df_original, c).alias(c))\n",
    "                elif c == main_current_period_col:\n",
    "                    main_select_cols.append(clean_date_column_Excel_CP(main_df_original, c).alias(c))\n",
    "                else:\n",
    "                    main_select_cols.append(col(c))\n",
    "            \n",
    "            # Add missing UFR columns if they don't exist\n",
    "            if not main_ufr_amount_col:\n",
    "                main_select_cols.append(lit(None).cast(DecimalType()).alias('\"UFR Amount\"'))\n",
    "                main_ufr_amount_col = '\"UFR Amount\"'\n",
    "                print(\"Added UFR Amount column\")\n",
    "                \n",
    "            if not main_ufr_date_col:\n",
    "                main_select_cols.append(lit(None).cast(DateType()).alias('\"UFR Date\"'))\n",
    "                main_ufr_date_col = '\"UFR Date\"'\n",
    "                print(\"Added UFR Date column\")\n",
    "                \n",
    "            if not main_ufr_tag_col:\n",
    "                main_select_cols.append(lit(\"Not UFR\").cast(StringType()).alias('\"UFR Tag\"'))\n",
    "                main_ufr_tag_col = '\"UFR Tag\"'\n",
    "                print(\"Added UFR Tag column\")\n",
    "            \n",
    "            main_clean = main_df_original.select(*main_select_cols)\n",
    "            \n",
    "            # Prepare all secondary data for all periods\n",
    "            all_secondary_data = []\n",
    "            valid_periods = []\n",
    "            \n",
    "            for period in lookback_list_UFR:\n",
    "                secondary_data = prepare_secondary_data_for_period_Excel_CP(session, period)\n",
    "                if secondary_data is not None:\n",
    "                    all_secondary_data.append(secondary_data)\n",
    "                    valid_periods.append(period)\n",
    "                    print(f\"Successfully prepared data for period {period}\")\n",
    "                else:\n",
    "                    print(f\"Skipping period {period} due to data preparation failure\")\n",
    "            \n",
    "            if not all_secondary_data:\n",
    "                print(\"No valid secondary data found for any period\")\n",
    "                return main_clean\n",
    "            \n",
    "            # Union all secondary data\n",
    "            print(f\"Combining secondary data from {len(all_secondary_data)} periods\")\n",
    "            combined_secondary = all_secondary_data[0]\n",
    "            for i in range(1, len(all_secondary_data)):\n",
    "                combined_secondary = combined_secondary.union(all_secondary_data[i])\n",
    "            \n",
    "            print(f\"Combined secondary data has {combined_secondary.count()} rows\")\n",
    "            \n",
    "            # Create mapping of periods to filter main table (if Period column exists)\n",
    "            period_filter_values = [period_mapping.get(p) for p in valid_periods]\n",
    "            print(f\"Period mappings for filtering: {period_filter_values}\")\n",
    "            \n",
    "            # Filter main table based on Period column if it exists\n",
    "            if main_period_col:\n",
    "                print(f\"Filtering main table rows with Period in: {period_filter_values}\")\n",
    "                main_filtered = main_clean.filter(\n",
    "                    col(main_period_col).isin(period_filter_values) &\n",
    "                    col(main_amount_col).is_not_null() & \n",
    "                    col(main_custname_col).is_not_null() &\n",
    "                    col(main_product_col).is_not_null() &\n",
    "                    col(main_current_period_col).is_not_null() &\n",
    "                    (trim(col(main_custname_col)) != \"\") &\n",
    "                    (trim(col(main_product_col)) != \"\")\n",
    "                )\n",
    "            else:\n",
    "                print(\"No Period column found in main table, processing all rows\")\n",
    "                main_filtered = main_clean.filter(\n",
    "                    col(main_amount_col).is_not_null() & \n",
    "                    col(main_custname_col).is_not_null() &\n",
    "                    col(main_product_col).is_not_null() &\n",
    "                    col(main_current_period_col).is_not_null() &\n",
    "                    (trim(col(main_custname_col)) != \"\") &\n",
    "                    (trim(col(main_product_col)) != \"\")\n",
    "                )\n",
    "            \n",
    "            print(f\"Main table rows after filtering: {main_filtered.count()}\")\n",
    "            \n",
    "            # Perform join with combined secondary data - UPDATED to include Product\n",
    "            if main_period_col:\n",
    "                # Join with period matching\n",
    "                join_condition = (\n",
    "                    (main_filtered[main_amount_col] == combined_secondary[\"Amount\"]) & \n",
    "                    (main_filtered[main_custname_col] == combined_secondary[\"CustName\"]) &\n",
    "                    (main_filtered[main_product_col] == combined_secondary[\"Product\"]) &\n",
    "                    (main_filtered[main_current_period_col] == combined_secondary[\"UFR_Date\"]) &\n",
    "                    (main_filtered[main_period_col] == combined_secondary[\"Period_Source\"])\n",
    "                )\n",
    "            else:\n",
    "                # Join without period matching\n",
    "                join_condition = (\n",
    "                    (main_filtered[main_amount_col] == combined_secondary[\"Amount\"]) & \n",
    "                    (main_filtered[main_custname_col] == combined_secondary[\"CustName\"]) &\n",
    "                    (main_filtered[main_product_col] == combined_secondary[\"Product\"]) &\n",
    "                    (main_filtered[main_current_period_col] == combined_secondary[\"UFR_Date\"])\n",
    "                )\n",
    "            \n",
    "            print(\"Performing left outer join with multi-period data...\")\n",
    "            joined_df = main_filtered.join(combined_secondary, join_condition, \"left\")\n",
    "            \n",
    "            # Build result with updated UFR values\n",
    "            select_columns = []\n",
    "            \n",
    "            # Add all main table columns except UFR columns\n",
    "            for c in main_filtered.columns:\n",
    "                if c not in [main_ufr_amount_col, main_ufr_date_col, main_ufr_tag_col]:\n",
    "                    select_columns.append(col(c))\n",
    "            \n",
    "            # Handle UFR Amount (secondary takes precedence)\n",
    "            ufr_amount_col = coalesce(\n",
    "                col(\"UFR_Amount\"),  # From secondary\n",
    "                col(main_ufr_amount_col)  # From main\n",
    "            ).alias(main_ufr_amount_col)\n",
    "            select_columns.append(ufr_amount_col)\n",
    "            \n",
    "            # Handle UFR Date (secondary takes precedence)\n",
    "            ufr_date_col = coalesce(\n",
    "                col(\"UFR_Date\"),  # From secondary\n",
    "                col(main_ufr_date_col)  # From main\n",
    "            ).alias(main_ufr_date_col)\n",
    "            select_columns.append(ufr_date_col)\n",
    "            \n",
    "            # Handle UFR Tag (set to \"UFR\" if UFR Date exists, otherwise \"Not UFR\")\n",
    "            ufr_tag_col = when(\n",
    "                coalesce(col(\"UFR_Date\"), col(main_ufr_date_col)).is_not_null(),\n",
    "                lit(\"UFR\")\n",
    "            ).otherwise(lit(\"Not UFR\")).alias(main_ufr_tag_col)\n",
    "            select_columns.append(ufr_tag_col)\n",
    "            \n",
    "            joined_result = joined_df.select(*select_columns)\n",
    "            \n",
    "            # Merge back with original main table to maintain all rows\n",
    "            print(\"Merging results back with original main table...\")\n",
    "            \n",
    "            # Create unique keys for joining back - UPDATED to include Product\n",
    "            main_with_key = main_clean.with_column(\n",
    "                \"temp_join_key\",\n",
    "                concat(\n",
    "                    coalesce(col(main_amount_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_custname_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_product_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_current_period_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_period_col), lit(\"NULL\")) if main_period_col else lit(\"NULL\")\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            joined_with_key = joined_result.with_column(\n",
    "                \"temp_join_key\",\n",
    "                concat(\n",
    "                    coalesce(col(main_amount_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_custname_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_product_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_current_period_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_period_col), lit(\"NULL\")) if main_period_col else lit(\"NULL\")\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Final merge\n",
    "            final_result = main_with_key.join(\n",
    "                joined_with_key.select(\n",
    "                    col(\"temp_join_key\").alias(\"joined_key\"),\n",
    "                    col(main_ufr_amount_col).alias(\"new_UFR_Amount\"),\n",
    "                    col(main_ufr_date_col).alias(\"new_UFR_Date\"),\n",
    "                    col(main_ufr_tag_col).alias(\"new_UFR_Tag\")\n",
    "                ),\n",
    "                main_with_key[\"temp_join_key\"] == col(\"joined_key\"),\n",
    "                \"left\"\n",
    "            )\n",
    "            \n",
    "            # Build final select with all original columns\n",
    "            final_select_cols = []\n",
    "            for c in main_clean.columns:\n",
    "                if c == main_ufr_amount_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Amount\"), col(c)).alias(c))\n",
    "                elif c == main_ufr_date_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Date\"), col(c)).alias(c))\n",
    "                elif c == main_ufr_tag_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Tag\"), col(c), lit(\"Not UFR\")).alias(c))\n",
    "                else:\n",
    "                    final_select_cols.append(col(c))\n",
    "            \n",
    "            result_df = final_result.select(*final_select_cols)\n",
    "            \n",
    "            # Show summary statistics\n",
    "            total_original_rows = main_df_original.count()\n",
    "            total_result_rows = result_df.count()\n",
    "            \n",
    "            print(f\"Original main table rows: {total_original_rows}\")\n",
    "            print(f\"Final result rows: {total_result_rows}\")\n",
    "            \n",
    "            if total_result_rows == total_original_rows:\n",
    "                print(\"✓ Successfully maintained original row count\")\n",
    "            else:\n",
    "                print(\"⚠ Warning: Row count changed!\")\n",
    "            \n",
    "            # Show UFR statistics by period\n",
    "            print(f\"\\nProcessed periods: {valid_periods}\")\n",
    "            print(f\"Period mappings: {[period_mapping.get(p) for p in valid_periods]}\")\n",
    "            \n",
    "            # Show UFR Tag summary\n",
    "            try:\n",
    "                result_df.group_by(main_ufr_tag_col).count().order_by(main_ufr_tag_col).show()\n",
    "            except:\n",
    "                print(\"Could not display UFR Tag summary\")\n",
    "            \n",
    "            return result_df, main_period_col  # Return main_period_col for use in main function\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in multi-period join operation: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None, None\n",
    "    \n",
    "    def UFR_Excel_CP_main(session):\n",
    "        \"\"\"Main function to execute the multi-period join process\"\"\"\n",
    "        print(\"=*=\"*25)\n",
    "        print(\"Starting multi-period UFR join process...\")\n",
    "        print(f\"Lookback periods: {lookback_list_UFR}\")\n",
    "        print(f\"Period mappings: {[period_mapping.get(p, 'Unknown') for p in lookback_list_UFR]}\")\n",
    "        print(\"Join conditions: Amount = Amount AND CustName = CustName AND Product = Product AND Current Period = UFR Date\")\n",
    "        \n",
    "        result, main_period_col = perform_multi_period_join_Excel_CP(session, lookback_list_UFR)\n",
    "        \n",
    "        if main_period_col:\n",
    "            print(\"Additional condition: Period = Period_Source (filtered by period mappings)\")\n",
    "        \n",
    "        if result is not None:\n",
    "            print(\"Multi-period join completed successfully!\")\n",
    "            \n",
    "            # Save result\n",
    "            result.write.mode(\"overwrite\").save_as_table(output_Excel_CP)\n",
    "            print(\"=\"*100)\n",
    "            print(f\"___Saving on path: {output_Excel_CP}\")\n",
    "            print(\"=\"*100)\n",
    "            \n",
    "            # Show final statistics\n",
    "            row_count = result.count()\n",
    "            print(f\"Total rows in output table: {row_count}\")\n",
    "            \n",
    "            # Show UFR Tag summary\n",
    "            print(\"Final UFR Tag summary:\")\n",
    "            try:\n",
    "                # Find the UFR Tag column name\n",
    "                main_df_original = session.table(main_table_name_Excel_CP)\n",
    "                main_ufr_tag_col = find_column_name_Excel_CP(main_df_original, 'UFR Tag')\n",
    "                if not main_ufr_tag_col:\n",
    "                    main_ufr_tag_col = '\"UFR Tag\"'\n",
    "                result.group_by(main_ufr_tag_col).count().order_by(main_ufr_tag_col).show()\n",
    "            except:\n",
    "                print(\"Could not display final UFR Tag summary\")\n",
    "        else:\n",
    "            print(\"Multi-period join process failed\")\n",
    "            print(\"=\"*100)\n",
    "    \n",
    "    UFR_Excel_CP_main(session)\n",
    "\n",
    "# Example usage:\n",
    "# UFR_Excel_CP_final(session, [1, 3])      # For MOM and QOQ\n",
    "# UFR_Excel_CP_final(session, [1, 3, 12])  # For MOM, QOQ, and YOY\n",
    "# UFR_Excel_CP_final(session, [12])        # For YOY only\n",
    "\n",
    "\n",
    "\n",
    "# Enhanced Configuration\n",
    "# lookback_list_UFR = [1, 3, 12]  # Multiple periods: MOM, QOQ, YOY\n",
    "# excel_retention_output_path = \"NEWFOLD_TEST_DB.SOURCE.MRR_BLOWOUT_10K_SM22JUNE_EXCEL_MASTER\"\n",
    "# input_file_path = \"NEWFOLD_TEST_DB.SOURCE.MRR_BLOWOUT_10K\"\n",
    "\n",
    "# Common columns for join and update (Enhanced with Product and Revenue Type)\n",
    "join_columns = ['Amount', 'CustName', 'Current Period', 'Product', 'Revenue Type']\n",
    "update_columns = ['UFR Amount', 'UFR Date']\n",
    "\n",
    "# Period mapping based on lookback values\n",
    "period_mapping = {\n",
    "    1: \"Month\",\n",
    "    3: \"Quarter\", \n",
    "    12: \"Year\"\n",
    "}\n",
    "\n",
    "def UFR_Excel_CPR_final(session, lookback_list_UFR):\n",
    "    \n",
    "    # Define table names\n",
    "    main_table_name_Excel_CPR = f\"{excel_retention_output_path}_CPR\"\n",
    "    output_Excel_CPR = main_table_name_Excel_CPR + \"_UFR\"\n",
    "    secondary_tables_Excel_CPR = {\n",
    "        1: f\"{input_file_path}_CPR_MOM\",   # Month\n",
    "        3: f\"{input_file_path}_CPR_QOQ\",   # Quarter\n",
    "        12: f\"{input_file_path}_CPR_YOY\"   # Year\n",
    "    }\n",
    "    \n",
    "    def clean_numeric_column_Excel_CPR(df, col_name):\n",
    "        \"\"\"Clean numeric column by converting empty strings and whitespace to NULL\"\"\"\n",
    "        return when(\n",
    "            (col(col_name).is_null()) | \n",
    "            (trim(col(col_name)) == \"\") | \n",
    "            (length(trim(col(col_name))) == 0),\n",
    "            lit(None)\n",
    "        ).otherwise(col(col_name))\n",
    "    \n",
    "    def clean_date_column_Excel_CPR(df, col_name):\n",
    "        \"\"\"Clean date column by converting empty strings and whitespace to NULL\"\"\"\n",
    "        return when(\n",
    "            (col(col_name).is_null()) | \n",
    "            (trim(col(col_name)) == \"\") | \n",
    "            (length(trim(col(col_name))) == 0),\n",
    "            lit(None)\n",
    "        ).otherwise(col(col_name))\n",
    "    \n",
    "    def clean_string_column_Excel_CPR(df, col_name):\n",
    "        \"\"\"Clean string column by converting empty strings and whitespace to NULL\"\"\"\n",
    "        return when(\n",
    "            (col(col_name).is_null()) | \n",
    "            (trim(col(col_name)) == \"\") | \n",
    "            (length(trim(col(col_name))) == 0),\n",
    "            lit(None)\n",
    "        ).otherwise(trim(col(col_name)))\n",
    "    \n",
    "    def find_column_name_Excel_CPR(df, target_col):\n",
    "        \"\"\"Find the actual column name that matches the target (case insensitive, handles quotes)\"\"\"\n",
    "        target_clean = target_col.upper().replace(' ', '').replace('_', '').replace('\"', '')\n",
    "        for col_name in df.columns:\n",
    "            col_clean = col_name.upper().replace(' ', '').replace('_', '').replace('\"', '')\n",
    "            if col_clean == target_clean:\n",
    "                return col_name\n",
    "        return None\n",
    "    \n",
    "    def prepare_secondary_data_for_period_Excel_CPR(session, lookback_period):\n",
    "        \"\"\"Prepare secondary table data for a specific period with enhanced join conditions\"\"\"\n",
    "        try:\n",
    "            if lookback_period not in secondary_tables_Excel_CPR:\n",
    "                print(f\"Invalid lookback period: {lookback_period}\")\n",
    "                return None\n",
    "                \n",
    "            secondary_table_name = secondary_tables_Excel_CPR[lookback_period]\n",
    "            period_filter_value = period_mapping.get(lookback_period)\n",
    "            \n",
    "            print(f\"Processing secondary table: {secondary_table_name} for period: {period_filter_value}\")\n",
    "            \n",
    "            # Load secondary table\n",
    "            try:\n",
    "                secondary_df = session.table(secondary_table_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Secondary table {secondary_table_name} not available: {str(e)}\")\n",
    "                return None\n",
    "            \n",
    "            # Find column names in secondary table (Enhanced with Product and Revenue Type)\n",
    "            sec_amount_col = find_column_name_Excel_CPR(secondary_df, 'Amount')\n",
    "            sec_custname_col = find_column_name_Excel_CPR(secondary_df, 'CustName')\n",
    "            sec_product_col = find_column_name_Excel_CPR(secondary_df, 'Product')\n",
    "            sec_revenue_type_col = find_column_name_Excel_CPR(secondary_df, 'Revenue Type')\n",
    "            sec_ufr_amount_col = find_column_name_Excel_CPR(secondary_df, 'UFR Amount')\n",
    "            sec_ufr_date_col = find_column_name_Excel_CPR(secondary_df, 'UFR Date')\n",
    "            \n",
    "            if not all([sec_amount_col, sec_custname_col, sec_product_col, sec_revenue_type_col, sec_ufr_date_col]):\n",
    "                print(f\"Required columns not found in secondary table {secondary_table_name}\")\n",
    "                print(f\"Missing: Amount:{sec_amount_col}, CustName:{sec_custname_col}, Product:{sec_product_col}, Revenue Type:{sec_revenue_type_col}, UFR Date:{sec_ufr_date_col}\")\n",
    "                return None\n",
    "            \n",
    "            # Clean and prepare secondary data with period identifier and enhanced columns\n",
    "            secondary_clean = secondary_df.select(\n",
    "                clean_numeric_column_Excel_CPR(secondary_df, sec_amount_col).alias(\"Amount\"),\n",
    "                clean_string_column_Excel_CPR(secondary_df, sec_custname_col).alias(\"CustName\"),\n",
    "                clean_string_column_Excel_CPR(secondary_df, sec_product_col).alias(\"Product\"),\n",
    "                clean_string_column_Excel_CPR(secondary_df, sec_revenue_type_col).alias(\"Revenue_Type\"),\n",
    "                clean_date_column_Excel_CPR(secondary_df, sec_ufr_date_col).alias(\"UFR_Date\"),\n",
    "                clean_numeric_column_Excel_CPR(secondary_df, sec_ufr_amount_col).alias(\"UFR_Amount\") if sec_ufr_amount_col else lit(None).alias(\"UFR_Amount\"),\n",
    "                lit(period_filter_value).alias(\"Period_Source\"),  # Track which period this data came from\n",
    "                lit(lookback_period).alias(\"Lookback_Period\")     # Track lookback period\n",
    "            ).filter(\n",
    "                col(\"Amount\").is_not_null() & \n",
    "                col(\"CustName\").is_not_null() &\n",
    "                col(\"Product\").is_not_null() &\n",
    "                col(\"Revenue_Type\").is_not_null() &\n",
    "                col(\"UFR_Date\").is_not_null() &\n",
    "                (trim(col(\"CustName\")) != \"\") &\n",
    "                (trim(col(\"Product\")) != \"\") &\n",
    "                (trim(col(\"Revenue_Type\")) != \"\")\n",
    "            )\n",
    "            \n",
    "            return secondary_clean\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing secondary data for period {lookback_period}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def perform_multi_period_join_Excel_CPR(session, lookback_list_UFR):\n",
    "        \"\"\"Perform joins for multiple periods and combine results with enhanced join conditions\"\"\"\n",
    "        try:\n",
    "            # Load main table\n",
    "            main_df_original = session.table(main_table_name_Excel_CPR)\n",
    "            print(f\"Main table loaded successfully with {main_df_original.count()} rows\")\n",
    "            \n",
    "            # Find column names in main table (Enhanced with Product and Revenue Type)\n",
    "            main_amount_col = find_column_name_Excel_CPR(main_df_original, 'Amount')\n",
    "            main_custname_col = find_column_name_Excel_CPR(main_df_original, 'CustName')\n",
    "            main_current_period_col = find_column_name_Excel_CPR(main_df_original, 'Current Period')\n",
    "            main_product_col = find_column_name_Excel_CPR(main_df_original, 'Product')\n",
    "            main_revenue_type_col = find_column_name_Excel_CPR(main_df_original, 'Revenue Type')\n",
    "            main_period_col = find_column_name_Excel_CPR(main_df_original, 'Period')\n",
    "            main_ufr_amount_col = find_column_name_Excel_CPR(main_df_original, 'UFR Amount')\n",
    "            main_ufr_date_col = find_column_name_Excel_CPR(main_df_original, 'UFR Date')\n",
    "            main_ufr_tag_col = find_column_name_Excel_CPR(main_df_original, 'UFR Tag')\n",
    "            \n",
    "            if not all([main_amount_col, main_custname_col, main_current_period_col, main_product_col, main_revenue_type_col]):\n",
    "                print(\"Required columns not found in main table\")\n",
    "                print(f\"Missing: Amount:{main_amount_col}, CustName:{main_custname_col}, Current Period:{main_current_period_col}\")\n",
    "                print(f\"Missing: Product:{main_product_col}, Revenue Type:{main_revenue_type_col}\")\n",
    "                return None\n",
    "            \n",
    "            # Clean main table with enhanced columns\n",
    "            main_select_cols = []\n",
    "            for c in main_df_original.columns:\n",
    "                if c == main_amount_col:\n",
    "                    main_select_cols.append(clean_numeric_column_Excel_CPR(main_df_original, c).alias(c))\n",
    "                elif c == main_ufr_amount_col:\n",
    "                    main_select_cols.append(clean_numeric_column_Excel_CPR(main_df_original, c).alias(c))\n",
    "                elif c == main_ufr_date_col:\n",
    "                    main_select_cols.append(clean_date_column_Excel_CPR(main_df_original, c).alias(c))\n",
    "                elif c == main_current_period_col:\n",
    "                    main_select_cols.append(clean_date_column_Excel_CPR(main_df_original, c).alias(c))\n",
    "                elif c in [main_custname_col, main_product_col, main_revenue_type_col]:\n",
    "                    main_select_cols.append(clean_string_column_Excel_CPR(main_df_original, c).alias(c))\n",
    "                else:\n",
    "                    main_select_cols.append(col(c))\n",
    "            \n",
    "            # Add missing UFR columns if they don't exist\n",
    "            if not main_ufr_amount_col:\n",
    "                main_select_cols.append(lit(None).cast(DecimalType()).alias('\"UFR Amount\"'))\n",
    "                main_ufr_amount_col = '\"UFR Amount\"'\n",
    "                print(\"Added UFR Amount column\")\n",
    "                \n",
    "            if not main_ufr_date_col:\n",
    "                main_select_cols.append(lit(None).cast(DateType()).alias('\"UFR Date\"'))\n",
    "                main_ufr_date_col = '\"UFR Date\"'\n",
    "                print(\"Added UFR Date column\")\n",
    "                \n",
    "            if not main_ufr_tag_col:\n",
    "                main_select_cols.append(lit(\"Not UFR\").cast(StringType()).alias('\"UFR Tag\"'))\n",
    "                main_ufr_tag_col = '\"UFR Tag\"'\n",
    "                print(\"Added UFR Tag column\")\n",
    "            \n",
    "            main_clean = main_df_original.select(*main_select_cols)\n",
    "            \n",
    "            # Prepare all secondary data for all periods\n",
    "            all_secondary_data = []\n",
    "            valid_periods = []\n",
    "            \n",
    "            for period in lookback_list_UFR:\n",
    "                secondary_data = prepare_secondary_data_for_period_Excel_CPR(session, period)\n",
    "                if secondary_data is not None:\n",
    "                    all_secondary_data.append(secondary_data)\n",
    "                    valid_periods.append(period)\n",
    "                    print(f\"Successfully prepared data for period {period}\")\n",
    "                else:\n",
    "                    print(f\"Skipping period {period} due to data preparation failure\")\n",
    "            \n",
    "            if not all_secondary_data:\n",
    "                print(\"No valid secondary data found for any period\")\n",
    "                return main_clean\n",
    "            \n",
    "            # Union all secondary data\n",
    "            print(f\"Combining secondary data from {len(all_secondary_data)} periods\")\n",
    "            combined_secondary = all_secondary_data[0]\n",
    "            for i in range(1, len(all_secondary_data)):\n",
    "                combined_secondary = combined_secondary.union(all_secondary_data[i])\n",
    "            \n",
    "            print(f\"Combined secondary data has {combined_secondary.count()} rows\")\n",
    "            \n",
    "            # Create mapping of periods to filter main table (if Period column exists)\n",
    "            period_filter_values = [period_mapping.get(p) for p in valid_periods]\n",
    "            print(f\"Period mappings for filtering: {period_filter_values}\")\n",
    "            \n",
    "            # Filter main table based on Period column if it exists\n",
    "            if main_period_col:\n",
    "                print(f\"Filtering main table rows with Period in: {period_filter_values}\")\n",
    "                main_filtered = main_clean.filter(\n",
    "                    col(main_period_col).isin(period_filter_values) &\n",
    "                    col(main_amount_col).is_not_null() & \n",
    "                    col(main_custname_col).is_not_null() &\n",
    "                    col(main_current_period_col).is_not_null() &\n",
    "                    col(main_product_col).is_not_null() &\n",
    "                    col(main_revenue_type_col).is_not_null() &\n",
    "                    (trim(col(main_custname_col)) != \"\") &\n",
    "                    (trim(col(main_product_col)) != \"\") &\n",
    "                    (trim(col(main_revenue_type_col)) != \"\")\n",
    "                )\n",
    "            else:\n",
    "                print(\"No Period column found in main table, processing all rows\")\n",
    "                main_filtered = main_clean.filter(\n",
    "                    col(main_amount_col).is_not_null() & \n",
    "                    col(main_custname_col).is_not_null() &\n",
    "                    col(main_current_period_col).is_not_null() &\n",
    "                    col(main_product_col).is_not_null() &\n",
    "                    col(main_revenue_type_col).is_not_null() &\n",
    "                    (trim(col(main_custname_col)) != \"\") &\n",
    "                    (trim(col(main_product_col)) != \"\") &\n",
    "                    (trim(col(main_revenue_type_col)) != \"\")\n",
    "                )\n",
    "            \n",
    "            print(f\"Main table rows after filtering: {main_filtered.count()}\")\n",
    "            \n",
    "            # Perform enhanced join with combined secondary data (5 join conditions)\n",
    "            print(\"Enhanced join conditions:\")\n",
    "            print(f\"  Main.{main_amount_col} = Secondary.Amount\")\n",
    "            print(f\"  Main.{main_custname_col} = Secondary.CustName\")\n",
    "            print(f\"  Main.{main_current_period_col} = Secondary.UFR_Date\")\n",
    "            print(f\"  Main.{main_product_col} = Secondary.Product\")\n",
    "            print(f\"  Main.{main_revenue_type_col} = Secondary.Revenue_Type\")\n",
    "            \n",
    "            if main_period_col:\n",
    "                print(f\"  Main.{main_period_col} = Secondary.Period_Source\")\n",
    "                # Join with period matching (6 conditions)\n",
    "                join_condition = (\n",
    "                    (main_filtered[main_amount_col] == combined_secondary[\"Amount\"]) & \n",
    "                    (main_filtered[main_custname_col] == combined_secondary[\"CustName\"]) &\n",
    "                    (main_filtered[main_current_period_col] == combined_secondary[\"UFR_Date\"]) &\n",
    "                    (main_filtered[main_product_col] == combined_secondary[\"Product\"]) &\n",
    "                    (main_filtered[main_revenue_type_col] == combined_secondary[\"Revenue_Type\"]) &\n",
    "                    (main_filtered[main_period_col] == combined_secondary[\"Period_Source\"])\n",
    "                )\n",
    "            else:\n",
    "                # Join without period matching (5 conditions)\n",
    "                join_condition = (\n",
    "                    (main_filtered[main_amount_col] == combined_secondary[\"Amount\"]) & \n",
    "                    (main_filtered[main_custname_col] == combined_secondary[\"CustName\"]) &\n",
    "                    (main_filtered[main_current_period_col] == combined_secondary[\"UFR_Date\"]) &\n",
    "                    (main_filtered[main_product_col] == combined_secondary[\"Product\"]) &\n",
    "                    (main_filtered[main_revenue_type_col] == combined_secondary[\"Revenue_Type\"])\n",
    "                )\n",
    "            \n",
    "            print(\"Performing left outer join with multi-period enhanced data...\")\n",
    "            joined_df = main_filtered.join(combined_secondary, join_condition, \"left\")\n",
    "            \n",
    "            # Build result with updated UFR values\n",
    "            select_columns = []\n",
    "            \n",
    "            # Add all main table columns except UFR columns\n",
    "            for c in main_filtered.columns:\n",
    "                if c not in [main_ufr_amount_col, main_ufr_date_col, main_ufr_tag_col]:\n",
    "                    select_columns.append(col(c))\n",
    "            \n",
    "            # Handle UFR Amount (secondary takes precedence)\n",
    "            ufr_amount_col = coalesce(\n",
    "                col(\"UFR_Amount\"),  # From secondary\n",
    "                col(main_ufr_amount_col)  # From main\n",
    "            ).alias(main_ufr_amount_col)\n",
    "            select_columns.append(ufr_amount_col)\n",
    "            \n",
    "            # Handle UFR Date (secondary takes precedence)\n",
    "            ufr_date_col = coalesce(\n",
    "                col(\"UFR_Date\"),  # From secondary\n",
    "                col(main_ufr_date_col)  # From main\n",
    "            ).alias(main_ufr_date_col)\n",
    "            select_columns.append(ufr_date_col)\n",
    "            \n",
    "            # Handle UFR Tag (set to \"UFR\" if UFR Date exists, otherwise \"Not UFR\")\n",
    "            ufr_tag_col = when(\n",
    "                coalesce(col(\"UFR_Date\"), col(main_ufr_date_col)).is_not_null(),\n",
    "                lit(\"UFR\")\n",
    "            ).otherwise(lit(\"Not UFR\")).alias(main_ufr_tag_col)\n",
    "            select_columns.append(ufr_tag_col)\n",
    "            \n",
    "            joined_result = joined_df.select(*select_columns)\n",
    "            \n",
    "            # Merge back with original main table to maintain all rows\n",
    "            print(\"Merging results back with original main table...\")\n",
    "            \n",
    "            # Create unique keys for joining back (Enhanced with Product and Revenue Type)\n",
    "            main_with_key = main_clean.with_column(\n",
    "                \"temp_join_key\",\n",
    "                concat(\n",
    "                    coalesce(col(main_amount_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_custname_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_current_period_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_product_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_revenue_type_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_period_col), lit(\"NULL\")) if main_period_col else lit(\"NULL\")\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            joined_with_key = joined_result.with_column(\n",
    "                \"temp_join_key\",\n",
    "                concat(\n",
    "                    coalesce(col(main_amount_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_custname_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_current_period_col).cast(\"string\"), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_product_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_revenue_type_col), lit(\"NULL\")),\n",
    "                    lit(\"||\"),\n",
    "                    coalesce(col(main_period_col), lit(\"NULL\")) if main_period_col else lit(\"NULL\")\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Final merge\n",
    "            final_result = main_with_key.join(\n",
    "                joined_with_key.select(\n",
    "                    col(\"temp_join_key\").alias(\"joined_key\"),\n",
    "                    col(main_ufr_amount_col).alias(\"new_UFR_Amount\"),\n",
    "                    col(main_ufr_date_col).alias(\"new_UFR_Date\"),\n",
    "                    col(main_ufr_tag_col).alias(\"new_UFR_Tag\")\n",
    "                ),\n",
    "                main_with_key[\"temp_join_key\"] == col(\"joined_key\"),\n",
    "                \"left\"\n",
    "            )\n",
    "            \n",
    "            # Build final select with all original columns\n",
    "            final_select_cols = []\n",
    "            for c in main_clean.columns:\n",
    "                if c == main_ufr_amount_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Amount\"), col(c)).alias(c))\n",
    "                elif c == main_ufr_date_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Date\"), col(c)).alias(c))\n",
    "                elif c == main_ufr_tag_col:\n",
    "                    final_select_cols.append(coalesce(col(\"new_UFR_Tag\"), col(c), lit(\"Not UFR\")).alias(c))\n",
    "                else:\n",
    "                    final_select_cols.append(col(c))\n",
    "            \n",
    "            result_df = final_result.select(*final_select_cols)\n",
    "            \n",
    "            # Show summary statistics\n",
    "            total_original_rows = main_df_original.count()\n",
    "            total_result_rows = result_df.count()\n",
    "            \n",
    "            print(f\"Original main table rows: {total_original_rows}\")\n",
    "            print(f\"Final result rows: {total_result_rows}\")\n",
    "            \n",
    "            if total_result_rows == total_original_rows:\n",
    "                print(\"✓ Successfully maintained original row count\")\n",
    "            else:\n",
    "                print(\"⚠ Warning: Row count changed!\")\n",
    "            \n",
    "            # Show UFR statistics by period\n",
    "            print(f\"\\nProcessed periods: {valid_periods}\")\n",
    "            print(f\"Period mappings: {[period_mapping.get(p) for p in valid_periods]}\")\n",
    "            \n",
    "            # Show UFR Tag summary\n",
    "            try:\n",
    "                result_df.group_by(main_ufr_tag_col).count().order_by(main_ufr_tag_col).show()\n",
    "            except:\n",
    "                print(\"Could not display UFR Tag summary\")\n",
    "            \n",
    "            # Show join match statistics\n",
    "            try:\n",
    "                matched_rows = result_df.filter(col(main_ufr_date_col).is_not_null()).count()\n",
    "                print(f\"Rows with UFR matches (all join conditions): {matched_rows}\")\n",
    "                print(f\"Rows without UFR matches: {total_result_rows - matched_rows}\")\n",
    "            except:\n",
    "                print(\"Could not display match statistics\")\n",
    "            \n",
    "            return result_df, main_period_col  # Return main_period_col for use in main function\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in multi-period enhanced join operation: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None, None\n",
    "    \n",
    "    def UFR_Excel_CPR_main(session):\n",
    "        \"\"\"Main function to execute the multi-period enhanced join process\"\"\"\n",
    "        print(\"=*=\"*25)\n",
    "        print(\"Starting multi-period enhanced UFR join process...\")\n",
    "        print(f\"Lookback periods: {lookback_list_UFR}\")\n",
    "        print(f\"Period mappings: {[period_mapping.get(p, 'Unknown') for p in lookback_list_UFR]}\")\n",
    "        print(\"Enhanced join conditions:\")\n",
    "        print(\"  - Amount = Amount\")\n",
    "        print(\"  - CustName = CustName\")\n",
    "        print(\"  - Current Period = UFR Date\")\n",
    "        print(\"  - Product = Product\")\n",
    "        print(\"  - Revenue Type = Revenue Type\")\n",
    "        \n",
    "        result, main_period_col = perform_multi_period_join_Excel_CPR(session, lookback_list_UFR)\n",
    "        \n",
    "        if main_period_col:\n",
    "            print(\"  - Period = Period_Source (filtered by period mappings)\")\n",
    "        \n",
    "        if result is not None:\n",
    "            print(\"Multi-period enhanced join completed successfully!\")\n",
    "            \n",
    "            # Save result\n",
    "            result.write.mode(\"overwrite\").save_as_table(output_Excel_CPR)\n",
    "            print(\"=\"*100)\n",
    "            print(f\"___Saving on path: {output_Excel_CPR}\")\n",
    "            print(\"=\"*100)\n",
    "            \n",
    "            # Show final statistics\n",
    "            row_count = result.count()\n",
    "            print(f\"Total rows in output table: {row_count}\")\n",
    "            \n",
    "            # Show UFR Tag summary\n",
    "            print(\"Final UFR Tag summary:\")\n",
    "            try:\n",
    "                # Find the UFR Tag column name\n",
    "                main_df_original = session.table(main_table_name_Excel_CPR)\n",
    "                main_ufr_tag_col = find_column_name_Excel_CPR(main_df_original, 'UFR Tag')\n",
    "                if not main_ufr_tag_col:\n",
    "                    main_ufr_tag_col = '\"UFR Tag\"'\n",
    "                result.group_by(main_ufr_tag_col).count().order_by(main_ufr_tag_col).show()\n",
    "            except:\n",
    "                print(\"Could not display final UFR Tag summary\")\n",
    "        else:\n",
    "            print(\"Multi-period enhanced join process failed\")\n",
    "            print(\"=\"*100)\n",
    "    \n",
    "    UFR_Excel_CPR_main(session)\n",
    "\n",
    "# Enhanced Example usage:\n",
    "# UFR_Excel_CPR_final(session, [1, 3])      # For MOM and QOQ with enhanced join conditions\n",
    "# UFR_Excel_CPR_final(session, [1, 3, 12])  # For MOM, QOQ, and YOY with enhanced join conditions\n",
    "# UFR_Excel_CPR_final(session, [12])        # For YOY only with enhanced join conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3278c-78c0-4c9a-95b0-ccca0e69fe89",
   "metadata": {
    "collapsed": false,
    "name": "markdown_flow"
   },
   "source": [
    "# Flow block\n",
    "**Retention Flow (retention_flow())**\n",
    "\n",
    "`Purpose: Builds retention tables based on the given level.`\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Loads input data via data_loading().\n",
    "\n",
    "- Runs retention_pipeline_v2() with session, lookbacks, and config.\n",
    "\n",
    "- Caches the result and saves it to pbi_path.\n",
    "\n",
    "- Returns: Processed retention DataFrame.\n",
    "\n",
    "**Fact Table Flow (fact_table_flow())**\n",
    "\n",
    "`Purpose: Builds a summarized fact table using highest priority retention level.`\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Determines the highest-level granularity from retention_levels.\n",
    "\n",
    "- Loads retention table and base fact data.\n",
    "\n",
    "- Filters retention data for selected amount/period.\n",
    "\n",
    "- Aggregates and joins with fact data on customer and product keys.\n",
    "\n",
    "- Calculates CM (Cohort Month) and drops redundant columns.\n",
    "\n",
    "- Saves final fact table to fact_path.\n",
    "\n",
    "**Databook Table Flow (databook_table_flow())**\n",
    "\n",
    "`Purpose: Creates an Excel-aligned grouped version of retention data.`\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Groups by key attributes (CustName, Product, Period, etc.).\n",
    "\n",
    "- Aggregates financial metrics (EoP$, BoP$, YoY Variance, etc.).\n",
    "\n",
    "- Truncates date to month granularity.\n",
    "\n",
    "- Saves grouped results to the Excel output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81adf0-c75e-4685-a80d-d9c080bae589",
   "metadata": {
    "language": "python",
    "name": "flow"
   },
   "outputs": [],
   "source": [
    "def retention_flow(session: Session, retention_level: str, pbi_path: str, input_file_path=input_file_path) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Processes the input data to build a retention table.\n",
    "\n",
    "    Parameters:\n",
    "    session (Session): The Snowpark session object.\n",
    "    retention_level (str): The level at which retention is calculated.\n",
    "    pbi_path (str): The file path where the resulting retention table will be saved.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The resulting DataFrame after processing the retention pipeline.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n_Building Retention Table Started at {retention_level}.\")\n",
    "    loaded_df = data_loading(\n",
    "        session,\n",
    "        input_file_path,\n",
    "        column_mapping_file,\n",
    "        #filter_condition=filter_condition,\n",
    "        type=\"retention\",\n",
    "        input_amount=input_amount,\n",
    "        retention_level=retention_level,\n",
    "    )\n",
    "\n",
    "    result = retention_pipeline_v2(\n",
    "        session,\n",
    "        loaded_df,\n",
    "        lookback_list,\n",
    "        input_amount,\n",
    "        run_at_levels,\n",
    "    )\n",
    "\n",
    "    # Cache the result before saving to prevent recomputation\n",
    "\n",
    "    result = result.cache_result()\n",
    "    \n",
    "    # print(\"Printing retention level oooooooooooooo:   \",retention_level)\n",
    "\n",
    "    # if retention_level == \"Customer_Product_level\" or retention_level == \"Customer_Product_RetentionType_level\":\n",
    "    #     result = transform_retention_data(session, result, input_table_templogic, retention_level)\n",
    "\n",
    "    if input_table_templogic is not None and (\n",
    "    retention_level == \"Customer_Product_level\" or \n",
    "    retention_level == \"Customer_Product_RetentionType_level\"):\n",
    "        result = transform_retention_data(session, result, input_table_templogic, retention_level) \n",
    "        \n",
    "    # result = result.drop('\"TOTAL_ARR\"','\"ARR_ROLLCHECK\"','\"COUNT_ROLLCHECK\"','\"COHORT_MAX_DATES_CHECK\"')\n",
    "    save_results(result, pbi_path)\n",
    "    print(f\"_Building Retention Table Completed at {retention_level}.\")\n",
    "    end_time = time.time()\n",
    "    print(f\"⏱️ Retention Flow Runtime: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def fact_table_flow(session: Session, fact_path: str, input_file_path=input_file_path) -> None:\n",
    "    \"\"\"\n",
    "    Processes the retention result DataFrame to build a fact table.\n",
    "\n",
    "    Parameters:\n",
    "    session (Session): The Snowpark session object.\n",
    "    fact_path (str): The file path where the resulting fact table will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"\\n_Building Fact Table Started...\")\n",
    "    level_map = {\"Customer_level\": 1\n",
    "                , \"Customer_Product_level\": 2\n",
    "                , \"Customer_Product_RetentionType_level\": 3\n",
    "                , \"Level4\": 4}\n",
    "\n",
    "    highest_level = None\n",
    "    highest_value = -1\n",
    "\n",
    "    for level in retention_levels:\n",
    "        value = level_map.get(level, 0)\n",
    "        if value > highest_value:\n",
    "           highest_value = value\n",
    "           highest_level = level\n",
    "\n",
    "    print(f\"Manual approach result: {highest_level}\")\n",
    "\n",
    "    #highest_level = max(retention_levels, key=lambda x: level_map.get(x, 0), default=None)\n",
    "    pbi_path = get_file_path(highest_level, pbi_retention_output_path)\n",
    "    retention_result = session.table(pbi_path)\n",
    "\n",
    "    \n",
    "    # print(\"_Building Fact Table Started...\")\n",
    "    fact_df = data_loading(\n",
    "        session,\n",
    "        input_file_path,\n",
    "        column_mapping_file,\n",
    "        #filter_condition=filter_condition,\n",
    "        type=\"fact\",\n",
    "        retention_level = highest_level\n",
    "    )\n",
    "\n",
    "    unique_amount = run_at_levels[0]\n",
    "\n",
    "    lb_value = lookback_list[0]\n",
    "    if lb_value == 1:\n",
    "        unique_period = \"Month\"\n",
    "    elif lb_value == 3:\n",
    "        unique_period = \"Quarter\"\n",
    "    elif lb_value == 12:\n",
    "        unique_period = \"Year\"\n",
    "    else:\n",
    "        unique_period = \"Year\"\n",
    "\n",
    "    result_temp = retention_result.filter(\n",
    "        (col('\"Amount\"') == unique_amount) & (col('\"Period\"') == unique_period)\n",
    "    )\n",
    "\n",
    "    result_short = (\n",
    "        result_temp.group_by(\n",
    "            col('\"CustName\"'),\n",
    "            col('\"Cohort Date\"'),\n",
    "            col('\"Current Period\"'),\n",
    "            col('\"Product\"'),\n",
    "            col('\"Revenue Type\"'),\n",
    "            col(\"LEVEL4\"),\n",
    "            col(\"Account Size\")  # Add Account Size to the group by\n",
    "        )\n",
    "        .agg(\n",
    "            count(col('\"CustName\"')).alias(\"Count of rows\"),\n",
    "            sum(col(\"T3M\")).alias(\"T3M\"),\n",
    "            sum(col(\"TTM\")).alias(\"TTM\"),\n",
    "            sum(col(\"MRR\")).alias(\"MRR\"),\n",
    "            sum(col(\"ARR\")).alias(\"ARR\"),\n",
    "            sum(col('\"T3M (Annualized)\"')).alias('\"T3M (Annualized)\"'),\n",
    "        )\n",
    "        .drop(\"Count of rows\")\n",
    "    )\n",
    "\n",
    "    fact_df_joined = fact_df.join(\n",
    "        result_short,\n",
    "        (fact_df['\"CustomerID\"'] == result_short['\"CustName\"'])\n",
    "        & (fact_df['\"Date\"'] == result_short['\"Current Period\"'])\n",
    "        & (fact_df['\"Product\"'] == result_short['\"Product\"'])\n",
    "        & (fact_df['\"Revenue Type\"'] == result_short['\"Revenue Type\"'])\n",
    "        & (fact_df['\"LEVEL4\"'] == result_short['\"LEVEL4\"']),\n",
    "        lsuffix=\"_left\",\n",
    "    )\n",
    "\n",
    "    # Updating fact_df values\n",
    "    fact_df_joined = fact_df_joined.withColumn(\n",
    "        \"CM\", datediff(\"day\", col('\"Cohort Date\"'), col('\"Date\"'))\n",
    "    )\n",
    "\n",
    "    fact_df_joined = fact_df_joined.drop(\n",
    "        ['\"CustName\"', '\"Current Period\"', '\"Product_left\"', '\"Revenue Type_left\"']\n",
    "    )\n",
    "\n",
    "    save_results(fact_df_joined, fact_path)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"_Building Fact Table Completed.\")\n",
    "    print(f\"⏱️ Fact Table Flow Runtime: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "def databook_table_flow(result: DataFrame, excel_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Generates a databook table from the given DataFrame and saves it to an Excel file.\n",
    "\n",
    "    Parameters:\n",
    "    result (DataFrame): Input DataFrame to process.\n",
    "    excel_path (str): Path to save the Excel file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"_Building Databook Table Started...\")\n",
    "\n",
    "    databook_gp = [\n",
    "        '\"Current Period\"',\n",
    "        '\"CustName\"',\n",
    "        '\"Product\"',\n",
    "        '\"Revenue Type\"',\n",
    "        '\"RetentionCategory\"',\n",
    "        '\"Period\"',\n",
    "        '\"Cohort Date\"',\n",
    "        '\"Amount\"',\n",
    "        '\"UFR Tag\"',\n",
    "        '\"Account Size\"',  # Add Account Size to the group by\n",
    "    ]\n",
    "\n",
    "    result_databook = result.groupBy(databook_gp).agg(\n",
    "        sum('\"EoP$\"').alias('\"EoP$\"'),\n",
    "        sum('\"BoP$\"').alias('\"BoP$\"'),\n",
    "        sum('\"YoY Variance\"').alias('\"YoY Variance\"'),\n",
    "        sum('\"UFR Amount\"').alias('\"UFR Amount\"'),\n",
    "        sum('\"Net Credit\"').alias('\"Net Credit\"'),\n",
    "    )\n",
    "\n",
    "    result_databook = result_databook.with_column(\n",
    "        \"Current Period\", date_trunc(\"MONTH\", col(\"Current Period\"))\n",
    "    )\n",
    "\n",
    "    #### result grouping outputs the full retention table GROUPED to align to the Excel template\n",
    "    print(\"Saving Results...\")\n",
    "    save_results(result_databook, excel_path)\n",
    "    print(\"_Building Databook Table Completed.\")\n",
    "    end_time = time.time()\n",
    "    print(f\"⏱️ Databook Flow Runtime: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0045ea-2bd7-4113-bad4-1cd07e677796",
   "metadata": {
    "collapsed": false,
    "name": "markdown_main"
   },
   "source": [
    "# Optimized_main_with_mapping_bundling\n",
    "This pipeline prepares, analyzes, and summarizes customer data to support retention reporting and revenue insights.\n",
    "\n",
    "**Step 0: Standardize Data**\n",
    "\n",
    "`standardize_customer_fields()`\n",
    "Cleans and maps raw customer data using column mapping.\n",
    "\n",
    "**Step 0.5: Round Float Columns**\n",
    "\n",
    "`round_float_columns()`\n",
    "Rounds all numeric fields to 3 decimal places for consistency.\n",
    "\n",
    "**Step 1: Retention Analysis**\n",
    "\n",
    "`retention_flow() (via retention_worker)`\n",
    "Runs retention logic at multiple levels (Customer, Customer_Product, CPR).\n",
    "Outputs to Power BI and Excel formats.\n",
    "Parallelized using ThreadPoolExecutor.\n",
    "\n",
    "**Step 2: Fact Table**\n",
    "\n",
    "`fact_table_flow()`\n",
    "Aggregates retention data into a summarized fact table for dashboarding.\n",
    "\n",
    "**Step 3: Product Bundle Analysis**\n",
    "\n",
    "`product_bundle_analysis()`\n",
    "Derives ARR/MRR, cross-sell, and bundle size insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b57e3-113b-47e0-bfa5-33ed08bd4dd3",
   "metadata": {
    "language": "python",
    "name": "main"
   },
   "outputs": [],
   "source": [
    "# Original start timing code\n",
    "start_time = time.time()\n",
    "\n",
    "# Add standardization step first - before any retention processing\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 0: STANDARDIZING CUSTOMER FIELDS\")\n",
    "print(\"=\"*50)\n",
    "standardized_df, affected_customers_report = standardize_customer_fields(session, input_file_path, column_mapping_file)\n",
    "\n",
    "# If you want to use the standardized data for all subsequent operations,\n",
    "# update the input path to the processed output\n",
    "processed_input_path = input_file_path + \"_ARR_Mapped\"\n",
    "\n",
    "# Round all float columns to 3 decimal places\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 0.5: ROUNDING FLOAT COLUMNS TO 3 DECIMAL PLACES\")\n",
    "print(\"=\"*50)\n",
    "# Use the properly implemented Snowpark decimal function\n",
    "rounded_df = round_float_columns(session, processed_input_path, decimal_places=3)\n",
    "\n",
    "# Now proceed with the original workflow but using the standardized data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 1: RUNNING RETENTION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def retention_worker(retention_level):\n",
    "    print(\"Running for retention level:\", retention_level)\n",
    "    if retention_level == \"Customer_level\":\n",
    "        pbi_path = pbi_retention_output_path + \"_C_NOTEBOOK\"\n",
    "        excel_path = excel_retention_output_path + \"_C\"\n",
    "    elif retention_level == \"Customer_Product_level\":\n",
    "        pbi_path = pbi_retention_output_path + \"_CP\"\n",
    "        excel_path = excel_retention_output_path + \"_CP\"\n",
    "    elif retention_level == \"Customer_Product_RetentionType_level\":\n",
    "        pbi_path = pbi_retention_output_path + \"_CPR\"\n",
    "        excel_path = excel_retention_output_path + \"_CPR\"\n",
    "    else:\n",
    "        print(f\"Unknown retention level: {retention_level}\")\n",
    "        return None\n",
    "    \n",
    "    # Use the standardized data instead of the original input\n",
    "    retention_result = retention_flow(session, retention_level, pbi_path, input_file_path=processed_input_path)\n",
    "    print(\"-\" * 50)\n",
    "    databook_table_flow(retention_result, excel_path)\n",
    "    print(\"=\" * 50)\n",
    "    return retention_level\n",
    "\n",
    "# Execute in parallel\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    futures = {executor.submit(retention_worker, level): level for level in retention_levels}\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {futures[future]}: {e}\")\n",
    "\n",
    "# Run fact table flow after all retention levels\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 2.1: GENERATING FACT TABLE\")\n",
    "print(\"=\"*50)\n",
    "fact_table_flow(session, fact_table_output_path, input_file_path=processed_input_path)\n",
    "\n",
    "# DIMENSION_DIM table creation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 2.2: GENERATING DIMENSION_DIM TABLE\")\n",
    "output_dimension = fact_table_output_path + \"_DIMENSION_DIM\"\n",
    "result_df = create_master_dimension_table(session, fact_table_output_path, output_dimension)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# DATE_DIM table creation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 2.3: GENERATING DATE_DIM TABLE\")\n",
    "output_date = fact_table_output_path + \"_DATE_DIM\"\n",
    "result_df = create_date_dimension_table(session, fact_table_output_path, output_date, row_count=10000)\n",
    "print(\"=\"*50)\n",
    "\n",
    "#Add product bundle analysis\n",
    "cache_s_time = time.time()\n",
    "try:\n",
    "    product_bundle_result = product_bundle_analysis(session)\n",
    "except Exception as e:\n",
    "    print(f\"Error in product bundle analysis step: {str(e)}\")\n",
    "    traceback.print_exc()\n",
    "    product_bundle_result = None\n",
    "cache_e_time = time.time()\n",
    "print(f\"⏱️ Caching bundling: {cache_e_time - cache_s_time:.2f} seconds\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Rename columns\n",
    "rename_fixed_tables(session)\n",
    "rename_pbi_tables(session, retention_levels)\n",
    "\n",
    "# Print execution time\n",
    "end_time = time.time()\n",
    "elapsed_seconds = end_time - start_time\n",
    "hours = int(elapsed_seconds // 3600)\n",
    "minutes = int((elapsed_seconds % 3600) // 60)\n",
    "seconds = int(elapsed_seconds % 60)\n",
    "print(f\"Total execution time: {hours} hours {minutes} minutes {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a226189c-2970-4139-b333-b97676822f05",
   "metadata": {
    "collapsed": false,
    "name": "markdown_main_ufr"
   },
   "source": [
    "# main_ufr                       \n",
    "Functions included- `ufr_c`, `ufr_cp` and `ufr_cpr`. The output generation will depend on `lookback_list_UFR` and `retention_levels` selction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760fec2-94fa-41fc-8532-3b83f2aba047",
   "metadata": {
    "language": "python",
    "name": "main_ufr"
   },
   "outputs": [],
   "source": [
    "def run_selected_ufr(session: Session, table_name: str, lookback_list_UFR: list[int], retention_levels: list[str]):\n",
    "    \"\"\"\n",
    "    Run UFR functions based on period comparison and retention level.\n",
    "    \n",
    "    :param session: Snowflake Snowpark session\n",
    "    :param table_name: Input table name\n",
    "    :param lookback_list_UFR: List of lookback periods [1, 3, 12]\n",
    "    :param retention_levels: List of levels ['Customer_level', 'Customer_Product_level', 'Customer_Product_RetentionType_level']\n",
    "    \"\"\"\n",
    "    if lookback_list_UFR is None:\n",
    "        print(\"Skipping UFR execution because lookback_list_UFR is None.\")\n",
    "        return\n",
    "\n",
    "    period_map = {\n",
    "        1: 'MoM',\n",
    "        3: 'QoQ',\n",
    "        12: 'YoY'\n",
    "    }\n",
    "\n",
    "    level_map = {\n",
    "        'Customer_level': 'C',\n",
    "        'Customer_Product_level': 'CP',\n",
    "        'Customer_Product_RetentionType_level': 'CPR'\n",
    "    }\n",
    "\n",
    "    for lookback in lookback_list_UFR:\n",
    "        if lookback not in period_map:\n",
    "            print(f\"Unsupported period: {lookback}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        period_suffix = period_map[lookback]\n",
    "\n",
    "        for level in retention_levels:\n",
    "            if level not in level_map:\n",
    "                print(f\"Unsupported level: {level}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            level_prefix = level_map[level]\n",
    "            func_name = f\"UFR_{level_prefix}_{period_suffix}\"\n",
    "\n",
    "            try:\n",
    "                print(f\"Running: {func_name}()\")\n",
    "                func = globals()[func_name]\n",
    "                result_df = func(session, table_name)\n",
    "                result_table_name = f\"{table_name}_{level_prefix}_{period_suffix}\"\n",
    "                result_df.write.mode(\"overwrite\").saveAsTable(result_table_name)\n",
    "                print(f\"___Saving on path: {result_table_name}\")\n",
    "                print(\"-\" * 50)\n",
    "            except KeyError:\n",
    "                print(f\"Function {func_name} not defined.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error running {func_name}: {e}\")\n",
    "\n",
    "run_selected_ufr(\n",
    "    session=session, \n",
    "    table_name=input_file_path, \n",
    "    lookback_list_UFR=lookback_list_UFR,\n",
    "    retention_levels=retention_levels\n",
    ")\n",
    "\n",
    "def run_selected_functions(session, retention_levels, lookback_list_UFR):\n",
    "    if lookback_list_UFR is None:\n",
    "        print(\"Skipping function execution because lookback_list_UFR is None.\")\n",
    "        return\n",
    "\n",
    "    function_map = {\n",
    "        \"Customer_level\": [UFR_PBI_C_final, UFR_Excel_C_final],\n",
    "        \"Customer_Product_level\": [UFR_PBI_CP_final, UFR_Excel_CP_final],\n",
    "        \"Customer_Product_RetentionType_level\": [UFR_PBI_CPR_final, UFR_Excel_CPR_final]\n",
    "    }\n",
    "\n",
    "    for level in retention_levels:\n",
    "        functions_to_run = function_map.get(level, [])\n",
    "        for func in functions_to_run:\n",
    "            func(session, lookback_list_UFR)\n",
    "\n",
    "# Example usage:\n",
    "run_selected_functions(session, retention_levels, lookback_list_UFR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e765be05-e325-4292-ba88-c4202d843b3a",
   "metadata": {
    "collapsed": false,
    "name": "markdown_qc_mechanism"
   },
   "source": [
    "# QC_Mechanism\n",
    "Generates two kind of outputs-       \n",
    "(1) `from input table`- Two columns generates- `Current Period` and `Total_Value`.   \n",
    "(2) `from PBI outputs`- Four columns generates- `Current Period`, `Period`, `SUM_EOP$` and `SUM_BOP$`.  \n",
    "**Note**- Filter second output wrt `Period` and then sort by `Current Period` and then compare it with first output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76102a5e-7943-4bc4-b3dd-ac73ed52d2ad",
   "metadata": {
    "language": "python",
    "name": "qc_mechanism"
   },
   "outputs": [],
   "source": [
    "# Read the input table\n",
    "print(\"QC table generation started for input file...\")\n",
    "input_df = session.table(input_file_path)\n",
    "\n",
    "# Group by Date and sum the ARR values\n",
    "monthly_arr_summary = input_df.group_by(col(column_mapping_file[\"CURRENTPERIOD\"])) \\\n",
    "                             .agg(snowpark_sum(col(column_mapping_file[\"VALUE\"])).alias(\"TOTAL_VALUE\")) \\\n",
    "                             .order_by(col(column_mapping_file[\"CURRENTPERIOD\"]))\n",
    "\n",
    "output_qc= input_file_path + \"_QC\"\n",
    "# Save the output to a new table\n",
    "monthly_arr_summary.write.mode('overwrite').save_as_table(output_qc)\n",
    "\n",
    "print(f\"✅ Monthly value summary table created and saved to {output_qc} for raw input file.\")\n",
    "print(\"=\"*50)\n",
    "print(\"QC table generation started for processed input file...\")\n",
    "\n",
    "# List of 6 input file paths\n",
    "input_files = [\n",
    "    pbi_retention_output_path + \"_C_NOTEBOOK\",\n",
    "    pbi_retention_output_path + \"_CP\",\n",
    "    pbi_retention_output_path + \"_CPR\",\n",
    "]\n",
    "\n",
    "# Iterate through each input file\n",
    "for input_table in input_files:\n",
    "    try:\n",
    "        # Attempt to load the table\n",
    "        df = session.table(input_table)\n",
    "\n",
    "        # Filter the DataFrame based on input_amount value\n",
    "        df = df.filter(F.col('\"Amount\"') == input_amount)\n",
    "\n",
    "        # Perform SUM aggregations grouped by \"Current Period\"\n",
    "        aggregated_df = (\n",
    "            df.group_by(\"Current Period\", '\"Period\"')\n",
    "              .agg(\n",
    "                  \n",
    "                  snowflake_sum('\"EoP$\"').alias(\"Sum_EoP$\"),\n",
    "                  snowflake_sum('\"BoP$\"').alias(\"Sum_BoP$\")\n",
    "              )\n",
    "        )\n",
    "\n",
    "        # Define output table name\n",
    "        output_table = input_table + \"_QC\"\n",
    "\n",
    "        # Save the result as a new table (overwrite if exists)\n",
    "        aggregated_df.write.mode(\"overwrite\").save_as_table(output_table)\n",
    "\n",
    "        print(f\"✅ Output saved with EoP$, BoP$ and Current Period to: {output_table} \")\n",
    "\n",
    "    except SnowparkSQLException as e:\n",
    "        print(f\"⚠️ Skipping unavailable file: {input_table} (Reason: {e.message})\")\n",
    "print(\"=\"*50)\n",
    "print(\"You can now compare the outputs of each file to verify the results for input and output files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb6a9a-2d9d-4576-bdf4-adb42cfb23e2",
   "metadata": {
    "collapsed": false,
    "name": "markdown_test"
   },
   "source": [
    "# Retention QA & PoP Validation\n",
    "`Input`\n",
    "- Load retention table (e.g., \"_C_NOTEBOOK\")\n",
    "\n",
    "- Extract distinct Period & Amount values\n",
    "\n",
    "`Loop Logic`\n",
    "\n",
    "- Filter data\n",
    "\n",
    "- all_checks() – Validates QA columns vs. thresholds\n",
    "\n",
    "- period_on_period_check_v2() – Compares BoP vs. lagged EoP\n",
    "\n",
    "`Functions`\n",
    "\n",
    "- all_checks(df, cols, thresholds)\n",
    "→ Marks Pass/Fail, shows failed rows\n",
    "\n",
    "- period_on_period_check_v2(period, df)\n",
    "→ Ensures EoP → BoP continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d8ebd-96ee-4fdf-bde8-28a2e4697f9b",
   "metadata": {
    "language": "python",
    "name": "test"
   },
   "outputs": [],
   "source": [
    "# User inputs\n",
    "pbi_retention_output_path = pbi_retention_output_path + \"_C_NOTEBOOK\" # Customer_level\n",
    "# pbi_retention_output_path = pbi_retention_output_path + \"_CP\" # Customer_Product_level\n",
    "# pbi_retention_output_path = pbi_retention_output_path + \"_CPR\" # Customer_Product_RetentionType_level\n",
    "\n",
    "retention_result = session.table(pbi_retention_output_path)\n",
    "periods = retention_result.select('\"Period\"').distinct().collect()\n",
    "amounts = retention_result.select('\"Amount\"').distinct().collect()\n",
    "\n",
    "def period_on_period_check_v2(period, filtered_df):\n",
    " \n",
    "    if period[0] == 'Month':\n",
    "        criteria = col(\"Current Period\")\n",
    "    elif period[0]  == 'Year':\n",
    "        criteria = year(col(\"Current Period\"))\n",
    "        window_spec = Window.orderBy()\n",
    "    # elif period[0]  == 'Quarter':\n",
    "    #     criteria = (year(col(\"Current Period\")), quarter(col(\"Current Period\")))\n",
    "    \n",
    "    result_df = filtered_df.group_by(criteria).agg(\n",
    "        sum(col('\"EoP$\"')).alias(\"sum_EoP$\"),\n",
    "        sum(col('\"BoP$\"')).alias(\"sum_BoP$\")\n",
    "    )\n",
    "    window_spec = Window.orderBy(criteria)\n",
    "    result_df = result_df.sort(criteria)\n",
    "        \n",
    "    df_with_lag = result_df.withColumn(\"sum_EoP$_pr\", lag(col(\"sum_EoP$\"), 1).over(window_spec))\n",
    "        \n",
    "    df_final = df_with_lag.withColumn(\n",
    "            \"Check_value\",\n",
    "            when(col(\"sum_BoP$\") == col(\"sum_EoP$_pr\"), 'True').otherwise('False')\n",
    "        )\n",
    "        \n",
    "    chk_result = df_final.filter(col(\"Check_value\") != True)\n",
    "        \n",
    "    if chk_result.count() == 1:\n",
    "        print(\"Period on Period Check\", \"-->\", \"Check Passed!\")\n",
    "        print(\"=\" * 50)\n",
    "    else:\n",
    "        print(\"Period on Period Check\", \"-->\", \"Check Failed!\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Snapshot view\")\n",
    "        df_final.show()\n",
    "        \n",
    "period_on_period_mapping={\n",
    "    \"Year\":\"ARR\",\n",
    "    \"Month\":\"MRR\",\n",
    "    # \"Quarter\":\"T3M\"\n",
    "}\n",
    "\n",
    "def all_checks(tmp, qa_columns, qa_check_thresholds):\n",
    "    for qa_column in qa_columns:\n",
    "        check_df = verify_qa_check(tmp, [qa_column], qa_check_thresholds)\n",
    "        # Try Catch block has been added to handle empty Dataframes after filter conditions in verify_qa_check\n",
    "        try:\n",
    "            sample = check_df.limit(1).collect()\n",
    "            if len(sample) == 0:\n",
    "                print(qa_column,\"-->\", \"Check Passed!\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(qa_column,\"-->\", \"Check if column exists in retention result: Specify correct coulumn in Input!\")\n",
    "            continue\n",
    "        result_str = \"Check Failed!\" if check_df.count() > 0 else \"Check Passed!\"\n",
    "        print(qa_column,\"-->\", result_str)\n",
    "        if result_str == \"Check Failed!\":\n",
    "            print(\"Snapshot view\")\n",
    "            check_df.show()\n",
    "    print(\"-\"*50)\n",
    "\n",
    "for amount in amounts: \n",
    "    for period in periods:\n",
    "        print(f\"Running for amount: {amount[0]} and period: {period[0]}\")\n",
    "        print(\"=\"*50)\n",
    "        tmp = retention_result.filter((col('\"Period\"') == period[0]) & (col('\"Amount\"')== amount[0]))\n",
    "        \n",
    "        all_checks(tmp, qa_columns, qa_check_thresholds)\n",
    "        \n",
    "        # if period_on_period_mapping.get(period[0]) == amount[0]:\n",
    "        #     print(\"Period on period check!\", period[0], amount[0])\n",
    "        #     tmp = retention_result.filter((col('\"Period\"') == period[0]) & (col('\"Amount\"')== amount[0]))\n",
    "        #     period_on_period_check_v2(period, tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  },
  "lastEditStatus": {
   "authorEmail": "satyam.modanwal@gds.ey.com",
   "authorId": "4132570847525",
   "authorName": "SATYAM_MODANWAL",
   "lastEditTime": 1755610997966,
   "notebookId": "f35lrybiti3sb7ys5ewp",
   "sessionId": "2e213652-e6f7-460c-88b2-e8f236275b51"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
