{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "q4k775s752kzlhaa3jse",
   "authorId": "4132570847525",
   "authorName": "SATYAM_MODANWAL",
   "authorEmail": "satyam.modanwal@gds.ey.com",
   "sessionId": "85a1d803-2027-4dd8-aa07-4fdac08021a4",
   "lastEditTime": 1755606326106
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "collapsed": false,
    "name": "notebook_steps"
   },
   "source": "# Step by step heriarcy of notebook-\n\n`import_libraries` -> `input_selection` -> `user_input` -> `utils` -> `arr_mapping_fields` -> `product_bundling` -> `round_off` -> `dimension_date_dim` -> `{ufr_logics}` -> `flows` -> `main` -> `main_ufr`-> `qc_mechanism`-> `test`\n\n**Note-** Do not alter the cell position."
  },
  {
   "cell_type": "markdown",
   "id": "765f8a57-fe34-4fde-8ea6-69d739a29bbe",
   "metadata": {
    "collapsed": false,
    "name": "markdown_import_libraries"
   },
   "source": [
    "# Import libraries for data processing and snowpark\n",
    "\n",
    "**Key libraries**\n",
    "\n",
    "- `Snowflake snowpark` libraries\n",
    "    - Session, Mathematical Operations\n",
    "    - Snowpark data types\n",
    "- `Pandas` for data frame processing\n",
    "- `functools.reduce`: For functional programming utilities\n",
    "- `ThreadPoolExecutor`: For parallel processing of function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a946e-b00c-463d-9f27-8d8669a00f7a",
   "metadata": {
    "language": "python",
    "name": "import_libraries"
   },
   "outputs": [],
   "source": "# STANDARD LIBRARY IMPORTS\nimport time\nimport traceback\nimport logging\nimport json\nimport glob\nfrom functools import reduce\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# CORE SNOWPARK IMPORTS\nimport snowflake.snowpark as snowpark\nfrom snowflake.snowpark import Session, DataFrame\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.exceptions import SnowparkSQLException\n\n# WINDOW FUNCTIONS\nfrom snowflake.snowpark import Window\nfrom snowflake.snowpark.window import Window  # Note: This is duplicate - keep one\nfrom snowflake.snowpark.functions import row_number, lag\n\n# SNOWPARK FUNCTIONS MODULE\nfrom snowflake.snowpark import functions as F\n\n# COLUMN OPERATIONS\nfrom snowflake.snowpark.functions import (\n    col,\n    when,\n    lit,\n    coalesce,\n    is_null\n)\n\n# AGGREGATION FUNCTIONS\nfrom snowflake.snowpark.functions import (\n    sum,\n    # Multiple aliases for sum\n    sum as sum_sf,\n    sum as spark_sum,\n    sum as snowflake_sum,\n    sum as snowpark_sum,\n    count,\n    max,\n    max as spark_max,\n    min,\n    min as spark_min\n)\n\n# DATE/TIME FUNCTIONS\nfrom snowflake.snowpark.functions import (\n    to_date,\n    dateadd,\n    add_months,\n    date_trunc,\n    datediff,\n    dayofmonth\n)\n\n# STRING FUNCTIONS\nfrom snowflake.snowpark.functions import (\n    upper,\n    trim,\n    regexp_replace,\n    regexp_count,\n    concat,\n    length\n)\n\n# MATHEMATICAL FUNCTIONS\nfrom snowflake.snowpark.functions import (\n    abs,\n    round\n)\n\n# SEQUENCE FUNCTIONS\nfrom snowflake.snowpark.functions import (\n    seq1,\n    seq8\n)\n\n# DATA TYPES\nfrom snowflake.snowpark.types import *\nfrom snowflake.snowpark.types import (\n    DecimalType,\n    DateType,\n    StringType\n)\n\n# SESSION INITIALIZATION\nsession = get_active_session()"
  },
  {
   "cell_type": "code",
   "id": "eca88d38-9771-45f2-9c36-3f7d8acbe0af",
   "metadata": {
    "language": "python",
    "name": "input_selection"
   },
   "outputs": [],
   "source": "import streamlit as st\n# --- Title ---\nst.title(\"TMT Enabler Input Parameters\")\n\n# --- User Inputs ---\n# Lookback list selection (allow multiple)\nlookback_list = st.multiselect(\n    \"Select lookback period(s):\",\n    options=[1, 3, 12],\n    help=\"Choose one or more of Month (1), Quarter (3), or Year (12).\"\n)\n\n# Input amount selection\ninput_amount = st.selectbox(\n    \"Select input amount type:\",\n    options=[\"ARR\", \"MRR\"]\n)\n\n# Run at levels\nrun_at_levels = st.multiselect(\n    \"Select run at levels:\",\n    options=[\"ARR\", \"MRR\", \"T3M\", \"TTM\", \"T3M (Annualized)\"],\n    default=[\"ARR\", \"MRR\"]\n)\n\n# Retention levels\nretention_levels = st.multiselect(\n    \"Select retention level(s):\",\n    options=[\n        \"Customer_level\",\n        \"Customer_Product_level\", \n        \"Customer_Product_RetentionType_level\",\n        \"Level4\"\n    ],\n    default=[\n        \"Customer_level\"\n    ]\n)\n\n# --- Check if all required inputs are provided ---\nif (\n    lookback_list and \n    input_amount and \n    run_at_levels and \n    retention_levels\n):\n    st.success(\"All required inputs selected. Ready to run!\")\n    \n    # --- Add your business logic or function call below ---\n    st.write(\"### Selected Parameters:\")\n    st.write(f\"Lookback List: {lookback_list}\")\n    st.write(f\"Input Amount: {input_amount}\")\n    st.write(f\"Run At Levels: {run_at_levels}\")\n    st.write(f\"Retention Levels: {retention_levels}\")\n    \n    # Debug info to show the actual values\n    # st.write(\"### Debug Info:\")\n    # st.write(f\"Use UFR Logic: {use_ufr_logic}\")\n    # st.write(f\"lookback_list_UFR type: {type(lookback_list_UFR)}\")\n    # st.write(f\"lookback_list_UFR value: {lookback_list_UFR}\")\n    \n    # Code representation (like your comment example)\n    # st.write(\"### Code Representation:\")\n    # if use_ufr_logic and lookback_list_UFR:\n    #     st.code(f\"# lookback_list_UFR = None  # Commented out\\nlookback_list_UFR = {lookback_list_UFR}\")\n    # else:\n    #     st.code(f\"lookback_list_UFR = None\\n# lookback_list_UFR = [1,3,12]  # Commented out\")\n    \n    # You can now call your main function here\n    # result = your_main_function(\n    #     lookback_list=lookback_list,\n    #     lookback_list_UFR=lookback_list_UFR,\n    #     input_amount=input_amount,\n    #     run_at_levels=run_at_levels,\n    #     retention_levels=retention_levels\n    # )\n    # st.write(result)\nelse:\n    st.warning(\"Please select all required parameters to proceed.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e61920-8f51-4a44-bf9a-c8c95606a478",
   "metadata": {
    "name": "configuration",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Step 1: Find the JSON file matching *.config.json in current directory\n",
    "config_files = glob.glob(\"*.config.json\")\n",
    "\n",
    "if not config_files:\n",
    "    raise FileNotFoundError(\"No config file matching '*.config.json' found in the current directory.\")\n",
    "elif len(config_files) > 1:\n",
    "    raise RuntimeError(\"Multiple '*.config.json' files found. Expected only one.\")\n",
    "else:\n",
    "    config_file = config_files[0]\n",
    "    print(f\"Loading config from: {config_file}\")\n",
    "\n",
    "# Step 2: Load the config file\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Step 3: Extract input and pbi table names\n",
    "try:\n",
    "    input_table = config[\"ODBC config\"][\"tables\"][\"input_table\"]\n",
    "    pbi_table = config[\"ODBC config\"][\"tables\"][\"pbi_table\"]\n",
    "    excel_table = config[\"ODBC config\"][\"tables\"][\"excel_table\"]\n",
    "    fact_table = config[\"ODBC config\"][\"tables\"][\"fact_table\"]\n",
    "except KeyError as e:\n",
    "    raise KeyError(f\"Missing expected key in config: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b52c56-7c20-4a99-ae64-0d79e2532ab6",
   "metadata": {
    "collapsed": false,
    "name": "markdown_user_input"
   },
   "source": "# User input variables & levers\n\n**Input Variables**\n\n- `lookback_list`\n- `input_amount`\n- `run_at_levels`\n- `retention_levels`\n- `input_file_path`\n- `input_table_templogic`\n- `input_table_product_bundle`\n- `pbi_retention_output_path`\n- `excel_retention_output_path`\n- `fact_table_output_path`\n\n**Mapping columns**\n- `column_mapping_file`\n\n**Filter Condition**\n- Can only be done on base columns like  `CUSTOMERID, PRODUCT, REVENUETYPE, CURRENTPERIOD, ARR` \n- SAMPLE: col(\"CustomerID\") == \"99999_TEST\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19632869-2b70-47fb-8b9c-cb5a79ff3e16",
   "metadata": {
    "language": "python",
    "name": "user_input"
   },
   "outputs": [],
   "source": "# #####################################################################\n# ### defining lookbacks function, default is 3 for quarter lookbacks\n# ### for monthly ARR data, YoY = 12, QoQ = 3, MoM = 1\n# ### for quarterly ARR data, YoY = 4, QoQ = 1\n# #####################################################################\n# # Period Comparison. 1 = Month, 3 = Quarter, 12 = Year\n# lookback_list= [1,3,12]\n\n# # Input amount - \"ARR\" or \"MRR\"\n# input_amount = \"ARR\"\n\n# # run_at_levels - [\"ARR\", \"MRR\", \"T3M\", \"TTM\", \"T3M (Annualized)\"]\n# run_at_levels = [\"ARR\", \"MRR\", \"T3M\", \"TTM\", \"T3M (Annualized)\"]\n\n# # Retention_level | \n# # Valid values [\"Customer_level\",\"Customer_Product_level\",\"Customer_Product_RetentionType_level\",\"Level4\"]\n# retention_levels = [\"Customer_level\",\"Customer_Product_level\",\"Customer_Product_RetentionType_level\"]\n\n# Input Tables\ninput_file_path = input_table\n \n# Input is a small sample of SUBSCRIPTION_ACCEL.PYTHON_TESTING.INPUT_BOOKING_TEST\n# Output Tables Path\n## Note: Do not add _C, _CP and _CPR at the end of the paths. This is handled internally!\npbi_retention_output_path = pbi_table\nexcel_retention_output_path = excel_table\nfact_table_output_path = fact_table\n\n# Input Table for Temp logic. \n# NOTE: If not to use Temp logic, make it \"None\". \n# input_table_templogic = pbi_retention_output_path + \"_C_NOTEBOOK\"\ninput_table_templogic = None\n\n# Input Table for product bundling\ninput_table_product_bundle = pbi_retention_output_path + \"_C_NOTEBOOK\"\n\n# Mapping File with respect to columns available in input file\ncolumn_mapping_file = {\n    \"CUSTOMERID\":'\"CUSTOMERID\"'\n    ,\"PRODUCT\": '\"PRODUCT\"'\n    ,\"REVENUETYPE\": '\"REVENUETYPE\"'\n    ,\"CURRENTPERIOD\":'\"CURRENTPERIOD\"'\n    ,\"VALUE\":'\"VALUE\"'\n    ,\"Account Size\": None\n    ,\"Region\": None\n    ,\"Industry\": None\n    ,\"Channel\": None}\n\n\n####################### Filter condition ############################\n# Can only be done on base columns like \n# CUSTOMERID, PRODUCT, REVENUETYPE, CURRENTPERIOD, ARR. \n# SAMPLE: col(\"CustomerID\") == \"99999_TEST\"\n\nfilter_condition: None # default\n# filter_condition = col(\"CUSTOMERID\") == \"Customer 002651\"\n# filter_condition = col(\"CURRENTPERIOD\").between(\"2020-01-01\", \"2020-06-31\")\n\n# Add column names here to run the all_qa_check function\nqa_columns = ['ARR_ROLLCHECK', 'Count_RollCheck', 'Cohort_Max_Dates_Check']\n\n# Thresholds\nqa_check_thresholds = 0.001"
  },
  {
   "cell_type": "markdown",
   "id": "4f632b52-2334-4ed8-9356-2898103c7386",
   "metadata": {
    "collapsed": false,
    "name": "markdown_utils"
   },
   "source": [
    "**This pipeline offers a comprehensive framework for customer retention analysis, valuable for data analysts and business intelligence professionals.**\n",
    "\n",
    "## Key Functions\n",
    "\n",
    "1. **`determine_account_size(arr_value)`**: Classifies account sizes based on ARR values.\n",
    "2. **`add_rename_cols(df, mapping_file)`**: Adds and renames columns in a DataFrame based on a mapping.\n",
    "3. **`add_fact_table_cols(df)`**: Adds dummy columns for reporting in a fact table.\n",
    "4. **`data_loading(session, path, mapping_file, filter_condition, type, input_amount, retention_level)`**: Loads and processes data based on specified analysis type.\n",
    "5. **`generate_months_snowpark(session, input_df)`**: Generates monthly records for all customers.\n",
    "6. **`credit_df_prepare(df)`**: Prepares DataFrame for credit analysis by adjusting ARR values.\n",
    "7. **`window_cols(df)`**: Adds windowed calculations for customer metrics.\n",
    "8. **`base_table_creation(session, df)`**: Creates a base table for further analysis.\n",
    "9. **`retention(df, l)`**: Applies retention logic to calculate various metrics.\n",
    "10. **`save_results(result, path, type)`**: Saves processed results to a specified Snowflake table.\n",
    "\n",
    "## Outputs\n",
    "- DataFrames with retention metrics.\n",
    "- Aggregated results saved in Snowflake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9bf9c-e2ac-4ade-920d-9e14be60ec2d",
   "metadata": {
    "language": "python",
    "name": "utils"
   },
   "outputs": [],
   "source": [
    "# Function to determine account size based on ARR value\n",
    "def determine_account_size(arr_value):\n",
    "    if arr_value < 10000:\n",
    "        return \"1. <10K\"\n",
    "    elif 10000 <= arr_value <= 50000:\n",
    "        return \"2. 10K - 50K\"\n",
    "    elif 50000 < arr_value <= 100000:\n",
    "        return \"3. 50K - 100K\"\n",
    "    elif 100000 < arr_value <= 250000:\n",
    "        return \"4. 100K - 250K\"\n",
    "    elif 250000 < arr_value <= 500000:\n",
    "        return \"5. 250K - 500K\"\n",
    "    elif 500000 < arr_value < 1000000:\n",
    "        return \"6. 500K-1M\"\n",
    "    else:\n",
    "        return \"7. >1M\"\n",
    "\n",
    "# To add missing columns and rename them is required format\n",
    "def add_rename_cols(df: DataFrame, mapping_file: dict) -> DataFrame:\n",
    "    for key, value in mapping_file.items():\n",
    "        if key == \"Account Size\" and \"Account Size\" in df.columns:\n",
    "            # Skip this to preserve our calculated Account Size\n",
    "            continue\n",
    "        elif value is None:\n",
    "            df = df.with_column(key,lit(\"NA\"))\n",
    "        else:\n",
    "            df = df.with_column_renamed(value,key)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Add dummy columns to fact table for PBI template \n",
    "def add_fact_table_cols(df: DataFrame) -> DataFrame:\n",
    "    # dummy colsr\n",
    "    df = df.withColumn('\"Boomerang flag\"', lit('\"No Boomerang\"'))\n",
    "    df = df.withColumn(\"Cust+Prdt\", concat(col(\"CUSTOMERID\"), lit(\"_\"), col('\"Product\"')))\n",
    "    df = df.withColumn('\"Cust+Prdt+RevType\"', concat(col(\"CUSTOMERID\"), lit(\"_\"), col('\"Product\"'), lit(\"_\"), col('\"Revenue Type\"')))\n",
    "    df = df.withColumn('\"Cust+Prdt+RevType+Lv4\"', concat(col(\"CUSTOMERID\"), lit(\"_\"), col('\"Product\"'), lit(\"_\"), col('\"Revenue Type\"'), lit(\"_\"), col('\"LEVEL4\"')))\n",
    "    df = df.withColumn('\"Product Category\"', lit(\"NA\"))\n",
    "    df = df.withColumn('\"Product Level 2\"', lit(\"NA\"))\n",
    "\n",
    "    # index col\n",
    "    window_spec = Window.order_by(lit(1))\n",
    "    df = df.withColumn('\"Index\"', row_number().over(window_spec))\n",
    "\n",
    "    # cohort cols\n",
    "    df = df.withColumn(\"CM\", lit(1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generic method to loading data from table\n",
    "def data_loading(session: Session, path: str, mapping_file: dict, filter_condition:str = None, \n",
    "                 type:str =\"retention\", input_amount:str =\"MRR\", retention_level:str =\"Customer_Product_RetentionType_level\"):\n",
    "    \n",
    "    df = session.table(path)\n",
    "    \n",
    "    df = add_rename_cols(df, mapping_file)\n",
    "\n",
    "    if type == \"retention\":\n",
    "\n",
    "        # df = df.with_column(\"CURRENTPERIOD\", to_date(col(\"CURRENTPERIOD\"), \"yyyy-MM-dd\"))\n",
    "        if retention_level == \"Customer_level\":\n",
    "            df = df.group_by(col(\"CUSTOMERID\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('CURRENTPERIOD')\n",
    "            ).agg(sum(col(\"VALUE\")).alias(\"VALUE\"))\n",
    "            \n",
    "            df = df.with_column(\"PRODUCT\", lit(\"NA\"))\n",
    "            df = df.with_column(\"REVENUETYPE\", lit(\"NA\"))\n",
    "            df = df.with_column(\"LEVEL4\", lit(\"NA\"))\n",
    "            \n",
    "        elif retention_level == \"Customer_Product_level\":\n",
    "            df = df.group_by(col(\"CUSTOMERID\")\n",
    "                , col(\"PRODUCT\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('CURRENTPERIOD')\n",
    "            ).agg(sum(col(\"VALUE\")).alias(\"VALUE\"))\n",
    "            \n",
    "            df = df.with_column(\"REVENUETYPE\", lit(\"NA\"))\n",
    "            df = df.with_column(\"LEVEL4\", lit(\"NA\"))\n",
    "            \n",
    "        elif retention_level == \"Customer_Product_RetentionType_level\":\n",
    "            df = df.group_by(col(\"CUSTOMERID\")\n",
    "                , col(\"PRODUCT\")\n",
    "                , col(\"REVENUETYPE\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('CURRENTPERIOD')\n",
    "            ).agg(sum(col(\"VALUE\")).alias(\"VALUE\"))\n",
    "            \n",
    "            df = df.with_column(\"LEVEL4\", lit(\"NA\"))\n",
    "            \n",
    "        elif retention_level == \"Level4\":\n",
    "            df = df.group_by(col(\"CUSTOMERID\")\n",
    "                , col(\"PRODUCT\")\n",
    "                , col(\"REVENUETYPE\")\n",
    "                , col(\"LEVEL4\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('CURRENTPERIOD')\n",
    "            ).agg(sum(col(\"VALUE\")).alias(\"VALUE\"))       \n",
    "    \n",
    "        if not filter_condition is None:\n",
    "            df = df.filter(filter_condition)\n",
    "            \n",
    "    elif type == \"fact\":\n",
    "        # df = df.with_column(\"CURRENTPERIOD\", to_date(col(\"CURRENTPERIOD\"), \"yyyy-MM-dd\"))\n",
    "        \n",
    "        if retention_level == \"Customer_level\":\n",
    "            df = df.group_by(\n",
    "                col(\"CUSTOMERID\")\n",
    "                , col(\"Account Size\")\n",
    "                , col(\"Region\")\n",
    "                , col(\"Industry\")\n",
    "                , col(\"Channel\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('\"Date\"')\n",
    "            ).agg(\n",
    "                count(col(\"CUSTOMERID\")).alias(\"Count of rows\"),\n",
    "                sum(col(\"VALUE\")).alias(\"VALUE\") \n",
    "            )\n",
    "            \n",
    "            df = df.with_column('\"Product\"', lit(\"NA\"))            \n",
    "            df = df.with_column('\"Revenue Type\"', lit(\"NA\"))\n",
    "            df = df.with_column(\"LEVEL4\", lit(\"NA\"))\n",
    "            \n",
    "        elif retention_level == \"Customer_Product_level\":\n",
    "            df = df.group_by(\n",
    "                col(\"CUSTOMERID\")\n",
    "                , col(\"Account Size\")\n",
    "                , col(\"Region\")\n",
    "                , col(\"Industry\")\n",
    "                , col(\"Channel\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('\"Date\"')\n",
    "                , col(\"PRODUCT\" ).alias('\"Product\"')\n",
    "            ).agg(\n",
    "                count(col(\"CUSTOMERID\")).alias(\"Count of rows\"),\n",
    "                sum(col(\"VALUE\")).alias(\"VALUE\") \n",
    "            )\n",
    "            \n",
    "            df = df.with_column('\"Revenue Type\"', lit(\"NA\"))\n",
    "            df = df.with_column(\"LEVEL4\", lit(\"NA\"))\n",
    "            \n",
    "        elif retention_level == \"Customer_Product_RetentionType_level\":\n",
    "            df = df.group_by(\n",
    "                col(\"CUSTOMERID\")\n",
    "                , col(\"Account Size\")\n",
    "                , col(\"Region\")\n",
    "                , col(\"Industry\")\n",
    "                , col(\"Channel\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('\"Date\"')\n",
    "                , col(\"PRODUCT\" ).alias('\"Product\"')\n",
    "                , col(\"REVENUETYPE\").alias('\"Revenue Type\"')\n",
    "            ).agg(\n",
    "                count(col(\"CUSTOMERID\")).alias(\"Count of rows\"),\n",
    "                sum(col(\"VALUE\")).alias(\"VALUE\") \n",
    "            )\n",
    "            \n",
    "            df = df.with_column(\"LEVEL4\", lit(\"NA\"))\n",
    "            \n",
    "        elif retention_level == \"Level4\":\n",
    "            df = df.group_by(\n",
    "                col(\"CUSTOMERID\")\n",
    "                , col(\"Account Size\")\n",
    "                , col(\"Region\")\n",
    "                , col(\"Industry\")\n",
    "                , col(\"Channel\")\n",
    "                , date_trunc('month', col('CURRENTPERIOD')).alias('\"Date\"')\n",
    "                , col(\"PRODUCT\" ).alias('\"Product\"')\n",
    "                , col(\"REVENUETYPE\").alias('\"Revenue Type\"')\n",
    "                , col(\"LEVEL4\")\n",
    "            ).agg(\n",
    "                count(col(\"CUSTOMERID\")).alias(\"Count of rows\"),\n",
    "                sum(col(\"VALUE\")).alias(\"VALUE\") \n",
    "            )\n",
    "\n",
    "        df = add_fact_table_cols(df) # adding new cols\n",
    "\n",
    "        if not filter_condition is None:\n",
    "            df = df.filter(filter_condition)\n",
    "\n",
    "        df = df.with_column_renamed(\"CUSTOMERID\",'\"CustomerID\"')\n",
    "  \n",
    "    return df\n",
    "\n",
    "\n",
    "# Generating all the months rows for all the customers\n",
    "def generate_months_snowpark(session: Session, input_df: DataFrame) -> DataFrame:    \n",
    "    print(\"generate_months_snowpark started...\")\n",
    "    cache_s_time = time.time()\n",
    "    # Get date boundaries\n",
    "    date_boundaries = input_df.agg(min(col(\"CURRENTPERIOD\")), max(col(\"CURRENTPERIOD\"))).collect()\n",
    "    min_date = date_boundaries[0][0]\n",
    "    max_date = date_boundaries[0][1]\n",
    "\n",
    "    # Generate month-end dates using Snowpark\n",
    "    date_range_df = session.range((max_date - min_date).days + 1).select(\n",
    "        (to_date(lit(min_date)) + col(\"id\")).alias(\"date\")\n",
    "    ).filter(\n",
    "        dayofmonth(col(\"date\")) == 1\n",
    "    ).select(\n",
    "        col(\"date\").alias(\"month_date\")\n",
    "    ).distinct()\n",
    "\n",
    "    \n",
    "    # Cross join\n",
    "    cross_join = input_df.cross_join(date_range_df)\n",
    "    \n",
    "    # Update ARR column\n",
    "    cross_join = cross_join.with_column(\n",
    "        \"VALUE\",\n",
    "        when(col(\"CURRENTPERIOD\") == col(\"month_date\"), col(\"VALUE\")).otherwise(lit(0))\n",
    "    )\n",
    "    \n",
    "    # Group by and sum ARR\n",
    "    final_table = cross_join.group_by(\n",
    "        *[col for col in cross_join.columns if col not in ['CURRENTPERIOD', \"VALUE\"]]\n",
    "    ).agg(sum(col(\"VALUE\")).alias(\"VALUE\"))\n",
    "    \n",
    "    # Rename column\n",
    "    final_table = final_table.with_column_renamed(\"month_date\", \"CURRENTPERIOD\")\n",
    "    final_table = final_table.with_column(\"Max Date\", lit(max_date))\n",
    "    cache_e_time = time.time()\n",
    "    print(f\"⏱️ Caching df generate_months_snowpark done: {cache_e_time - cache_s_time:.2f} seconds\")\n",
    "    return final_table\n",
    "    \n",
    "\n",
    "# Generating rows for credit\n",
    "def credit_df_prepare(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    # Split into positive and negative ARR\n",
    "    positive_arr = df.filter(col(\"VALUE\") >= 0)\n",
    "    negative_arr = df.filter(col(\"VALUE\") < 0)\n",
    "    \n",
    "    # Add \"-CR$\" to CUSTOMERID\n",
    "    neg_arr_w_name_change = negative_arr.with_column(\n",
    "        \"CUSTOMERID\", concat(col(\"CUSTOMERID\"), lit(\"-CR$\"))\n",
    "    )\n",
    "    \n",
    "    # Adding back the row with ARR as 0\n",
    "    zero_arr = negative_arr.with_column(\"VALUE\", lit(0.0))\n",
    "\n",
    "    df_final = positive_arr.union_by_name(neg_arr_w_name_change).union_by_name(zero_arr)\n",
    "    \n",
    "    df_final = df_final.sort(col('CURRENTPERIOD'))\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# Creating date columns\n",
    "def window_cols(df: DataFrame) -> DataFrame:\n",
    "\n",
    "    ### Define the window specifications for each set of operations (Group By Summarizations)\n",
    "    customer_window = Window.partitionBy(\"CustomerID\")\n",
    "    customer_date_window = Window.partitionBy(\"CustomerID\",\"CurrentPeriod\")\n",
    "    \n",
    "    customer_product_window = Window.partitionBy(\"CustomerID\", \"Product\")\n",
    "    \n",
    "    customer_product_revenue_window = Window.partitionBy(\"CustomerID\", \"Product\", \"RevenueType\")\n",
    "    \n",
    "    customer_product_revenue_level4_window = Window.partitionBy(\"CustomerID\", \"Product\", \"RevenueType\", \"Level4\")\n",
    "\n",
    "    min_date_w_non_zero_arr = min(when(col('VALUE') != 0, col(\"CurrentPeriod\")))\n",
    "    max_date_w_non_zero_arr = max(when(col('VALUE') != 0, col(\"CurrentPeriod\")))\n",
    "\n",
    "    # Calculate max ARR for each customer (use ARR*12 if the input is MRR)\n",
    "    max_value = max(col(\"VALUE\")).over(customer_window)\n",
    "    df = df.withColumn(\"Customer_Max_ARR\", max_value)\n",
    "    \n",
    "    # Determine account size based on max ARR\n",
    "    df = df.withColumn(\"Account Size\", \n",
    "        when(col(\"Customer_Max_ARR\") < 10000, \"1. <10K\")\n",
    "        .when((col(\"Customer_Max_ARR\") >= 10000) & (col(\"Customer_Max_ARR\") <= 50000), \"2. 10K - 50K\")\n",
    "        .when((col(\"Customer_Max_ARR\") > 50000) & (col(\"Customer_Max_ARR\") <= 100000), \"3. 50K - 100K\")\n",
    "        .when((col(\"Customer_Max_ARR\") > 100000) & (col(\"Customer_Max_ARR\") <= 250000), \"4. 100K - 250K\")\n",
    "        .when((col(\"Customer_Max_ARR\") > 250000) & (col(\"Customer_Max_ARR\") <= 500000), \"5. 250K - 500K\")\n",
    "        .when((col(\"Customer_Max_ARR\") > 500000) & (col(\"Customer_Max_ARR\") < 1000000), \"6. 500K-1M\")\n",
    "        .otherwise(\"7. >1M\")\n",
    "    )\n",
    "    \n",
    "    ## Add minimum and maximum dates for each customer with ARR > 0 (Cohort and MaxARR Dates)\n",
    "    df = df.withColumn(\"Cust_MinDate\", min_date_w_non_zero_arr.over(customer_window))\n",
    "    df = df.withColumn(\"Cust_MaxDate\", max_date_w_non_zero_arr.over(customer_window))\n",
    "\n",
    "    ### Add minimum and maximum dates for each customer-product combination with ARR > 0 (Cohort and MaxARR Dates, 2nd lvl)\n",
    "    df = df.withColumn(\"Cust_Prod_MinDate\", min_date_w_non_zero_arr.over(customer_product_window))\n",
    "    df = df.withColumn(\"Cust_Prod_MaxDate\", max_date_w_non_zero_arr.over(customer_product_window))\n",
    "\n",
    "    ### Add minimum and maximum dates for each customer-product-revtype combination with ARR > 0 (Cohort and MaxARR Dates, 3rd lvl)\n",
    "    df = df.withColumn(\"Cust_Prod_Rev_MinDate\", min_date_w_non_zero_arr.over(customer_product_revenue_window))\n",
    "    df = df.withColumn(\"Cust_Prod_Rev_MaxDate\", max_date_w_non_zero_arr.over(customer_product_revenue_window))\n",
    "    \n",
    "    ### Add minimum and maximum dates for each customer-product-revtype combination with ARR > 0 (Cohort and MaxARR Dates, 3rd lvl)\n",
    "    df = df.withColumn(\"Cust_Prod_Rev_Lv4_MinDate\", min_date_w_non_zero_arr.over(customer_product_revenue_level4_window))\n",
    "    df = df.withColumn(\"Cust_Prod_Rev_Lv4_MaxDate\", max_date_w_non_zero_arr.over(customer_product_revenue_level4_window))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Base table creation: calling function for all row generating logic\n",
    "def base_table_creation(session, df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    df = window_cols(df)\n",
    "    \n",
    "    ### Credit treatment \n",
    "    df = credit_df_prepare(df)\n",
    "    \n",
    "    df = generate_months_snowpark(session, df)\n",
    "\n",
    "    ### add column for total revenue for a customer in a specific period\n",
    "    customer_date_window = Window.partitionBy(\"CustomerID\",\"CurrentPeriod\")\n",
    "    df = df.withColumn(\"Total_ARR\", sum(col(\"VALUE\")).over(customer_date_window))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Creating credit related columns\n",
    "def credit_cols(df: DataFrame) -> DataFrame:\n",
    "    ### Define Credit ARR\n",
    "    df = df.withColumn(\"Credit\",\n",
    "        when(\n",
    "            col(\"VALUE\") < 0\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "\n",
    "    ### Define Credit Reversal ARR\n",
    "    df = df.withColumn(\"Credit_Reversal\",\n",
    "        when(\n",
    "            (col(\"Prior_ARR\") < 0) & (col(\"VALUE\") >= 0)\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "        \n",
    "    ### Define Net Credit  ARR\n",
    "    df = df.withColumn(\"Net_Credit\",\n",
    "                       col('Credit') + col('Credit_Reversal') \n",
    "                      )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Creating ARR related columns\n",
    "def arr_cols(df: DataFrame,l) -> DataFrame:\n",
    "    customer_product_revenue_date_order_window = Window.partitionBy(\"CustomerID\", \"Product\", \"RevenueType\").orderBy(col(\"CurrentPeriod\").asc())\n",
    "    ARR_positive_check = (col(\"Prior_ARR\") == 0) & (col(\"VALUE\") > 0)\n",
    "    Prior_ARR_positive_check = (col(\"VALUE\") == 0) & (col(\"Prior_ARR\") > 0)\n",
    "    \n",
    "    ### Assign Lookback by customer by product by revenue type for ARR (Prior ARR)\n",
    "    df = df.withColumn(\"Prior_ARR\"\n",
    "        , lag(col(\"VALUE\"), offset=l, default_value=0)\n",
    "        .over(customer_product_revenue_date_order_window))\n",
    "    \n",
    "    ### Calculate ARR Change Period of Period (PoP Variance)\n",
    "    df = df.withColumn(\"ARR_Variance\", col(\"VALUE\") - col(\"Prior_ARR\"))\n",
    "\n",
    "    ### Define New Customer ARR\n",
    "    df = df.withColumn(\"NewCust_ARR\",\n",
    "        when(\n",
    "            ARR_positive_check \n",
    "            & (col(\"CurrentPeriod\") < add_months(col(\"Cust_MinDate\"), l)) \n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    ### Define New Product ARR (Cross-sell)\n",
    "    df = df.withColumn(\"NewProd_ARR\",\n",
    "        when(\n",
    "            ARR_positive_check\n",
    "            & (col(\"NewCust_ARR\") == 0)\n",
    "            & (col(\"CurrentPeriod\") < add_months(col(\"Cust_Prod_MinDate\"), l))\n",
    "            , col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    ### Define New Revenue Type ARR (Cross-sell Rev Type)\n",
    "    df = df.withColumn(\"NewRev_ARR\",\n",
    "        when(\n",
    "            ARR_positive_check\n",
    "            & (col(\"NewCust_ARR\") == 0)\n",
    "            & (col(\"NewProd_ARR\") == 0)\n",
    "            & (col(\"CurrentPeriod\") < add_months(col(\"Cust_Prod_Rev_MinDate\"), l))\n",
    "            , col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    ### Define New Lv4 ARR\n",
    "    df = df.withColumn(\"NewLv4_ARR\",\n",
    "        when(\n",
    "            ARR_positive_check\n",
    "            & (col(\"NewCust_ARR\") == 0)\n",
    "            & (col(\"NewProd_ARR\") == 0)\n",
    "            & (col(\"NewRev_ARR\") == 0)\n",
    "            & (col(\"CurrentPeriod\") < add_months(col(\"Cust_Prod_Rev_Lv4_MinDate\"), l))\n",
    "            , col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "        \n",
    "    ### Define Temp New ARR     \n",
    "    df = df.withColumn(\"TempNew_ARR\",\n",
    "        when(\n",
    "            ARR_positive_check\n",
    "            & (col(\"NewCust_ARR\") == 0)\n",
    "            & (col(\"NewProd_ARR\") == 0)\n",
    "            & (col(\"NewRev_ARR\") == 0)\n",
    "            & (col(\"NewLv4_ARR\") == 0)\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    ### Define Upsell ARR       \n",
    "    df = df.withColumn(\"Upsell_ARR\",\n",
    "        when(\n",
    "            (col(\"VALUE\") > col(\"Prior_ARR\"))\n",
    "            & (col(\"Prior_ARR\") > 0)\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "   \n",
    "    ### Define Customer Churn ARR        \n",
    "    df = df.withColumn(\"ChurnCust_ARR\",\n",
    "        when(\n",
    "            Prior_ARR_positive_check\n",
    "            & (col(\"CurrentPeriod\") > col(\"Cust_MaxDate\"))\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    ### Define Product Churn ARR\n",
    "    df = df.withColumn(\"ChurnProd_ARR\",\n",
    "        when(\n",
    "            Prior_ARR_positive_check\n",
    "            & (col(\"ChurnCust_ARR\") == 0)\n",
    "            & (col(\"CurrentPeriod\") > col(\"Cust_Prod_MaxDate\"))\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    ### Define Revenue Type Churn ARR\n",
    "    df = df.withColumn(\"ChurnRev_ARR\",\n",
    "        when(\n",
    "            Prior_ARR_positive_check\n",
    "            & (col(\"ChurnCust_ARR\") == 0)\n",
    "            & (col(\"ChurnProd_ARR\") == 0)\n",
    "            & (col(\"CurrentPeriod\") > col(\"Cust_Prod_Rev_MaxDate\"))\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    ### Define Lv4 Churn ARR\n",
    "    df = df.withColumn(\"ChurnLv4_ARR\",\n",
    "        when(\n",
    "            Prior_ARR_positive_check\n",
    "            & (col(\"ChurnCust_ARR\") == 0)\n",
    "            & (col(\"ChurnProd_ARR\") == 0)\n",
    "            & (col(\"ChurnRev_ARR\") == 0)\n",
    "            & (col(\"CurrentPeriod\") > col(\"Cust_Prod_Rev_Lv4_MaxDate\"))\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    ### Define Temp Loss ARR\n",
    "    df = df.withColumn(\"TempLoss_ARR\",\n",
    "        when(\n",
    "            Prior_ARR_positive_check\n",
    "            & (col(\"ChurnCust_ARR\") == 0)\n",
    "            & (col(\"ChurnProd_ARR\") == 0)\n",
    "            & (col(\"ChurnRev_ARR\") == 0)\n",
    "            & (col(\"ChurnLv4_ARR\") == 0)\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "\n",
    "    ### Define Downsell ARR       \n",
    "    df = df.withColumn(\"Downsell_ARR\",\n",
    "        when(\n",
    "            (col(\"VALUE\") < col(\"Prior_ARR\"))\n",
    "            & (col(\"VALUE\") > 0)\n",
    "            ,col(\"ARR_Variance\")\n",
    "        ).otherwise(0)\n",
    "    ) \n",
    "    return df\n",
    "\n",
    "# Creating QA checks related columns   \n",
    "def qa_cols(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    ### Define Roll Forward Check      \n",
    "    df = df.withColumn(\n",
    "        \"ARR_RollCheck\",\n",
    "        (col('NewCust_ARR') + col('NewProd_ARR') \n",
    "        + col('NewRev_ARR') + col('TempNew_ARR')\n",
    "        + col('Upsell_ARR') + col('ChurnCust_ARR')\n",
    "        + col('ChurnProd_ARR') + col('ChurnRev_ARR') \n",
    "        + col('TempLoss_ARR') + col('Downsell_ARR') \n",
    "        + col('Prior_ARR') + col('Net_Credit')\n",
    "        - col(\"VALUE\")).cast(\"float\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"Count_RollCheck\",\n",
    "        col('BoP_count')\n",
    "        + col('NewCust_count') + col('NewProd_count')\n",
    "        + col('NewRev_count') + col('TempNew_count') \n",
    "        + col('ChurnCust_count') + col('ChurnProd_count') \n",
    "        + col('ChurnRev_count') + col('TempLoss_count') \n",
    "        - col('EoP_count')\n",
    "    )\n",
    "    \n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"Cohort_Max_Dates_Check\",\n",
    "        when( (col(\"VALUE\")!=0 ) &\n",
    "             (\n",
    "                 (col('CurrentPeriod') > col('Cust_MaxDate')) | \n",
    "                 (col('CurrentPeriod') > col('Cust_Prod_MaxDate')) | \n",
    "                 (col('CurrentPeriod') > col('Cust_Prod_Rev_MaxDate'))\n",
    "             ),1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Creating count related columns\n",
    "def count_cols(df: DataFrame) -> DataFrame:\n",
    "\n",
    "    ### Define New Customer Count\n",
    "    df = df.withColumn(\"NewCust_count\", when(col(\"NewCust_ARR\") != 0, 1).otherwise(0))\n",
    "\n",
    "    ### Define New Product Count     \n",
    "    df = df.withColumn(\"NewProd_count\", when(col(\"NewProd_ARR\") != 0, 1).otherwise(0))\n",
    "\n",
    "    ### Define New Revenue Type Count       \n",
    "    df = df.withColumn(\"NewRev_count\", when(col(\"NewRev_ARR\") > 0, 1).otherwise(0))\n",
    "\n",
    "    ### Define Temp New Customer Count          \n",
    "    df = df.withColumn(\"TempNew_count\", when(col(\"TempNew_ARR\") > 0, 1).otherwise(0)) \n",
    "\n",
    "    ### Define Upsell Customer Count        \n",
    "    df = df.withColumn(\"Upsell_count\", when(col(\"Upsell_ARR\") > 0, 1).otherwise(0))\n",
    "\n",
    "    ### Define Churn Customer Count            \n",
    "    df = df.withColumn(\"ChurnCust_count\", when(col(\"ChurnCust_ARR\") != 0, -1).otherwise(0))\n",
    "        \n",
    "    ### Define Churn Product Count        \n",
    "    df = df.withColumn(\"ChurnProd_count\", when(col(\"ChurnProd_ARR\") != 0, -1).otherwise(0))\n",
    "\n",
    "    ### Define Churn Revenue Type Count        \n",
    "    df = df.withColumn(\"ChurnRev_count\", when(col(\"ChurnRev_ARR\") < 0, -1).otherwise(0))\n",
    "\n",
    "    ### Define Downsell Customer Count\n",
    "    df = df.withColumn(\"Downsell_count\", when(col(\"Downsell_ARR\") < 0, -1).otherwise(0))\n",
    "\n",
    "    ### Define Temp Loss Customer Count     \n",
    "    df = df.withColumn(\"TempLoss_count\", when(col(\"TempLoss_ARR\") < 0, -1).otherwise(0))\n",
    "\n",
    "    ### EoP Customer Count\n",
    "    df = df.withColumn(\"EoP_count\", when(col(\"VALUE\") > 0, 1).otherwise(0))\n",
    "\n",
    "    ### BoP Customer Count\n",
    "    df = df.with_column(\"BoP_count\", when(col(\"Prior_ARR\") > 0, 1).otherwise(0))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Creating columns not falling in any category for example, RetentionCategory and Period\n",
    "def extra_cols(df: DataFrame,l) -> DataFrame:\n",
    "\n",
    "    #### Define Retention Categories ####  \n",
    "    df = df.with_column(\"RetentionCategory\",\n",
    "        when(col(\"Credit\") != 0,'Credit')\n",
    "        .when(col(\"Credit_Reversal\") != 0,'Credit Reversal')\n",
    "        .when(col(\"NewCust_ARR\") != 0, 'New Cust')\n",
    "        .when(col(\"NewProd_ARR\") != 0, 'New Prod')\n",
    "        .when(col(\"NewRev_ARR\") != 0, 'New Rev Type')\n",
    "        .when(col(\"NewLv4_ARR\") != 0, 'New Lv4')\n",
    "        .when(col(\"TempNew_ARR\") != 0, 'Increase_TempNew')\n",
    "        .when(col(\"Upsell_ARR\") != 0, 'Increase')\n",
    "        .when(col(\"ChurnCust_ARR\") != 0, 'Churn Cust')\n",
    "        .when(col(\"ChurnProd_ARR\") != 0, 'Churn Prod')\n",
    "        .when(col(\"ChurnRev_ARR\") != 0, 'Churn Rev Type')\n",
    "        .when(col(\"ChurnLv4_ARR\") != 0, 'Churn Lv4')\n",
    "        .when(col(\"TempLoss_ARR\") != 0, 'Decrease_TempLost')\n",
    "        .when(col(\"Downsell_ARR\") != 0, 'Decrease')\n",
    "        .otherwise('NoChange')\n",
    "    )\n",
    "\n",
    "    ### add lookback column as a tag\n",
    "    df = df.with_column(\"Period\", lit(l).cast(\"string\"))\n",
    "    \n",
    "    df = df.with_column(\"Period\",\n",
    "        when(col(\"Period\") == \"1\", \"Month\")\n",
    "        .when(col(\"Period\") == \"3\", \"Quarter\")\n",
    "        .when(col(\"Period\") == \"12\", \"Year\")\n",
    "        .otherwise(col(\"Period\")) \n",
    "    )\n",
    "\n",
    "    df = df.with_column(\n",
    "        \"Retained100%$\",\n",
    "        when(col(\"RetentionCategory\") == 'NoChange'\n",
    "             ,col(\"ARR_Variance\")\n",
    "            )\n",
    "    )\n",
    "\n",
    "    df = df.with_column(\n",
    "        \"Date SOM\", date_trunc('month', col('CURRENTPERIOD'))\n",
    "    )\n",
    "\n",
    "    df = df.with_column(\n",
    "        \"TTM Date\",when(col('Date SOM') > col('Max Date'), dateadd('year',lit(1),'Date SOM'))\n",
    "    .otherwise(col('Date SOM'))\n",
    "    )\n",
    "\n",
    "    ## Placeholder columns\n",
    "    df = df.with_column(\"UFR Amount\", lit(0.0))\n",
    "    df = df.with_column(\"UFR Tag\", lit(\"Not UFR\"))\n",
    "    df = df.with_column(\"Current Winback Tag\", lit(\"\"))\n",
    "    df = df.with_column(\"UFR Date\", lit(\"\"))\n",
    "    df = df.with_column(\"WinbackTag\", lit(\"No Winback\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calling function for Retention logic \n",
    "def retention(df: DataFrame,l) -> DataFrame: \n",
    "    new_df = arr_cols(df,l)\n",
    "    new_df = credit_cols(new_df)\n",
    "    new_df = count_cols(new_df)\n",
    "    new_df = extra_cols(new_df,l)\n",
    "    new_df = qa_cols(new_df)\n",
    "    return new_df\n",
    "\n",
    "# Syncing columns name to Alteryx version\n",
    "def rename_cols(df: DataFrame) -> DataFrame:\n",
    "    alyterx_col_mapping = {\n",
    "        \"ARR_VARIANCE\": '\"YoY Variance\"',\n",
    "        \"VALUE\": '\"EoP$\"',\n",
    "        \"PRIOR_ARR\": '\"BoP$\"',\n",
    "        \"NEWCUST_ARR\": '\"NewCust$\"',\n",
    "        \"NEWPROD_ARR\": '\"NewProd$\"',\n",
    "        \"NEWREV_ARR\": '\"NewRevType$\"',\n",
    "        \"TEMPNEW_ARR\": '\"Increase_TempNew\"',\n",
    "        \"UPSELL_ARR\": '\"Increase$\"',\n",
    "        \"CHURNCUST_ARR\": '\"ChurnCust$\"',\n",
    "        \"CHURNPROD_ARR\": '\"ChurnProd$\"',\n",
    "        \"CHURNREV_ARR\": '\"ChurnRevType$\"',\n",
    "        \"TEMPLOSS_ARR\": '\"Decrease_TempLost\"',\n",
    "        \"DOWNSELL_ARR\": '\"Decrease$\"',\n",
    "        \"NET_CREDIT\": '\"Net Credit\"',\n",
    "        \"NEWCUST_COUNT\": '\"NewCust_count\"',\n",
    "        \"NEWPROD_COUNT\": '\"NewProd_count\"',\n",
    "        \"NEWREV_COUNT\": '\"NewRevType_count\"',\n",
    "        \"TEMPNEW_COUNT\": '\"Increases_TempNew_count\"', #Added extra 's'\n",
    "        \"UPSELL_COUNT\": '\"Increase_count\"',\n",
    "        \"CHURNCUST_COUNT\": '\"ChurnCust_count\"',\n",
    "        \"CHURNPROD_COUNT\": '\"ChurnProd_count\"',\n",
    "        \"CHURNREV_COUNT\": '\"ChurnRevType_count\"',\n",
    "        \"DOWNSELL_COUNT\": '\"Decreases_count\"',\n",
    "        \"TEMPLOSS_COUNT\": '\"Decreases_TempLost_count\"',\n",
    "        \"CURRENTPERIOD\": '\"Current Period\"',\n",
    "        \"CUSTOMERID\": '\"CustName\"',\n",
    "        \"PERIOD\": '\"Period\"',\n",
    "        \"PRODUCT\": '\"Product\"',\n",
    "        \"RETENTIONCATEGORY\": '\"RetentionCategory\"',\n",
    "        \"REVENUETYPE\": '\"Revenue Type\"',\n",
    "        \"CUST_MINDATE\": '\"Cohort Date\"',\n",
    "        \"CUST_MAXDATE\": '\"Max Date w ARR\"',\n",
    "        \"CUST_PROD_MAXDATE\": '\"Max Date w ARR_2nd lv\"',\n",
    "        \"CUST_PROD_MINDATE\": '\"Cohort Date_2nd lv\"',\n",
    "        \"CUST_PROD_REV_MAXDATE\": '\"Max Date w ARR_3rd lv\"',\n",
    "        \"CUST_PROD_REV_MINDATE\": '\"Cohort Date_3rd lv\"',\n",
    "        \"CUST_PROD_REV_Lv4_MAXDATE\": '\"Max Date w ARR_4th lv\"',\n",
    "        \"CUST_PROD_REV_Lv4_MINDATE\": '\"Cohort Date_4th lv\"',\n",
    "        \"Credit\": '\"Credit_$\"',\n",
    "        \"CREDIT_REVERSAL\":'\"Credit_Reversal$\"',\n",
    "        \"EOP_COUNT\": '\"EoP_count\"',\n",
    "        \"BOP_COUNT\": '\"BoP_count\"',\n",
    "        \"ARR_ROLLCHECK\": '\"Check\"',\n",
    "        \"COUNT_ROLLCHECK\": '\"Check_Count\"'\n",
    "    }\n",
    "\n",
    "    for python_name, alyterx_name in alyterx_col_mapping.items():\n",
    "        df = df.withColumnRenamed(python_name, alyterx_name)\n",
    "\n",
    "    return df\n",
    "    \n",
    "# Cleaning step to make output equivalent to Alteryx version\n",
    "def post_cleaning(df_retention: DataFrame) -> DataFrame:\n",
    "    \n",
    "    ## Returning to original customer name/ remove '-CR$' from end\n",
    "    df_retention = df_retention.with_column(\n",
    "        \"CUSTOMERID\", regexp_replace(col(\"CUSTOMERID\"), r\"-CR\\$$\", \"\")\n",
    "    )\n",
    "\n",
    "    ## Removing row with ARR = 0 and PRIOR_ARR = 0\n",
    "    non_zero_arr_prior_arr = ~((col(\"ARR\") == 0) & (col(\"PRIOR_ARR\") == 0))\n",
    "    df_retention = df_retention.filter(non_zero_arr_prior_arr).sort(col('CURRENTPERIOD'))\n",
    "\n",
    "    df_retention = rename_cols(df_retention)\n",
    "    \n",
    "    return df_retention\n",
    "\n",
    "# Custom sum function to add list of columns\n",
    "def sum_columns(columns):\n",
    "    return reduce(lambda a, b: a + b, columns)\n",
    "\n",
    "# Creating amount columns\n",
    "def add_amount_columns(df: DataFrame, input_amount) -> DataFrame:\n",
    "    customer_product_revenue_date_order_window = Window.partitionBy(\"CustomerID\", \"Product\", \"RevenueType\").orderBy(col(\"CurrentPeriod\").asc())\n",
    "\n",
    "    ## Adding new amounts\n",
    "    if input_amount == \"MRR\":\n",
    "        df = df.withColumn(\"ARR\",col(\"VALUE\")*12)\n",
    "        df = df.withColumn(\"MRR\",col(\"VALUE\"))\n",
    "    elif input_amount == \"ARR\":\n",
    "        df = df.withColumn(\"MRR\",col(\"VALUE\")/12)\n",
    "        df = df.withColumn(\"ARR\",col(\"VALUE\"))\n",
    "    else:\n",
    "        raise ValueError(\"Input amount not valid!\")\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"T3M\", sum_columns([lag(col(\"MRR\"), offset=i, default_value=0).over(customer_product_revenue_date_order_window) for i in range(3)]))\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"TTM\", sum_columns([lag(col(\"MRR\"), offset=i, default_value=0).over(customer_product_revenue_date_order_window) for i in range(12)]))\n",
    "\n",
    "    df = df.withColumn(\"T3M (Annualized)\", col(\"T3M\")*4)\n",
    "    return df\n",
    "\n",
    "# Main pipeline function\n",
    "def retention_pipeline_v2(session, df: DataFrame, lb_periods = [3], input_amount=\"MRR\", run_at_levels=[\"ARR\"]) -> DataFrame:\n",
    "    print(\"retention_pipeline_v2 started...\")\n",
    "    cache_s_time = time.time()\n",
    "    base_table = base_table_creation(session, df)\n",
    "    base_table_W_amount = add_amount_columns(base_table, input_amount)\n",
    "    base_table_W_amount_cols = base_table_W_amount.cache_result()\n",
    "    \n",
    "    # Process each run_at_level and collect results\n",
    "    run_at_level_dfs = []\n",
    "    \n",
    "    for run_at_level in run_at_levels:\n",
    "        if run_at_level not in [\"ARR\", \"MRR\", \"T3M\", \"TTM\", \"T3M (Annualized)\"]:\n",
    "            raise ValueError(\"Incorrect Run_at_level. Check user inputs.\")\n",
    "        \n",
    "        df_with_value = base_table_W_amount_cols.withColumn(\"VALUE\", col(run_at_level))\n",
    "        \n",
    "        # Process each lookback period within this run_at_level\n",
    "        retention_dfs = []\n",
    "        for lb in lb_periods:\n",
    "            if lb not in [1, 3, 12, 4]:\n",
    "                raise ValueError(\"Incorrect lookback period. Check user inputs.\")\n",
    "            \n",
    "            retention_df = retention(df_with_value, lb)\n",
    "            retention_dfs.append(retention_df.select(*retention_df.columns))\n",
    "        \n",
    "        # Union all retention_dfs for this run_at_level\n",
    "        if retention_dfs:\n",
    "            df_retention = retention_dfs[0]\n",
    "            for df in retention_dfs[1:]:\n",
    "                df_retention = df_retention.union_all(df)\n",
    "            \n",
    "            df_retention = post_cleaning(df_retention)\n",
    "            df_retention = df_retention.withColumn('\"Amount\"', lit(run_at_level))\n",
    "            run_at_level_dfs.append(df_retention.select(*df_retention.columns))\n",
    "    \n",
    "    # Union all run_at_level results\n",
    "    if run_at_level_dfs:\n",
    "        df_retention_all_amounts = run_at_level_dfs[0]\n",
    "        for df in run_at_level_dfs[1:]:\n",
    "            df_retention_all_amounts = df_retention_all_amounts.union_all(df)\n",
    "    else:\n",
    "        # Handle empty case to match original behavior\n",
    "        df_retention_all_amounts = None\n",
    "    cache_e_time = time.time()\n",
    "    print(f\"⏱️ retention_pipeline_v2 done: {cache_e_time - cache_s_time:.2f} seconds\")\n",
    "    return df_retention_all_amounts\n",
    "\n",
    "  \n",
    "# Data prep for period on period qa check\n",
    "def prep_data_pop_test_v2(df: DataFrame) -> DataFrame:\n",
    "    agg_df_2 = df.group_by(\"Current Period\").agg(\n",
    "        sum('\"EoP$\"').alias(\"EoP$\"),\n",
    "        sum('\"BoP$\"').alias(\"BoP$\")\n",
    "    )\n",
    "    \n",
    "    melted_df = agg_df_2.select(\n",
    "        col(\"Current Period\"),\n",
    "        col(\"EoP$\").alias(\"Value\"),\n",
    "        lit(\"EoP$\").alias(\"Metric\")\n",
    "    ).union_all(\n",
    "        agg_df_2.select(\n",
    "            col(\"Current Period\"),\n",
    "            col(\"BoP$\").alias(\"Value\"),\n",
    "            lit(\"BoP$\").alias(\"Metric\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    pivot_df_2 = melted_df.group_by(\"Metric\").pivot(\"Current Period\").agg(sum(\"Value\"))\n",
    "    pivot_df_2 = pivot_df_2.rename({col(\"Metric\"):\"Source\"})\n",
    "    pivot_df_2 = pivot_df_2.sort(col(\"Source\"))\n",
    "\n",
    "    return pivot_df_2\n",
    "    \n",
    "# Verifying QA check\n",
    "def verify_qa_check(input_df: DataFrame, columns_to_check: list, qa_check_thresholds: float) -> DataFrame:\n",
    "    \"\"\"Fliters the dataframe based on given column and threshold \n",
    "\n",
    "    Args:\n",
    "        input_df (DataFrame): _description_\n",
    "        columns_to_check (list): _description_\n",
    "        qa_check_thresholds (float): _description_\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: _description_\n",
    "    \"\"\"\n",
    "    filter_condition = None\n",
    "    for column in columns_to_check:\n",
    "        if filter_condition is None:\n",
    "            filter_condition = abs(col(column)) >= qa_check_thresholds\n",
    "        else:\n",
    "            filter_condition |= abs(col(column)) >= qa_check_thresholds\n",
    "    \n",
    "    # Apply the filter condition to the DataFrame\n",
    "    filtered_df = input_df.filter(filter_condition)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Save result in table\n",
    "def save_results(result: DataFrame, path: str, type:str = 'overwrite'):\n",
    "    \"\"\"Saves results in a table on the given patj\n",
    "\n",
    "    Args:\n",
    "        result (DataFrame): _description_\n",
    "        path (str): _description_\n",
    "        type (str): _description_\n",
    "    \"\"\"\n",
    "    #print(\"__Writing to table started...\")\n",
    "    start_time = time.time()\n",
    "    print(\"___Saving on path: \", path)\n",
    "    result.write.mode(type).save_as_table(path)\n",
    "    end_time = time.time()\n",
    "    print(\"_save_results.\")\n",
    "    print(f\"⏱️ Save result: {end_time - start_time:.2f} seconds\")\n",
    "    #print(\"__Writing to table completed!\")\n",
    "\n",
    "\n",
    "# Clear console output\n",
    "def clear_console():\n",
    "    \"\"\" Clears console\n",
    "    \"\"\"\n",
    "    if os.name == 'nt':  # For Windows\n",
    "        os.system('cls')\n",
    "    else:  # For macOS and Linux\n",
    "        os.system('clear')\n",
    "\n",
    "# To convert seconds into hours, mins and seconds\n",
    "def cal_time(start_time: int, end_time: int):\n",
    "    \"\"\" Take start and end time. Calculate hours, minutes, seconds\n",
    "\n",
    "    Args:\n",
    "        start_time: start time in secs\n",
    "        end_time: end time in secs\n",
    "\n",
    "    Returns:\n",
    "        hours, minutes, seconds\n",
    "    \"\"\"\n",
    "    elapsed_time = (end_time - start_time)\n",
    "    hours = elapsed_time // 3600\n",
    "    minutes = (elapsed_time % 3600) // 60\n",
    "    seconds = elapsed_time % 60\n",
    "    \n",
    "    return hours, minutes, seconds\n",
    "\n",
    "# To add suffix to file path based on retention level\n",
    "def get_file_path(retention_level: str, output_path: str) -> str:\n",
    "    if retention_level == \"Customer_level\":\n",
    "        updated_output_path = output_path + \"_C_NOTEBOOK\"\n",
    "    elif retention_level == \"Customer_Product_level\":\n",
    "        updated_output_path = output_path + \"_CP\"\n",
    "    elif retention_level == \"Customer_Product_RetentionType_level\":\n",
    "        updated_output_path = output_path + \"_CPR\"\n",
    "    elif retention_level == \"Level4\":\n",
    "        updated_output_path = output_path + \"_L4\"\n",
    "    return updated_output_path\n",
    "\n",
    "\n",
    "\n",
    "def transform_retention_data(session, df1: DataFrame, table2: str,  retention_level: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform retention data using Snowpark\n",
    "    \n",
    "    Args:\n",
    "        session: Snowpark session object\n",
    "        table1: Name of the first table (CUST_PROD_RETENTION_OLD)\n",
    "        table2: Name of the second table (CUST_RETENTION)\n",
    "    \n",
    "    Returns:\n",
    "        Snowpark DataFrame with transformed data\n",
    "    \"\"\"\n",
    "    # Load tables as DataFrames\n",
    "    print(\"**********In Tranform Retention Data***********\")\n",
    "    df2 = session.table(table2)\n",
    "\n",
    "    # Filter rows where RetentionCategory is in the list\n",
    "    filtered_df = df2.filter(\n",
    "        col('\"RetentionCategory\"').isin([\"Decrease_TempLost\", \"Increase_TempNew\"])\n",
    "    )\n",
    "\n",
    "    # Select the required columns\n",
    "    selected_df = filtered_df.select(\n",
    "        col('\"Amount\"'),\n",
    "        col('\"Current Period\"'),\n",
    "        col('\"CustName\"'),\n",
    "        col('\"RetentionCategory\"'),\n",
    "        col('\"Period\"')\n",
    "    )\n",
    "    # Group by all selected columns to mimic GROUP BY ALL\n",
    "    grouped_df = selected_df.group_by(\n",
    "        '\"CustName\"', '\"Current Period\"', '\"RetentionCategory\"', '\"Amount\"', '\"Period\"'\n",
    "    ).agg()\n",
    "\n",
    "    df2 = grouped_df\n",
    "    print(df2.schema)\n",
    "    # grouped_df now contains distinct rows matching the filter\n",
    "    # Columns to exclude\n",
    "    exclude_cols = [\n",
    "        '\"RetentionCategory\"', \"Credit_Reversal$\", \"Credit_$\", \"NewCust$\", \"ChurnCust$\", \"Increase$\", \n",
    "        \"Decrease$\", \"Decrease_TempLost$\", \"Increase_TempNew$\", \"Retained100%$\", \"NewProd$\", \n",
    "        \"ChurnProd$\", \"BoP_count\", \"ChurnCust_count\", \"ChurnProd_count\", \"Decreases_count\", \n",
    "        \"Decreases_TempLost_count\", \"Increases_TempNew_count\", \"Increase_count\", \"NewCust_count\", \n",
    "        \"NewProd_count\", \"EoP_count\",'\"Current Period\"','\"CustName\"','\"Period\"','\"Amount\"'\n",
    "    ]\n",
    "    new_cols = ['\"NewRevType$\"', '\"ChurnRevType$\"', '\"NewRevType_count\"', '\"ChurnRevType_count\"']\n",
    "\n",
    "    if retention_level == 'Customer_Product_RetentionType_level':\n",
    "        exclude_cols.extend(new_cols)\n",
    "\n",
    "    # Select all columns from T1 except excluded ones\n",
    "    select_cols = [col(c) for c in df1.columns if c not in exclude_cols]\n",
    "    print(df1.columns)\n",
    "    print(select_cols)\n",
    "    # Define new RetentionCategory logic (Tool ID 3438)\n",
    "    retention_category_expr = when(\n",
    "        (df1['\"RetentionCategory\"'] == \"Increase_TempNew\") & (df2['\"RetentionCategory\"'].is_null()),\n",
    "        lit(\"New Prod\")\n",
    "    ).when(\n",
    "        (df1['\"RetentionCategory\"'] == \"Decrease_TempLost\") & (df2['\"RetentionCategory\"'].is_null()),\n",
    "        lit(\"Churn Prod\")\n",
    "    ).when(\n",
    "        df2['\"RetentionCategory\"'].is_not_null(),\n",
    "        df2['\"RetentionCategory\"']\n",
    "    ).otherwise(df1['\"RetentionCategory\"'])\n",
    "\n",
    "    # Define YOY_VARIANCE-based columns (Tool ID 3439)\n",
    "    credit_reversal = when(col('\"RetentionCategory\"') == \"Credit Reversal\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    credit = when(col('\"RetentionCategory\"') == \"Credit\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    new_cust = when(col('\"RetentionCategory\"') == \"New Cust\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    churn_cust = when(col('\"RetentionCategory\"') == \"Churn Cust\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    increase = when(col('\"RetentionCategory\"') == \"Increase\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    decrease = when(col('\"RetentionCategory\"') == \"Decrease\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    decrease_temp_lost = when(col('\"RetentionCategory\"') == \"Decrease_TempLost\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    increase_temp_new = when(col('\"RetentionCategory\"') == \"Increase_TempNew\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    retained_100 = when(col('\"RetentionCategory\"') == \"NoChange\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    new_prod = when(col('\"RetentionCategory\"') == \"New Prod\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    churn_prod = when(col('\"RetentionCategory\"') == \"Churn Prod\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    if retention_level == 'Customer_Product_RetentionType_level':\n",
    "        new_rev_type = when(col('\"RetentionCategory\"') == \"New Rev Type\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "        churn_rev_type = when(col('\"RetentionCategory\"') == \"Churn Rev Type\", df1['\"YoY Variance\"']).otherwise(lit(0))\n",
    "    # Compute CHECK column\n",
    "    check_col = (\n",
    "        df1['\"BoP$\"']+ \n",
    "        new_cust + \n",
    "        churn_cust + \n",
    "        increase + \n",
    "        increase_temp_new + \n",
    "        decrease + \n",
    "        decrease_temp_lost + \n",
    "        new_prod + \n",
    "        churn_prod + \n",
    "        retained_100 - \n",
    "        df1['\"EoP$\"']\n",
    "    )\n",
    "\n",
    "    # Define count columns (Tool ID 3441)\n",
    "    bop_count = when(df1['\"BoP$\"'] != 0, lit(1)).otherwise(lit(0))\n",
    "    churn_cust_count = when(churn_cust != 0, lit(-1)).otherwise(lit(0))\n",
    "    churn_prod_count = when(churn_prod != 0, lit(-1)).otherwise(lit(0))\n",
    "    decreases_count = when(decrease != 0, lit(-1)).otherwise(lit(0))\n",
    "    decreases_temp_lost_count = when(decrease_temp_lost != 0, lit(-1)).otherwise(lit(0))\n",
    "    increases_temp_new_count = when(increase_temp_new != 0, lit(1)).otherwise(lit(0))\n",
    "    increase_count = when(increase != 0, lit(1)).otherwise(lit(0))\n",
    "    new_cust_count = when(new_cust != 0, lit(1)).otherwise(lit(0))\n",
    "    new_prod_count = when(new_prod != 0, lit(1)).otherwise(lit(0))\n",
    "    eop_count = when(df1['\"EoP$\"'] != 0, lit(1)).otherwise(lit(0))\n",
    "    if retention_level == 'Customer_Product_RetentionType_level':\n",
    "        new_rev_type_count = when(new_rev_type != 0, lit(1)).otherwise(lit(0))\n",
    "        churn_rev_type_count = when(churn_rev_type != 0, lit(1)).otherwise(lit(0))\n",
    "    \n",
    "\n",
    "    # Perform the full outer join\n",
    "    result_df = df1.join(\n",
    "        df2,\n",
    "        (df1['\"Current Period\"'] == df2['\"Current Period\"']) &\n",
    "        (df1['\"CustName\"'] == df2['\"CustName\"']) &\n",
    "        (df1['\"Period\"'] == df2['\"Period\"']) &\n",
    "        (df1['\"Amount\"'] == df2['\"Amount\"']),\n",
    "        \"full_outer\"\n",
    "    )\n",
    "\n",
    "    # Select final columns\n",
    "    if retention_level == 'Customer_Product_RetentionType_level':\n",
    "        result_df = result_df.select(\n",
    "            *select_cols,\n",
    "            df1['\"Current Period\"'].alias('\"Current Period\"'),df1['\"Period\"'].alias('\"Period\"'),\n",
    "            df1['\"CustName\"'].alias('\"CustName\"'),df1['\"Amount\"'].alias('\"Amount\"'),\n",
    "            retention_category_expr.alias('\"RetentionCategory\"'), credit_reversal.alias('\"Credit_Reversal$\"'),\n",
    "            credit.alias('\"Credit_$\"'),new_cust.alias('\"NewCust$\"'),\n",
    "            churn_cust.alias('\"ChurnCust$\"'),increase.alias('\"Increase$\"'),\n",
    "            decrease.alias('\"Decrease$\"'),decrease_temp_lost.alias('\"Decrease_TempLost$\"'),\n",
    "            increase_temp_new.alias('\"Increase_TempNew$\"'),retained_100.alias('\"Retained100%$\"'),\n",
    "            new_prod.alias('\"NewProd$\"'),churn_prod.alias('\"ChurnProd$\"'),\n",
    "            new_rev_type.alias('\"NewRevType$\"'),churn_rev_type.alias('\"ChurnRevType$\"'),\n",
    "            check_col.alias('\"Check\"'),bop_count.alias('\"BoP_count\"'),\n",
    "            churn_cust_count.alias('\"ChurnCust_count\"'),churn_prod_count.alias('\"ChurnProd_count\"'),\n",
    "            decreases_count.alias('\"Decreases_count\"'),decreases_temp_lost_count.alias('\"Decreases_TempLost_count\"'),\n",
    "            increases_temp_new_count.alias('\"Increases_TempNew_count\"'),increase_count.alias('\"Increase_count\"'),\n",
    "            new_cust_count.alias('\"NewCust_count\"'),new_prod_count.alias('\"NewProd_count\"'),\n",
    "            eop_count.alias('\"EoP_count\"'), new_rev_type_count.alias('\"NewRevType_count\"'),\n",
    "            churn_rev_type_count.alias('\"ChurnRevType_count\"')\n",
    "        )\n",
    "        result_df = result_df.drop(df1['\"ChurnCust$\"'],df1['\"Credit_Reversal$\"'],\n",
    "                                   df1['\"Credit_$\"'],df1['\"NewCust$\"'],df1['\"NewProd$\"'],\n",
    "                                   df1['\"Increase$\"'],df1['\"Decrease$\"'],df1['\"Retained100%$\"'],\n",
    "                                   df1['\"NewProd$\"'],df1['\"ChurnProd$\"'],df1['\"Check\"'],\n",
    "                                   df1['\"BoP_count\"'],df1['\"ChurnCust_count\"'],df1['\"ChurnProd_count\"'],\n",
    "                                   df1['\"Decreases_count\"'],df1['\"Decreases_TempLost_count\"'],\n",
    "                                   df1['\"Increases_TempNew_count\"'],df1['\"Increase_count\"'],\n",
    "                                   df1['\"NewCust_count\"'],df1['\"NewProd_count\"'],df1['\"EoP_count\"'])\n",
    "    else:\n",
    "        result_df = result_df.select(\n",
    "            *select_cols,\n",
    "            df1['\"Current Period\"'].alias('\"Current Period\"'),df1['\"Period\"'].alias('\"Period\"'),\n",
    "            df1['\"CustName\"'].alias('\"CustName\"'),df1['\"Amount\"'].alias('\"Amount\"'),\n",
    "            retention_category_expr.alias('\"RetentionCategory\"'), credit_reversal.alias('\"Credit_Reversal$\"'),\n",
    "            credit.alias('\"Credit_$\"'),new_cust.alias('\"NewCust$\"'),\n",
    "            churn_cust.alias('\"ChurnCust$\"'),increase.alias('\"Increase$\"'),\n",
    "            decrease.alias('\"Decrease$\"'),decrease_temp_lost.alias('\"Decrease_TempLost$\"'),\n",
    "            increase_temp_new.alias('\"Increase_TempNew$\"'),retained_100.alias('\"Retained100%$\"'),\n",
    "            new_prod.alias('\"NewProd$\"'),churn_prod.alias('\"ChurnProd$\"'),\n",
    "            check_col.alias('\"Check\"'),bop_count.alias('\"BoP_count\"'),\n",
    "            churn_cust_count.alias('\"ChurnCust_count\"'),churn_prod_count.alias('\"ChurnProd_count\"'),\n",
    "            decreases_count.alias('\"Decreases_count\"'),decreases_temp_lost_count.alias('\"Decreases_TempLost_count\"'),\n",
    "            increases_temp_new_count.alias('\"Increases_TempNew_count\"'),increase_count.alias('\"Increase_count\"'),\n",
    "            new_cust_count.alias('\"NewCust_count\"'),new_prod_count.alias('\"NewProd_count\"'),\n",
    "            eop_count.alias('\"EoP_count\"')\n",
    "        )\n",
    "        result_df = result_df.drop(df1['\"ChurnCust$\"'],df1['\"Credit_Reversal$\"'],\n",
    "                                   df1['\"Credit_$\"'],df1['\"NewCust$\"'],df1['\"NewProd$\"'],\n",
    "                                   df1['\"Increase$\"'],df1['\"Decrease$\"'],df1['\"Retained100%$\"'],\n",
    "                                   df1['\"NewProd$\"'],df1['\"ChurnProd$\"'],df1['\"Check\"'],\n",
    "                                   df1['\"BoP_count\"'],df1['\"ChurnCust_count\"'],df1['\"ChurnProd_count\"'],\n",
    "                                   df1['\"Decreases_count\"'],df1['\"Decreases_TempLost_count\"'],\n",
    "                                   df1['\"Increases_TempNew_count\"'],df1['\"Increase_count\"'],\n",
    "                                   df1['\"NewCust_count\"'],df1['\"NewProd_count\"'],df1['\"EoP_count\"'])\n",
    "    \n",
    "    print(result_df.schema)\n",
    "    print(\"**********Out Tranform Retention Data***********\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e8c90-92a4-4754-b363-4e0d2985883f",
   "metadata": {
    "collapsed": false,
    "name": "markdown_arr_mapping_fields"
   },
   "source": [
    "# Fields mapped according to highest ARR\n",
    "**Logic force fields to be 1:1 with customer based on highest ARR. Logic also outputs customers that were affected by the logic so that users can quantify it.**\n",
    "\n",
    "**Mapped Fields**\n",
    "- `REGION`\n",
    "- `INDUSTRY`\n",
    "- `CHANNEL`\n",
    "\n",
    "**IF you do not want to use it, kindly ignore \"ARR_mapping_fields\" block of code and use \"optimized_main\" instead of \"optimized_main_with_mapping_bundling\" to genearte output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00688e60-aa4f-4acb-b244-e579fa6c6c7c",
   "metadata": {
    "language": "python",
    "name": "arr_mapping_fields"
   },
   "outputs": [],
   "source": [
    "def standardize_customer_fields(session: Session, input_table_path: str, column_mapping_file: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizes fields to be 1:1 with customer based on the highest ARR value.\n",
    "    Also creates a report of affected customers.\n",
    "    \n",
    "    Parameters:\n",
    "    session (Session): The Snowpark session object.\n",
    "    input_table_path (str): Path to the input table.\n",
    "    column_mapping_file (dict): Dictionary mapping standard column names to input table column names.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (Standardized DataFrame, DataFrame of affected customers)\n",
    "    \"\"\"\n",
    "    print(\"\\n_Standardizing customer fields started...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read the input table\n",
    "    df = session.table(input_table_path)\n",
    "    \n",
    "    # Determine which fields to standardize based on the column mapping\n",
    "    fields_to_standardize = []\n",
    "    reverse_mapping = {}\n",
    "    \n",
    "    # Create a reverse mapping for easy reference and identify fields to standardize\n",
    "    for standard_col, input_col in column_mapping_file.items():\n",
    "        if input_col is not None:\n",
    "            # Remove quotes if present in the column name\n",
    "            clean_input_col = input_col.replace('\"', '')\n",
    "            reverse_mapping[clean_input_col] = standard_col\n",
    "            \n",
    "            # Add standard fields that should be standardized and exist in the input\n",
    "            if standard_col in ['Region', 'Industry', 'Channel'] and clean_input_col in df.columns:\n",
    "                fields_to_standardize.append(clean_input_col)\n",
    "    \n",
    "    # Get the customer ID and value (ARR) column names from the mapping\n",
    "    customer_id_col = column_mapping_file[\"CUSTOMERID\"].replace('\"', '') if column_mapping_file[\"CUSTOMERID\"] else \"CUSTOMERID\"\n",
    "    value_col = column_mapping_file[\"VALUE\"].replace('\"', '') if column_mapping_file[\"VALUE\"] else \"VALUE\"\n",
    "    \n",
    "    affected_customers = []\n",
    "    \n",
    "    # Create a standardized dataframe\n",
    "    standardized_df = df\n",
    "    \n",
    "    for field in fields_to_standardize:\n",
    "        # Check if we need to standardize this field (if customers have multiple values)\n",
    "        field_count_per_customer = standardized_df.group_by(customer_id_col, field).count()\n",
    "        customers_with_multiple_values = field_count_per_customer.group_by(customer_id_col).count().filter(col('COUNT') > 1)\n",
    "        \n",
    "        # Only proceed if there are customers that need standardization\n",
    "        if customers_with_multiple_values.count() > 0:\n",
    "            # Find dominant value for each customer based on highest total ARR\n",
    "            dominant_values = standardized_df.group_by(customer_id_col, field).agg(\n",
    "                sum(value_col).alias('TOTAL_ARR')\n",
    "            ).with_column(\n",
    "                'RANK', row_number().over(\n",
    "                    Window.partition_by(customer_id_col).order_by(col('TOTAL_ARR').desc())\n",
    "                )\n",
    "            ).filter(col('RANK') == 1).select(customer_id_col, field, 'TOTAL_ARR')\n",
    "            \n",
    "            # Get list of affected customers with their original and new values\n",
    "            affected_field_df = standardized_df.filter(\n",
    "                col(customer_id_col).in_(customers_with_multiple_values.select(customer_id_col))\n",
    "            ).select(customer_id_col, field, value_col)\n",
    "            \n",
    "            affected_field_df = affected_field_df.join(\n",
    "                dominant_values,\n",
    "                [customer_id_col],\n",
    "                suffix='_DOMINANT'\n",
    "            ).filter(\n",
    "                col(field) != col(f'{field}_DOMINANT')\n",
    "            ).select(\n",
    "                customer_id_col, \n",
    "                col(field).alias(f'ORIGINAL_{field}'),\n",
    "                col(f'{field}_DOMINANT').alias(f'NEW_{field}'),\n",
    "                value_col\n",
    "            ).distinct()\n",
    "            \n",
    "            # Add to affected customers report\n",
    "            if affected_field_df.count() > 0:\n",
    "                affected_customers.append(affected_field_df)\n",
    "            \n",
    "            # Create a map of customerid to dominant value\n",
    "            dominant_map = dominant_values.select(customer_id_col, field).to_pandas().set_index(customer_id_col)[field].to_dict()\n",
    "            \n",
    "            # Update values in the original dataframe\n",
    "            standardized_df = standardized_df.with_column(\n",
    "                field,\n",
    "                when(\n",
    "                    col(customer_id_col).in_(customers_with_multiple_values.select(customer_id_col)),\n",
    "                    # Map function to replace values for affected customers\n",
    "                    when(\n",
    "                        lit(True), \n",
    "                        lit(None)  # Placeholder that will be replaced\n",
    "                    ).otherwise(col(field))\n",
    "                ).otherwise(col(field))\n",
    "            )\n",
    "            \n",
    "            # Use pandas to efficiently apply the mapping\n",
    "            # Convert to pandas, apply the mapping, and convert back to Snowpark DataFrame\n",
    "            pdf = standardized_df.to_pandas()\n",
    "            affected_indices = pdf[customer_id_col].isin(dominant_map.keys())\n",
    "            pdf.loc[affected_indices, field] = pdf.loc[affected_indices, customer_id_col].map(dominant_map)\n",
    "            standardized_df = session.create_dataframe(pdf)\n",
    "            \n",
    "            print(f\"Standardized '{field}' for {customers_with_multiple_values.count()} customers\")\n",
    "    \n",
    "    # Combine all affected customers into a single report\n",
    "    affected_customers_df = None\n",
    "    if affected_customers:\n",
    "        affected_customers_df = reduce(\n",
    "            lambda df1, df2: df1.union(df2), \n",
    "            affected_customers\n",
    "        )\n",
    "        \n",
    "        # Save affected customers to a table\n",
    "        affected_customers_report_path = input_file_path + \"_AFFECTED_CUSTOMERS_REPORT\"\n",
    "        save_results(affected_customers_df, affected_customers_report_path)\n",
    "        print(f\"Saved affected customers report to {affected_customers_report_path}\")\n",
    "    else:\n",
    "        print(\"No customers were affected by standardization\")\n",
    "    \n",
    "    # Save standardized dataframe\n",
    "    output_path = input_file_path + \"_ARR_Mapped\"\n",
    "    save_results(standardized_df, output_path)\n",
    "    print(f\"___Saving on path: {output_path}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"⏱️ Field standardization runtime: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return standardized_df, affected_customers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0956d21-f2b8-40cd-ad23-44afb18d7a3d",
   "metadata": {
    "collapsed": false,
    "name": "markdown_product_bundle"
   },
   "source": [
    "# Product Bundle Analysis\n",
    "**Goal: Analyze customer-product bundles using Snowpark to generate ARR, MRR, and cross-sell insights.**\n",
    "\n",
    "`Load Data`\n",
    "\n",
    "- Read input Snowflake table: input_table_product_bundle.\n",
    "\n",
    "`Map & Validate Columns`\n",
    "\n",
    "- Rename columns using column_mapping_bundling.\n",
    "\n",
    "- Check for required fields like CUSTOMERID, PRODUCT, VALUE, etc.\n",
    "\n",
    "`Customer-Level Metrics`\n",
    "\n",
    "- Group by period & customer to calculate:\n",
    "\n",
    "- ARR, CROSSSELL\n",
    "\n",
    "- PRODUCT_COMBO (distinct products list)\n",
    "\n",
    "`Bundle Calculation`\n",
    "\n",
    "- Compute PRODUCT_BUNDLE as number of products (via comma count in PRODUCT_COMBO).\n",
    "\n",
    "`Final Aggregation`\n",
    "\n",
    "- Group by period, cohort date, bundle info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df75d4e-4b27-439c-90d0-9add49c38535",
   "metadata": {
    "language": "python",
    "name": "product_bundle"
   },
   "outputs": [],
   "source": [
    "def product_bundle_analysis(session: Session):\n",
    "    \"\"\"\n",
    "    Convert SQL query to Snowpark DataFrame for product bundle analysis\n",
    "    Integrates with existing notebook flow\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 3: GENERATING PRODUCT BUNDLE ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Mapping File with respect to columns available in input_table_product_bundle\n",
    "    column_mapping_bundling = {\n",
    "    \"CURRENTPERIOD\": '\"Current Period\"',\n",
    "    \"CUSTOMERID\": '\"CustName\"',\n",
    "    \"TRANSACTION_COHORT_DATE\": '\"Cohort Date\"',\n",
    "    \"VALUE\": '\"EoP$\"',\n",
    "    \"NEWPROD_ARR\": '\"NewRevType$\"',\n",
    "    \"PRODUCT\": '\"Product\"',\n",
    "    \"PERIOD\": '\"Period\"',\n",
    "    \"AMOUNT\": '\"Amount\"'}\n",
    "    \n",
    "    # Read the input data from the standardized output\n",
    "    #input_table = get_file_path(\"Customer_Product_level\", pbi_retention_output_path)\n",
    "    print(f\"Reading data from: {input_table_product_bundle}\")\n",
    "    \n",
    "    # Use Snowpark DataFrame\n",
    "    df = session.table(input_table_product_bundle)\n",
    "    \n",
    "    # Debug: Print column names to see what's available\n",
    "    # print(\"Available columns in source data:\")\n",
    "    # print(df.columns)\n",
    "    \n",
    "    # Create a working copy with mapped columns\n",
    "    work_df = df\n",
    "    \n",
    "    # Rename columns according to mapping\n",
    "    for sql_col, df_col in column_mapping_bundling.items():\n",
    "        if df_col in df.columns:\n",
    "            work_df = work_df.rename(df_col, sql_col)\n",
    "    \n",
    "    # print(\"Columns after mapping:\")\n",
    "    # print(work_df.columns)\n",
    "    \n",
    "    # Check for missing columns\n",
    "    required_columns = ['CURRENTPERIOD', 'CUSTOMERID', 'TRANSACTION_COHORT_DATE', 'VALUE', 'NEWPROD_ARR', 'PRODUCT']\n",
    "    missing_cols = [col for col in required_columns if col not in work_df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        # print(f\"Missing columns: {missing_cols}\")\n",
    "        raise ValueError(f\"Required columns are missing: {missing_cols}\")\n",
    "    \n",
    "    try:\n",
    "        # STEP 1: Equivalent to the customer_level CTE\n",
    "        print(\"Creating customer_level aggregation...\")\n",
    "        \n",
    "        # Using listagg with WITHIN GROUP to get distinct, sorted products\n",
    "        customer_level = work_df.group_by(\n",
    "            'CURRENTPERIOD', 'CUSTOMERID', 'TRANSACTION_COHORT_DATE', 'AMOUNT', 'PERIOD'\n",
    "        ).agg(\n",
    "            sum_sf('VALUE').alias('ARR'),\n",
    "            sum_sf('NEWPROD_ARR').alias('CROSSSELL'),\n",
    "            F.call_builtin(\"LISTAGG\", F.call_builtin(\"DISTINCT\", col('PRODUCT')), lit(',')).alias('PRODUCT_COMBO')\n",
    "        )\n",
    "        \n",
    "        # Calculate MRR from ARR using Snowpark's column operations\n",
    "        customer_level = customer_level.with_column('MRR', col('ARR') / 12)\n",
    "        \n",
    "        # STEP 2: Equivalent to the product_bundle CTE\n",
    "        print(\"Calculating product bundles...\")\n",
    "        \n",
    "        # Count commas in PRODUCT_COMBO to determine bundle size and add 1\n",
    "        # We need to handle NULL values with COALESCE in Snowpark\n",
    "        customer_level = customer_level.with_column(\n",
    "            'PRODUCT_BUNDLE', \n",
    "            regexp_count(col('PRODUCT_COMBO'), ',') + 1\n",
    "        )\n",
    "        \n",
    "        # STEP 3: Equivalent to the final SELECT query\n",
    "        print(\"Performing final aggregation...\")\n",
    "        \n",
    "        # Group by the required fields for sum aggregations using Snowpark\n",
    "        final_result = customer_level.group_by(\n",
    "            'CURRENTPERIOD', 'TRANSACTION_COHORT_DATE', 'PRODUCT_COMBO', 'PRODUCT_BUNDLE'\n",
    "        ).agg(\n",
    "            sum_sf('ARR').alias('ARR'),\n",
    "            sum_sf('MRR').alias('MRR'),\n",
    "            sum_sf('CROSSSELL').alias('CROSSSELL'),\n",
    "            count(when(col('ARR') > 0, col('CUSTOMERID'))).alias('DISTINCT_CUSTOMERS_WITH_ARR')\n",
    "        )\n",
    "        \n",
    "        # Write results to output table using Snowpark's native save method\n",
    "        output_table = input_table_product_bundle + \"_PRODUCT_BUNDLE_ANALYSIS\"\n",
    "        # print(f\"Writing results to: {output_table}\")\n",
    "        \n",
    "        # Save to Snowflake table with mode overwrite\n",
    "        final_result.write.mode(\"overwrite\").save_as_table(output_table)\n",
    "        \n",
    "        print(f\"___Saving on path: {output_table}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in product bundle analysis: {str(e)}\")\n",
    "        print(\"Detailed error information:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417bb747-8630-438f-b84d-48de30bdb511",
   "metadata": {
    "collapsed": false,
    "name": "markdown_round_off"
   },
   "source": [
    "# Float Column Rounding\n",
    "**Goal: Ensure consistency and control over decimal precision by rounding all float-type columns in a Snowflake table to a specified number of decimal places (default: 3).**\n",
    "\n",
    "`Read Input Table`\n",
    "\n",
    "- Loads table from the given table_path.\n",
    "\n",
    "`Identify Float Columns`\n",
    "\n",
    "- Uses DESCRIBE TABLE to detect columns of type: FLOAT, DOUBLE, REAL, DECIMAL, or NUMERIC.\n",
    "\n",
    "`Round Float Values`\n",
    "\n",
    "- Applies rounding to each float column using Snowpark's round() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8e169-d5d8-4616-94d0-50ad85764a07",
   "metadata": {
    "language": "python",
    "name": "round_off"
   },
   "outputs": [],
   "source": "def round_float_columns(session: Session, table_path, decimal_places=3):\n    \"\"\"\n    Rounds all float columns in a Snowflake table to the specified number of decimal places.\n    \n    Parameters:\n    session (Session): The Snowpark session object.\n    table_path (str): Path to the input table.\n    decimal_places (int): Number of decimal places to round to (default: 3).\n    \n    Returns:\n    DataFrame: DataFrame with float columns rounded to specified decimal places.\n    \"\"\"\n    from snowflake.snowpark.functions import round, col\n    import time\n    \n    print(f\"\\n_Rounding float columns to {decimal_places} decimal places...\")\n    start_time = time.time()\n    \n    # Read the table first\n    df = session.table(table_path)\n    \n    # Get column information using Snowflake's DESCRIBE TABLE command\n    table_columns = session.sql(f\"DESCRIBE TABLE {table_path}\").collect()\n    \n    # Identify float columns\n    float_columns = []\n    for column_info in table_columns:\n        column_type = str(column_info['type']).upper()\n        column_name = column_info['name']\n        if any(float_type in column_type for float_type in ['FLOAT', 'DOUBLE', 'REAL', 'DECIMAL', 'NUMERIC']):\n            float_columns.append(column_name)\n    \n    if not float_columns:\n        print(\"No float columns found in the table.\")\n        return df\n    \n    print(f\"Found {len(float_columns)} float columns to round: {', '.join(float_columns)}\")\n    \n    # Apply rounding to each float column using Snowpark DataFrame API\n    for column_name in float_columns:\n        df = df.withColumn(column_name, round(col(column_name), decimal_places))\n    \n    # Save the rounded data back to a new table\n    rounded_table_path = table_path + \"_ROUNDED\"\n    df.write.mode(\"overwrite\").save_as_table(rounded_table_path)\n    print(f\"___Saving on path: {rounded_table_path}\")\n    \n    end_time = time.time()\n    print(f\"⏱️ Float column rounding runtime: {end_time - start_time:.2f} seconds\")\n    \n    return df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4c35b-9413-43e6-9508-b266c1acfe52",
   "metadata": {
    "language": "python",
    "name": "dimension_date_dim"
   },
   "outputs": [],
   "source": "def create_master_dimension_table(session: Session, source_table_name: str, target_table_name: str):\n    \"\"\"\n    Args:\n        session: Snowpark session\n        source_table_name: Source table name (e.g., \"SUBSCRIPTION_ACCEL.PYTHON_TESTING.FACT_TABLE_AMSTERDAM_INPUT_100K_24JUNE_MASTER\")\n        target_table_name: Target table name (e.g., \"SUBSCRIPTION_ACCEL.PYTHON_TESTING.AMSTERDAM_INPUT_100K_24JUNE_MASTER_DIMENSION_DIM\")\n    \n    Returns:\n        DataFrame: The aggregated result that can be saved to a table\n    \"\"\"\n    \n    # Helper function to get column reference\n    def get_col(df, col_name):\n        \"\"\"Get column reference, handling both quoted and unquoted identifiers\"\"\"\n        columns = df.columns\n        \n        # Try exact match first\n        if col_name in columns:\n            return col(col_name)\n        \n        # Try with quotes\n        quoted_name = f'\"{col_name}\"'\n        if quoted_name in columns:\n            return col(quoted_name)\n        \n        # Try case variations\n        for c in columns:\n            if c.upper() == col_name.upper():\n                return col(c)\n        \n        # If not found, return the original and let Snowflake handle the error\n        return col(col_name)\n    \n    # print(f\"Reading source table: {source_table_name}\")\n    \n    # Read the source table\n    df = session.table(source_table_name)\n    \n    # print(\"Available columns:\", df.columns)\n    \n    # Group by all columns except \"Boomerang flag\" and aggregate\n    result_df = df.groupBy(\n        get_col(df, \"CustomerID\"),\n        get_col(df, \"Account Size\"),\n        get_col(df, \"Revenue Type\"),\n        get_col(df, \"Region\"),\n        get_col(df, \"Industry\"),\n        get_col(df, \"Channel\"),\n        get_col(df, \"Cohort Date\"),\n        get_col(df, \"Product Category\"),\n        get_col(df, \"Product\")\n    ).agg(\n        spark_max(get_col(df, \"Boomerang flag\")).alias(\"Boomerang flag\")\n    ).select(\n        # Explicitly select all columns to match the original SQL order\n        get_col(df, \"CustomerID\"),\n        get_col(df, \"Account Size\"),\n        get_col(df, \"Revenue Type\"),\n        get_col(df, \"Region\"),\n        get_col(df, \"Industry\"),\n        get_col(df, \"Channel\"),\n        col(\"Boomerang flag\"),  # This comes from the aggregation\n        get_col(df, \"Cohort Date\"),\n        get_col(df, \"Product Category\"),\n        get_col(df, \"Product\")\n    )\n    \n    # print(f\"Creating/replacing table: {target_table_name}\")\n    \n    # Write the result to the target table (equivalent to CREATE OR REPLACE TABLE)\n    result_df.write.mode(\"overwrite\").saveAsTable(target_table_name)\n    \n    print(f\"___Saving on path:  {target_table_name}\")\n    \n    return result_df\n\n# Example usage:\n# Assuming you have a session already created\n\n# Source and target table names\n# SOURCE_TABLE = fact_table_output_path\n# TARGET_TABLE = fact_table_output_path + \"_DIMENSION_DIM\"\n\n# Call the function\n# result_df = create_master_dimension_table(session, SOURCE_TABLE, TARGET_TABLE)\n\n# Display the results (optional)\n# result_df.show()\n\n# To get row count\n# print(f\"Total rows in result: {result_df.count()}\")\n\n\ndef create_date_dimension_table(session: Session, source_table_name: str, target_table_name: str, row_count: int = 10000):\n    \"\"\"\n    Convert the SQL CREATE TABLE query to Snowpark using Snowflake DataFrames\n    Creates a date dimension table with sequential dates from min to max date in source table\n    \n    Args:\n        session: Snowpark session\n        source_table_name: Source table name (e.g., \"SUBSCRIPTION_ACCEL.PYTHON_TESTING.FACT_TABLE_AMSTERDAM_INPUT_100K_24JUNE_MASTER\")\n        target_table_name: Target table name (e.g., \"SUBSCRIPTION_ACCEL.PYTHON_TESTING.AMSTERDAM_INPUT_100K_24JUNE_MASTER_DATE_DIM\")\n        row_count: Number of rows to generate in the sequence (default: 10000)\n    \n    Returns:\n        DataFrame: The date dimension result that can be saved to a table\n    \"\"\"\n    \n    # Helper function to get column reference\n    def get_col(df, col_name):\n        \"\"\"Get column reference, handling both quoted and unquoted identifiers\"\"\"\n        columns = df.columns\n        \n        # Try exact match first\n        if col_name in columns:\n            return col(col_name)\n        \n        # Try with quotes\n        quoted_name = f'\"{col_name}\"'\n        if quoted_name in columns:\n            return col(quoted_name)\n        \n        # Try case variations\n        for c in columns:\n            if c.upper() == col_name.upper():\n                return col(c)\n        \n        # If not found, return the original and let Snowflake handle the error\n        return col(col_name)\n    \n    # print(f\"Reading source table: {source_table_name}\")\n    \n    # Read the source table\n    df = session.table(source_table_name)\n    \n    # print(\"Available columns:\", df.columns)\n    \n    # Step 1: Min_Date_Summary CTE - Get minimum date from source table\n    # print(\"Step 1: Getting minimum date...\")\n    min_date_df = df.select(spark_min(get_col(df, \"Date\")).alias(\"Min_Date\"))\n    min_date_value = min_date_df.collect()[0][\"MIN_DATE\"]\n    # print(f\"Minimum date found: {min_date_value}\")\n    \n    # Step 2 & 3: CTE_MY_DATE - Generate date sequence using GENERATOR and SEQ4\n    # print(f\"Step 2-3: Generating date sequence with {row_count} rows...\")\n    \n    # Create generator table with specified row count\n    # In Snowpark, we need to use the table function approach\n    generator_df = session.sql(f\"SELECT SEQ4() as seq_num FROM TABLE(GENERATOR(ROWCOUNT=>{row_count}))\")\n    \n    # Add months to min_date using seq_num values\n    date_sequence_df = generator_df.select(\n        dateadd(\"month\", col(\"seq_num\"), lit(min_date_value)).alias(\"DATE\")\n    )\n    \n    # Step 4: Get max date from source where TTM <> 0 for filtering\n    # print(\"Step 4: Getting maximum TTM date for filtering...\")\n    max_ttm_date_df = df.filter(get_col(df, \"TTM\") != 0).select(\n        spark_max(get_col(df, \"Date\")).alias(\"Max_TTM_Date\")\n    )\n    max_ttm_date_value = max_ttm_date_df.collect()[0][\"MAX_TTM_DATE\"]\n    # print(f\"Maximum TTM date found: {max_ttm_date_value}\")\n    \n    # Step 4: Date_Dimension CTE - Filter dates and add row numbers\n    # print(\"Step 4: Creating date dimension with filtering and row numbers...\")\n    \n    # Filter dates <= max TTM date\n    filtered_dates_df = date_sequence_df.filter(col(\"DATE\") <= lit(max_ttm_date_value))\n    \n    # Add row number and TTM Date columns\n    window_spec = Window.orderBy(col(\"DATE\"))\n    \n    date_dimension_df = filtered_dates_df.select(\n        col(\"DATE\").alias(\"Date\"),\n        row_number().over(window_spec).alias(\"INDEX\"),\n        col(\"DATE\").alias(\"TTM Date\")\n    )\n    \n    # Final step: Order by Date ASC (equivalent to ORDER BY \"Date\" ASC)\n    # print(\"Final step: Ordering results by Date...\")\n    final_result_df = date_dimension_df.orderBy(col(\"Date\").asc())\n    \n    # print(f\"Creating/replacing table: {target_table_name}\")\n    \n    # Write the result to the target table (equivalent to CREATE TABLE)\n    final_result_df.write.mode(\"overwrite\").saveAsTable(target_table_name)\n    \n    print(f\"___Saving on path:  {target_table_name}\")\n    \n    return final_result_df\n\n# Example usage:\n# Source and target table names\n# SOURCE_TABLE = fact_table_output_path\n# TARGET_TABLE = fact_table_output_path + \"_DATE_DIM\"\n\n# Call the function to create the table\n# result_df = create_date_dimension_table(session, SOURCE_TABLE, TARGET_TABLE, row_count=10000)\n\n# Display the results (optional)\n# result_df.show(20)  # Show first 20 rows\n\n# To get row count\n# print(f\"Total rows in date dimension: {result_df.count()}\")\n\n# You can also customize the row count if needed\n# result_df = create_date_dimension_table(session, SOURCE_TABLE, TARGET_TABLE, row_count=5000)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4e86f-046e-4bfe-98c0-a3d6de19877f",
   "metadata": {
    "language": "python",
    "name": "rename_column"
   },
   "outputs": [],
   "source": "def rename_fixed_tables(session):\n    tables_and_renames = {\n        fact_table_output_path: [(\"REGION\", \"Region\"), (\"INDUSTRY\", \"Industry\"), (\"CHANNEL\", \"Channel\")],\n        fact_table_output_path + \"_DATE_DIM\": [(\"DATE\", \"Date\")],\n        fact_table_output_path + \"_DIMENSION_DIM\": [(\"REGION\", \"Region\"), (\"INDUSTRY\", \"Industry\"), (\"CHANNEL\", \"Channel\")]\n    }\n\n    for table_name, rename_list in tables_and_renames.items():\n        print(f\"\\n🔧 Renaming columns in: {table_name}\")\n        df = session.table(table_name)\n        actual_cols = df.columns\n        col_map = {}\n\n        for old_col, new_col in rename_list:\n            match = next((ac for ac in actual_cols if ac.upper() == old_col.upper()), None)\n            if match:\n                col_map[match] = new_col\n            else:\n                print(f\"⚠️ Column '{old_col}' not found in {table_name}\")\n\n        for src, tgt in col_map.items():\n            df = df.with_column_renamed(src, f'\"{tgt}\"')  # Quote to preserve casing\n\n        df.write.mode(\"overwrite\").save_as_table(table_name)\n        print(f\"✅ Updated: {table_name}\")\n\n\ndef rename_pbi_tables(session, retention_levels):\n    for level in retention_levels:\n        print(f\"\\n🔄 Processing PBI table for: {level}\")\n        \n        suffix = {\n            \"Customer_level\": \"_C_NOTEBOOK\",\n            \"Customer_Product_level\": \"_CP\",\n            \"Customer_Product_RetentionType_level\": \"_CPR\"\n        }.get(level)\n        \n        if not suffix:\n            print(f\"⚠️ Unknown retention level: {level}\")\n            continue\n\n        pbi_table = pbi_retention_output_path + suffix\n        df = session.table(pbi_table)\n        actual_cols = df.columns\n        col_map = {}\n\n        # Rename: \"Cohort Date\" → \"Cohort_Date\", WINBACKTAG → WinbackTag\n        rename_targets = [(\"Cohort Date\", \"Cohort_Date\"), (\"WINBACKTAG\", \"WinbackTag\")]\n\n        for old_col, new_col in rename_targets:\n            match = next((ac for ac in actual_cols if ac.replace('\"', '').upper() == old_col.upper()), None)\n            if match:\n                col_map[match] = new_col\n            else:\n                print(f\"⚠️ Column '{old_col}' not found in {pbi_table}, skipping.\")\n\n        for src, tgt in col_map.items():\n            df = df.with_column_renamed(src, f'\"{tgt}\"')  # Use quoted column names to preserve case\n\n        df.write.mode(\"overwrite\").save_as_table(pbi_table)\n        print(f\"✅ Updated PBI table: {pbi_table}\")"
  },
  {
   "cell_type": "markdown",
   "id": "9fc3278c-78c0-4c9a-95b0-ccca0e69fe89",
   "metadata": {
    "collapsed": false,
    "name": "markdown_flow"
   },
   "source": [
    "# Flow block\n",
    "**Retention Flow (retention_flow())**\n",
    "\n",
    "`Purpose: Builds retention tables based on the given level.`\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Loads input data via data_loading().\n",
    "\n",
    "- Runs retention_pipeline_v2() with session, lookbacks, and config.\n",
    "\n",
    "- Caches the result and saves it to pbi_path.\n",
    "\n",
    "- Returns: Processed retention DataFrame.\n",
    "\n",
    "**Fact Table Flow (fact_table_flow())**\n",
    "\n",
    "`Purpose: Builds a summarized fact table using highest priority retention level.`\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Determines the highest-level granularity from retention_levels.\n",
    "\n",
    "- Loads retention table and base fact data.\n",
    "\n",
    "- Filters retention data for selected amount/period.\n",
    "\n",
    "- Aggregates and joins with fact data on customer and product keys.\n",
    "\n",
    "- Calculates CM (Cohort Month) and drops redundant columns.\n",
    "\n",
    "- Saves final fact table to fact_path.\n",
    "\n",
    "**Databook Table Flow (databook_table_flow())**\n",
    "\n",
    "`Purpose: Creates an Excel-aligned grouped version of retention data.`\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Groups by key attributes (CustName, Product, Period, etc.).\n",
    "\n",
    "- Aggregates financial metrics (EoP$, BoP$, YoY Variance, etc.).\n",
    "\n",
    "- Truncates date to month granularity.\n",
    "\n",
    "- Saves grouped results to the Excel output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81adf0-c75e-4685-a80d-d9c080bae589",
   "metadata": {
    "language": "python",
    "name": "flow"
   },
   "outputs": [],
   "source": [
    "def retention_flow(session: Session, retention_level: str, pbi_path: str, input_file_path=input_file_path) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Processes the input data to build a retention table.\n",
    "\n",
    "    Parameters:\n",
    "    session (Session): The Snowpark session object.\n",
    "    retention_level (str): The level at which retention is calculated.\n",
    "    pbi_path (str): The file path where the resulting retention table will be saved.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The resulting DataFrame after processing the retention pipeline.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n_Building Retention Table Started at {retention_level}.\")\n",
    "    loaded_df = data_loading(\n",
    "        session,\n",
    "        input_file_path,\n",
    "        column_mapping_file,\n",
    "        #filter_condition=filter_condition,\n",
    "        type=\"retention\",\n",
    "        input_amount=input_amount,\n",
    "        retention_level=retention_level,\n",
    "    )\n",
    "\n",
    "    result = retention_pipeline_v2(\n",
    "        session,\n",
    "        loaded_df,\n",
    "        lookback_list,\n",
    "        input_amount,\n",
    "        run_at_levels,\n",
    "    )\n",
    "\n",
    "    # Cache the result before saving to prevent recomputation\n",
    "\n",
    "    result = result.cache_result()\n",
    "    \n",
    "    # print(\"Printing retention level oooooooooooooo:   \",retention_level)\n",
    "\n",
    "    # if retention_level == \"Customer_Product_level\" or retention_level == \"Customer_Product_RetentionType_level\":\n",
    "    #     result = transform_retention_data(session, result, input_table_templogic, retention_level)\n",
    "\n",
    "    if input_table_templogic is not None and (\n",
    "    retention_level == \"Customer_Product_level\" or \n",
    "    retention_level == \"Customer_Product_RetentionType_level\"):\n",
    "        result = transform_retention_data(session, result, input_table_templogic, retention_level) \n",
    "        \n",
    "    # result = result.drop('\"TOTAL_ARR\"','\"ARR_ROLLCHECK\"','\"COUNT_ROLLCHECK\"','\"COHORT_MAX_DATES_CHECK\"')\n",
    "    save_results(result, pbi_path)\n",
    "    print(f\"_Building Retention Table Completed at {retention_level}.\")\n",
    "    end_time = time.time()\n",
    "    print(f\"⏱️ Retention Flow Runtime: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def fact_table_flow(session: Session, fact_path: str, input_file_path=input_file_path) -> None:\n",
    "    \"\"\"\n",
    "    Processes the retention result DataFrame to build a fact table.\n",
    "\n",
    "    Parameters:\n",
    "    session (Session): The Snowpark session object.\n",
    "    fact_path (str): The file path where the resulting fact table will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"\\n_Building Fact Table Started...\")\n",
    "    level_map = {\"Customer_level\": 1\n",
    "                , \"Customer_Product_level\": 2\n",
    "                , \"Customer_Product_RetentionType_level\": 3\n",
    "                , \"Level4\": 4}\n",
    "\n",
    "    highest_level = None\n",
    "    highest_value = -1\n",
    "\n",
    "    for level in retention_levels:\n",
    "        value = level_map.get(level, 0)\n",
    "        if value > highest_value:\n",
    "           highest_value = value\n",
    "           highest_level = level\n",
    "\n",
    "    print(f\"Manual approach result: {highest_level}\")\n",
    "\n",
    "    #highest_level = max(retention_levels, key=lambda x: level_map.get(x, 0), default=None)\n",
    "    pbi_path = get_file_path(highest_level, pbi_retention_output_path)\n",
    "    retention_result = session.table(pbi_path)\n",
    "\n",
    "    \n",
    "    # print(\"_Building Fact Table Started...\")\n",
    "    fact_df = data_loading(\n",
    "        session,\n",
    "        input_file_path,\n",
    "        column_mapping_file,\n",
    "        #filter_condition=filter_condition,\n",
    "        type=\"fact\",\n",
    "        retention_level = highest_level\n",
    "    )\n",
    "\n",
    "    unique_amount = run_at_levels[0]\n",
    "\n",
    "    lb_value = lookback_list[0]\n",
    "    if lb_value == 1:\n",
    "        unique_period = \"Month\"\n",
    "    elif lb_value == 3:\n",
    "        unique_period = \"Quarter\"\n",
    "    elif lb_value == 12:\n",
    "        unique_period = \"Year\"\n",
    "    else:\n",
    "        unique_period = \"Year\"\n",
    "\n",
    "    result_temp = retention_result.filter(\n",
    "        (col('\"Amount\"') == unique_amount) & (col('\"Period\"') == unique_period)\n",
    "    )\n",
    "\n",
    "    result_short = (\n",
    "        result_temp.group_by(\n",
    "            col('\"CustName\"'),\n",
    "            col('\"Cohort Date\"'),\n",
    "            col('\"Current Period\"'),\n",
    "            col('\"Product\"'),\n",
    "            col('\"Revenue Type\"'),\n",
    "            col(\"LEVEL4\"),\n",
    "            col(\"Account Size\")  # Add Account Size to the group by\n",
    "        )\n",
    "        .agg(\n",
    "            count(col('\"CustName\"')).alias(\"Count of rows\"),\n",
    "            sum(col(\"T3M\")).alias(\"T3M\"),\n",
    "            sum(col(\"TTM\")).alias(\"TTM\"),\n",
    "            sum(col(\"MRR\")).alias(\"MRR\"),\n",
    "            sum(col(\"ARR\")).alias(\"ARR\"),\n",
    "            sum(col('\"T3M (Annualized)\"')).alias('\"T3M (Annualized)\"'),\n",
    "        )\n",
    "        .drop(\"Count of rows\")\n",
    "    )\n",
    "\n",
    "    fact_df_joined = fact_df.join(\n",
    "        result_short,\n",
    "        (fact_df['\"CustomerID\"'] == result_short['\"CustName\"'])\n",
    "        & (fact_df['\"Date\"'] == result_short['\"Current Period\"'])\n",
    "        & (fact_df['\"Product\"'] == result_short['\"Product\"'])\n",
    "        & (fact_df['\"Revenue Type\"'] == result_short['\"Revenue Type\"'])\n",
    "        & (fact_df['\"LEVEL4\"'] == result_short['\"LEVEL4\"']),\n",
    "        lsuffix=\"_left\",\n",
    "    )\n",
    "\n",
    "    # Updating fact_df values\n",
    "    fact_df_joined = fact_df_joined.withColumn(\n",
    "        \"CM\", datediff(\"day\", col('\"Cohort Date\"'), col('\"Date\"'))\n",
    "    )\n",
    "\n",
    "    fact_df_joined = fact_df_joined.drop(\n",
    "        ['\"CustName\"', '\"Current Period\"', '\"Product_left\"', '\"Revenue Type_left\"']\n",
    "    )\n",
    "\n",
    "    save_results(fact_df_joined, fact_path)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"_Building Fact Table Completed.\")\n",
    "    print(f\"⏱️ Fact Table Flow Runtime: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "def databook_table_flow(result: DataFrame, excel_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Generates a databook table from the given DataFrame and saves it to an Excel file.\n",
    "\n",
    "    Parameters:\n",
    "    result (DataFrame): Input DataFrame to process.\n",
    "    excel_path (str): Path to save the Excel file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"_Building Databook Table Started...\")\n",
    "\n",
    "    databook_gp = [\n",
    "        '\"Current Period\"',\n",
    "        '\"CustName\"',\n",
    "        '\"Product\"',\n",
    "        '\"Revenue Type\"',\n",
    "        '\"RetentionCategory\"',\n",
    "        '\"Period\"',\n",
    "        '\"Cohort Date\"',\n",
    "        '\"Amount\"',\n",
    "        '\"UFR Tag\"',\n",
    "        '\"Account Size\"',  # Add Account Size to the group by\n",
    "    ]\n",
    "\n",
    "    result_databook = result.groupBy(databook_gp).agg(\n",
    "        sum('\"EoP$\"').alias('\"EoP$\"'),\n",
    "        sum('\"BoP$\"').alias('\"BoP$\"'),\n",
    "        sum('\"YoY Variance\"').alias('\"YoY Variance\"'),\n",
    "        sum('\"UFR Amount\"').alias('\"UFR Amount\"'),\n",
    "        sum('\"Net Credit\"').alias('\"Net Credit\"'),\n",
    "    )\n",
    "\n",
    "    result_databook = result_databook.with_column(\n",
    "        \"Current Period\", date_trunc(\"MONTH\", col(\"Current Period\"))\n",
    "    )\n",
    "\n",
    "    #### result grouping outputs the full retention table GROUPED to align to the Excel template\n",
    "    print(\"Saving Results...\")\n",
    "    save_results(result_databook, excel_path)\n",
    "    print(\"_Building Databook Table Completed.\")\n",
    "    end_time = time.time()\n",
    "    print(f\"⏱️ Databook Flow Runtime: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0045ea-2bd7-4113-bad4-1cd07e677796",
   "metadata": {
    "collapsed": false,
    "name": "markdown_main"
   },
   "source": [
    "# Optimized_main_with_mapping_bundling\n",
    "This pipeline prepares, analyzes, and summarizes customer data to support retention reporting and revenue insights.\n",
    "\n",
    "**Step 0: Standardize Data**\n",
    "\n",
    "`standardize_customer_fields()`\n",
    "Cleans and maps raw customer data using column mapping.\n",
    "\n",
    "**Step 0.5: Round Float Columns**\n",
    "\n",
    "`round_float_columns()`\n",
    "Rounds all numeric fields to 3 decimal places for consistency.\n",
    "\n",
    "**Step 1: Retention Analysis**\n",
    "\n",
    "`retention_flow() (via retention_worker)`\n",
    "Runs retention logic at multiple levels (Customer, Customer_Product, CPR).\n",
    "Outputs to Power BI and Excel formats.\n",
    "Parallelized using ThreadPoolExecutor.\n",
    "\n",
    "**Step 2: Fact Table**\n",
    "\n",
    "`fact_table_flow()`\n",
    "Aggregates retention data into a summarized fact table for dashboarding.\n",
    "\n",
    "**Step 3: Product Bundle Analysis**\n",
    "\n",
    "`product_bundle_analysis()`\n",
    "Derives ARR/MRR, cross-sell, and bundle size insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b57e3-113b-47e0-bfa5-33ed08bd4dd3",
   "metadata": {
    "language": "python",
    "name": "main"
   },
   "outputs": [],
   "source": "# Original start timing code\nstart_time = time.time()\n\n# Add standardization step first - before any retention processing\nprint(\"\\n\" + \"=\"*50)\nprint(\"STEP 0: STANDARDIZING CUSTOMER FIELDS\")\nprint(\"=\"*50)\nstandardized_df, affected_customers_report = standardize_customer_fields(session, input_file_path, column_mapping_file)\n\n# If you want to use the standardized data for all subsequent operations,\n# update the input path to the processed output\nprocessed_input_path = input_file_path + \"_ARR_Mapped\"\n\n# Round all float columns to 3 decimal places\nprint(\"\\n\" + \"=\"*50)\nprint(\"STEP 0.5: ROUNDING FLOAT COLUMNS TO 3 DECIMAL PLACES\")\nprint(\"=\"*50)\n# Use the properly implemented Snowpark decimal function\nrounded_df = round_float_columns(session, processed_input_path, decimal_places=3)\n\n# Now proceed with the original workflow but using the standardized data\nprint(\"\\n\" + \"=\"*50)\nprint(\"STEP 1: RUNNING RETENTION ANALYSIS\")\nprint(\"=\"*50)\n\ndef retention_worker(retention_level):\n    print(\"Running for retention level:\", retention_level)\n    if retention_level == \"Customer_level\":\n        pbi_path = pbi_retention_output_path + \"_C_NOTEBOOK\"\n        excel_path = excel_retention_output_path + \"_C\"\n    elif retention_level == \"Customer_Product_level\":\n        pbi_path = pbi_retention_output_path + \"_CP\"\n        excel_path = excel_retention_output_path + \"_CP\"\n    elif retention_level == \"Customer_Product_RetentionType_level\":\n        pbi_path = pbi_retention_output_path + \"_CPR\"\n        excel_path = excel_retention_output_path + \"_CPR\"\n    else:\n        print(f\"Unknown retention level: {retention_level}\")\n        return None\n    \n    # Use the standardized data instead of the original input\n    retention_result = retention_flow(session, retention_level, pbi_path, input_file_path=processed_input_path)\n    print(\"-\" * 50)\n    databook_table_flow(retention_result, excel_path)\n    print(\"=\" * 50)\n    return retention_level\n\n# Execute in parallel\nwith ThreadPoolExecutor(max_workers=3) as executor:\n    futures = {executor.submit(retention_worker, level): level for level in retention_levels}\n    for future in as_completed(futures):\n        try:\n            result = future.result()\n        except Exception as e:\n            print(f\"Error processing {futures[future]}: {e}\")\n\n# Run fact table flow after all retention levels\nprint(\"\\n\" + \"=\"*50)\nprint(\"STEP 2.1: GENERATING FACT TABLE\")\nprint(\"=\"*50)\nfact_table_flow(session, fact_table_output_path, input_file_path=processed_input_path)\n\n# DIMENSION_DIM table creation\nprint(\"\\n\" + \"=\"*50)\nprint(\"STEP 2.2: GENERATING DIMENSION_DIM TABLE\")\noutput_dimension = fact_table_output_path + \"_DIMENSION_DIM\"\nresult_df = create_master_dimension_table(session, fact_table_output_path, output_dimension)\nprint(\"=\"*50)\n\n# DATE_DIM table creation\nprint(\"\\n\" + \"=\"*50)\nprint(\"STEP 2.3: GENERATING DATE_DIM TABLE\")\noutput_date = fact_table_output_path + \"_DATE_DIM\"\nresult_df = create_date_dimension_table(session, fact_table_output_path, output_date, row_count=10000)\nprint(\"=\"*50)\n\n#Add product bundle analysis\ncache_s_time = time.time()\ntry:\n    product_bundle_result = product_bundle_analysis(session)\nexcept Exception as e:\n    print(f\"Error in product bundle analysis step: {str(e)}\")\n    traceback.print_exc()\n    product_bundle_result = None\ncache_e_time = time.time()\nprint(f\"⏱️ Caching bundling: {cache_e_time - cache_s_time:.2f} seconds\")\n\nprint(\"-\" * 50)\n\n# Rename columns\nrename_fixed_tables(session)\nrename_pbi_tables(session, retention_levels)\n\n# Print execution time\nend_time = time.time()\nelapsed_seconds = end_time - start_time\nhours = int(elapsed_seconds // 3600)\nminutes = int((elapsed_seconds % 3600) // 60)\nseconds = int(elapsed_seconds % 60)\nprint(f\"Total execution time: {hours} hours {minutes} minutes {seconds} seconds\")"
  },
  {
   "cell_type": "markdown",
   "id": "e765be05-e325-4292-ba88-c4202d843b3a",
   "metadata": {
    "collapsed": false,
    "name": "markdown_qc_mechanism"
   },
   "source": [
    "# QC_Mechanism\n",
    "Generates two kind of outputs-       \n",
    "(1) `from input table`- Two columns generates- `Current Period` and `Total_Value`.   \n",
    "(2) `from PBI outputs`- Four columns generates- `Current Period`, `Period`, `SUM_EOP$` and `SUM_BOP$`.  \n",
    "**Note**- Filter second output wrt `Period` and then sort by `Current Period` and then compare it with first output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76102a5e-7943-4bc4-b3dd-ac73ed52d2ad",
   "metadata": {
    "language": "python",
    "name": "qc_mechanism"
   },
   "outputs": [],
   "source": [
    "# Read the input table\n",
    "print(\"QC table generation started for input file...\")\n",
    "input_df = session.table(input_file_path)\n",
    "\n",
    "# Group by Date and sum the ARR values\n",
    "monthly_arr_summary = input_df.group_by(col(column_mapping_file[\"CURRENTPERIOD\"])) \\\n",
    "                             .agg(snowpark_sum(col(column_mapping_file[\"VALUE\"])).alias(\"TOTAL_VALUE\")) \\\n",
    "                             .order_by(col(column_mapping_file[\"CURRENTPERIOD\"]))\n",
    "\n",
    "output_qc= input_file_path + \"_QC\"\n",
    "# Save the output to a new table\n",
    "monthly_arr_summary.write.mode('overwrite').save_as_table(output_qc)\n",
    "\n",
    "print(f\"✅ Monthly value summary table created and saved to {output_qc} for raw input file.\")\n",
    "print(\"=\"*50)\n",
    "print(\"QC table generation started for processed input file...\")\n",
    "\n",
    "# List of 6 input file paths\n",
    "input_files = [\n",
    "    pbi_retention_output_path + \"_C_NOTEBOOK\",\n",
    "    pbi_retention_output_path + \"_CP\",\n",
    "    pbi_retention_output_path + \"_CPR\",\n",
    "]\n",
    "\n",
    "# Iterate through each input file\n",
    "for input_table in input_files:\n",
    "    try:\n",
    "        # Attempt to load the table\n",
    "        df = session.table(input_table)\n",
    "\n",
    "        # Filter the DataFrame based on input_amount value\n",
    "        df = df.filter(F.col('\"Amount\"') == input_amount)\n",
    "\n",
    "        # Perform SUM aggregations grouped by \"Current Period\"\n",
    "        aggregated_df = (\n",
    "            df.group_by(\"Current Period\", '\"Period\"')\n",
    "              .agg(\n",
    "                  \n",
    "                  snowflake_sum('\"EoP$\"').alias(\"Sum_EoP$\"),\n",
    "                  snowflake_sum('\"BoP$\"').alias(\"Sum_BoP$\")\n",
    "              )\n",
    "        )\n",
    "\n",
    "        # Define output table name\n",
    "        output_table = input_table + \"_QC\"\n",
    "\n",
    "        # Save the result as a new table (overwrite if exists)\n",
    "        aggregated_df.write.mode(\"overwrite\").save_as_table(output_table)\n",
    "\n",
    "        print(f\"✅ Output saved with EoP$, BoP$ and Current Period to: {output_table} \")\n",
    "\n",
    "    except SnowparkSQLException as e:\n",
    "        print(f\"⚠️ Skipping unavailable file: {input_table} (Reason: {e.message})\")\n",
    "print(\"=\"*50)\n",
    "print(\"You can now compare the outputs of each file to verify the results for input and output files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb6a9a-2d9d-4576-bdf4-adb42cfb23e2",
   "metadata": {
    "collapsed": false,
    "name": "markdown_test"
   },
   "source": [
    "# Retention QA & PoP Validation\n",
    "`Input`\n",
    "- Load retention table (e.g., \"_C_NOTEBOOK\")\n",
    "\n",
    "- Extract distinct Period & Amount values\n",
    "\n",
    "`Loop Logic`\n",
    "\n",
    "- Filter data\n",
    "\n",
    "- all_checks() – Validates QA columns vs. thresholds\n",
    "\n",
    "- period_on_period_check_v2() – Compares BoP vs. lagged EoP\n",
    "\n",
    "`Functions`\n",
    "\n",
    "- all_checks(df, cols, thresholds)\n",
    "→ Marks Pass/Fail, shows failed rows\n",
    "\n",
    "- period_on_period_check_v2(period, df)\n",
    "→ Ensures EoP → BoP continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d8ebd-96ee-4fdf-bde8-28a2e4697f9b",
   "metadata": {
    "language": "python",
    "name": "test"
   },
   "outputs": [],
   "source": [
    "# User inputs\n",
    "pbi_retention_output_path = pbi_retention_output_path + \"_C_NOTEBOOK\" # Customer_level\n",
    "# pbi_retention_output_path = pbi_retention_output_path + \"_CP\" # Customer_Product_level\n",
    "# pbi_retention_output_path = pbi_retention_output_path + \"_CPR\" # Customer_Product_RetentionType_level\n",
    "\n",
    "retention_result = session.table(pbi_retention_output_path)\n",
    "periods = retention_result.select('\"Period\"').distinct().collect()\n",
    "amounts = retention_result.select('\"Amount\"').distinct().collect()\n",
    "\n",
    "def period_on_period_check_v2(period, filtered_df):\n",
    " \n",
    "    if period[0] == 'Month':\n",
    "        criteria = col(\"Current Period\")\n",
    "    elif period[0]  == 'Year':\n",
    "        criteria = year(col(\"Current Period\"))\n",
    "        window_spec = Window.orderBy()\n",
    "    # elif period[0]  == 'Quarter':\n",
    "    #     criteria = (year(col(\"Current Period\")), quarter(col(\"Current Period\")))\n",
    "    \n",
    "    result_df = filtered_df.group_by(criteria).agg(\n",
    "        sum(col('\"EoP$\"')).alias(\"sum_EoP$\"),\n",
    "        sum(col('\"BoP$\"')).alias(\"sum_BoP$\")\n",
    "    )\n",
    "    window_spec = Window.orderBy(criteria)\n",
    "    result_df = result_df.sort(criteria)\n",
    "        \n",
    "    df_with_lag = result_df.withColumn(\"sum_EoP$_pr\", lag(col(\"sum_EoP$\"), 1).over(window_spec))\n",
    "        \n",
    "    df_final = df_with_lag.withColumn(\n",
    "            \"Check_value\",\n",
    "            when(col(\"sum_BoP$\") == col(\"sum_EoP$_pr\"), 'True').otherwise('False')\n",
    "        )\n",
    "        \n",
    "    chk_result = df_final.filter(col(\"Check_value\") != True)\n",
    "        \n",
    "    if chk_result.count() == 1:\n",
    "        print(\"Period on Period Check\", \"-->\", \"Check Passed!\")\n",
    "        print(\"=\" * 50)\n",
    "    else:\n",
    "        print(\"Period on Period Check\", \"-->\", \"Check Failed!\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Snapshot view\")\n",
    "        df_final.show()\n",
    "        \n",
    "period_on_period_mapping={\n",
    "    \"Year\":\"ARR\",\n",
    "    \"Month\":\"MRR\",\n",
    "    # \"Quarter\":\"T3M\"\n",
    "}\n",
    "\n",
    "def all_checks(tmp, qa_columns, qa_check_thresholds):\n",
    "    for qa_column in qa_columns:\n",
    "        check_df = verify_qa_check(tmp, [qa_column], qa_check_thresholds)\n",
    "        # Try Catch block has been added to handle empty Dataframes after filter conditions in verify_qa_check\n",
    "        try:\n",
    "            sample = check_df.limit(1).collect()\n",
    "            if len(sample) == 0:\n",
    "                print(qa_column,\"-->\", \"Check Passed!\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(qa_column,\"-->\", \"Check if column exists in retention result: Specify correct coulumn in Input!\")\n",
    "            continue\n",
    "        result_str = \"Check Failed!\" if check_df.count() > 0 else \"Check Passed!\"\n",
    "        print(qa_column,\"-->\", result_str)\n",
    "        if result_str == \"Check Failed!\":\n",
    "            print(\"Snapshot view\")\n",
    "            check_df.show()\n",
    "    print(\"-\"*50)\n",
    "\n",
    "for amount in amounts: \n",
    "    for period in periods:\n",
    "        print(f\"Running for amount: {amount[0]} and period: {period[0]}\")\n",
    "        print(\"=\"*50)\n",
    "        tmp = retention_result.filter((col('\"Period\"') == period[0]) & (col('\"Amount\"')== amount[0]))\n",
    "        \n",
    "        all_checks(tmp, qa_columns, qa_check_thresholds)\n",
    "        \n",
    "        # if period_on_period_mapping.get(period[0]) == amount[0]:\n",
    "        #     print(\"Period on period check!\", period[0], amount[0])\n",
    "        #     tmp = retention_result.filter((col('\"Period\"') == period[0]) & (col('\"Amount\"')== amount[0]))\n",
    "        #     period_on_period_check_v2(period, tmp)"
   ]
  }
 ]
}